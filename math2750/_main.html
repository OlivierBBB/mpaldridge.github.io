<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Introduction to Markov Processes</title>
  <meta name="description" content="Introduction to Markov Processes" />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="Introduction to Markov Processes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Markov Processes" />
  
  
  

<meta name="author" content="Matthew Aldridge" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>



</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Introduction to Markov Processes</h1>
<p class="author"><em>Matthew Aldridge</em></p>
<p class="date"><em>2019–20</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Markov Processes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->

<p>This module is  . The module organiser and lecturer is Dr Matthew Aldridge, and my email address is .</p>

<p>This module lasts for 11 weeks. The first nine weeks run from 28 January to 29 March, then we break for Easter, then the final two weeks run from 29 April to 10 May.</p>

<p>There are two lectures each week, for a total of 22 lectures. Lectures are on Tuesdays at 1400 and Thursdays at 1000, both in Roger Stevens LT 20 (7M.20). Attendance at lectures is mandatory.</p>
<p>Outline lecture notes summarising the main definitions and theorems from the course will be made available on Minerva. I’m very keen to hear about errors, mathematical, typographical or otherwise, in the lecture notes – please email me or talk to me after lectures. The notes are not a substitute for attending the lectures. The lectures will be videoed on the lecture capture system.</p>

<p>There will be 10 problem sheets; Problem Sheet <span class="math inline">\(n\)</span> covers the material from two lectures in week <span class="math inline">\(n\)</span>, and will be discussed in your workshop in week <span class="math inline">\(n+1\)</span>.</p>
<p>The best way of learning the material in this course is to spend plenty of time working on the problem sheets in advance of your workshop and writing up your answers. Collaboration is encouraged when working through the problems, but I recommend writing up your work on your own.</p>

<p>There will be 10 workshops, starting in the second week. The main goal of the workshops will be to go over your answers to the problems sheets in smaller classes. You will have been assigned to one of five workshop groups, meeting on Mondays or Tuesdays, led by Dr Andrew Baczkowski, Dario Domingo (two groups), Dr Graham Murphy or me. Check your timetable for details.</p>


<p>There will be three pieces of assessed coursework, each making up 5% of your mark for the module, for a total of 15%. These will involve writing up answer to a few problems, in a similar style to the problem sheets. While you may want to discuss the assessment with others in advance of completing it by yourself, copying is not allowed and will be dealt with in accordance with University rules.</p>
<p>The assessments will be due on Thursdays 21 February, 14 March and 2 May at 1400.</p>

<p>There will be two computer practicals, to be completed in the fourth and seventh weeks of the module. There will be practical classes run during those weeks where you can work on the problems, get help with any difficulties, and have your work marked. (This does not form part of your official mark for the module.)</p>

<p>There will be a two-hour exam after the end of the module, making up the other 85% of your mark. The exam will consist of four questions, and you are expected to answer all of them. We will talk more about the exam in the revision sessions (Lectures 21 and 22).</p>

<p>I will run office hours on Mondays and Wednesdays at 1500 in my office: 9.320, Physics Research Deck. This is your opportunity to discuss with me anything that you want from the course, including material you are confused about and problems you don’t understand. If you can’t make these times, try emailing me to arrange a meeting, or knock on my office door and see if I’m free.</p>
<p>(The Physics Research Deck is easiest to reach from the 8th floor of EC Stoner at staircase 4. If you haven’t been there before, I recommend asking a friend who has for directions. Once in the maths area on the 9th floor of the PRD, my office is back on yourself, towards the Observatory.)</p>


<p>The course has two major parts: the first part will cover processes in discrete time and the second part processes in continuous time.</p>
An outline plan of the lectures is the following:



<p>You can do well on this module by attending the lectures and workshops, plus working on the problem sheets, assignments and practicals, without any further reading. However, for students who would like some book recommendations for optional extra background reading or an alternative view on the material, I recommend the following.</p>
The two books I found most useful in planning the course were:

<p>Norris discusses only Markov processes, and has some more detailed material that goes beyond this module. Grimmet and Stirzaker is an excellent handbook that covers most of undergraduate probability.</p>
The approach of Grimmet and Stirzaker is very closely mirrored in

<p>which might be useful if you have a copy to hand, or if the library runs out of Grimmet and Stirzaker.</p>
A gentler introduction with plenty of examples is provided by

<p>although it doesn’t cover everything in this module. It’s available online via the University library.</p>
Finally, I’ve heard good things about

<p>although I haven’t used it myself. It’s also available online.</p>
<p>(I also benefited from reading lecture notes from former lecturers of this course, particularly Dr Graham Murphy, whose help was invaluable.)</p>





<p>A  is an imitation of a real-world system. For example, you might want to have a model to imitate the world’s population, the level of water in a reservoir, future cashflows of a pension scheme, or future stock prices. Models allow us to try to predict what might happen in the real world in a low risk, cost effective and fast way.</p>
<p>To design a model requires a set of assumptions about how it will work and suitable parameters need to be determined, perhaps based on past collected data.</p>
<p>An important distinction is between  models and  (or ) models. Deterministic models do not contain any random components, so the output is completely determined by the inputs and any parameters. Random models have variable outcomes, so can be run many times to give a sense of the range of possible outcomes.</p>
Consider models for:

<p>In (a), the random components – for example, the effect of meteorites striking the Moon’s surface – are not very significant and a deterministic model based on physical laws is good enough for most purposes. In (b), the share price from day to day is highly uncertain, so a random model can take into account the variability and unpredictability in a useful way.</p>

<p>In this module we will see many examples of stochastic models. Lots of the applications we will consider come from financial mathematics and actuarial science where the use of models that take into account uncertainty is very important, but the principles apply in many areas.</p>

<p>If we want to model, for example, the total number of claims to an insurance company in the whole of 2019, we can use a random variable <span class="math inline">\(X\)</span> to model this – perhaps a Poisson distribution with an appropriate mean, for example. However, if we want to track how the number of claims changes over the course of the year 2019, we will need to use a . (The word <code>stochastic' means the same thing as</code>random’.)</p>
<p>A stochastic process, which we will write as <span class="math inline">\((X_n)\)</span>, is an indexed sequence of random variables that are (usually) dependent on each other. Each random variable <span class="math inline">\(X_n\)</span> takes a value in a  <span class="math inline">\(\mathcal S\)</span> which is the set of possible values for the process. As with usual random variables, the state space <span class="math inline">\(\mathcal S\)</span> can be  or . A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, <span class="math inline">\(\mathcal S = \{\text{Heads},\text{Tails}\}\)</span> is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers <span class="math inline">\(\mathcal S = \mathbb Z_+ = \{0,1,2,\dots\}\)</span>. A continuous state spaces denotes an uncountably infinite continuum of gradually varying outcomes. For example, the nonnegative real line <span class="math inline">\(\mathcal S = \mathbb R_+ = \{x \in \mathbb R : x \geq 0\}\)</span> is the state space for the amount of rainfall on a given day, while a bounded subset of <span class="math inline">\(\mathbb R^3\)</span> is the state space for the position of a gas particle in a box.</p>
<p>Further, the process has an  that puts the random variables that make up the process in order. The index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at different points, often denoted by <span class="math inline">\(n = 0,1,2,\dots\)</span>, while continuous time denotes a process monitored constantly over time, often denoted by <span class="math inline">\(t \in \mathbb R_+ = \{x \in \mathbb R : x \geq 0\}\)</span>. In the insurance example, we might count up the number of claims each day – then the discrete index set will be the days of the year, which we could denote <span class="math inline">\(\{1,2,\dots,365\}\)</span>. Alternatively, we might want to keep a constant tally that we update after every claim, requiring a continuous time index representing time across the whole year. In discrete time, we can write down the first few steps of the process as <span class="math inline">\((X_0, X_1, X_2, \dots)\)</span>.</p>

This gives us four possibilities in total:


<p>Because stochastic processes consist of a large number – even infinitely many – random variables that could all be dependent on each other, they can get extremely complicated. The Markov property is a crucial property that restricts the type of dependences in a process, to make the process easier to study, yet still leaves most of the useful and interesting examples intact.</p>
<p>Think of a simple board game where we roll a die and move that many squares forward on the board. Suppose we are currently on the square <span class="math inline">\(X_n\)</span>. Then which square <span class="math inline">\(X_{n+1}\)</span> we move to on our next turn:</p>


<p>It is this third point that is the crucial property of the stochastic processes we will study in this course, and it is called the  or . We say <code>memoryless', because it's as if the process forgot how it got here -- the process before this moment has no bearing on the future given where we are now. A mathematical way to say this is that</code>the past and the future are conditionally independent given the present.’</p>
<p>To write this down formally, we need to recall : the conditional probability of an event <span class="math inline">\(A\)</span> given another event <span class="math inline">\(B\)</span> is written <span class="math inline">\(\mathbb P(A \mid B)\)</span>, and is the probability that <span class="math inline">\(A\)</span> occurs  that <span class="math inline">\(B\)</span> definitely occurs. You may remember the definition <span class="math display">\[ \mathbb P(A \mid B) = \frac{\mathbb P(A \cap B)}{\mathbb P(B)} , \]</span> although is often more useful to reason directly about conditional probabilities than use this formula.</p>

<p>Here, the left hand side is the probability we go to state <span class="math inline">\(x_{n+1}\)</span> next conditioned on the entire history of the process, while the right hand side is the probability we go to state <span class="math inline">\(x_{n+1}\)</span> next conditioned only on where we are now. So this property tells us that it only matters where we are now and not how we got here.</p>
<p>(There’s also a similar definition for continuous time processes, which we’ll come to later in the course.)</p>
<p>Stochastic processes that have the Markov property are much easier to study than general processes, as we only have to keep track of where we are now and we don’t have to keep track of the entire history that came before.</p>




<p>Consider the following  on the integers <span class="math inline">\(\mathbb Z\)</span>: We start at <span class="math inline">\(0\)</span>, then at each time-step, we go up by one with probability <span class="math inline">\(p\)</span> and down by one with probability <span class="math inline">\(q = 1-p\)</span>. When <span class="math inline">\(p = q= 1/2\)</span>, we’re equally as likely to go up as down, and call this the .</p>
<p>The simple random walk can be used as simplified model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In most modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the `drunkard’s walk’, suggesting it could model a drunk person trying to stagger home.</p>



<p>We can write this as a stochastic process <span class="math inline">\((X_n)\)</span> with discrete time <span class="math inline">\(n = \{0,1,2,\dots\} = \mathbb Z_+\)</span> and discrete state space <span class="math inline">\(\mathcal S = \mathbb Z\)</span>, where <span class="math inline">\(X_0 = 0\)</span> and, for <span class="math inline">\(n \geq 0\)</span>, we have <span class="math display">\[ X_{n+1} = \begin{cases} X_n + 1 &amp; \text{with probability $p$,} \\
                             X_n - 1 &amp; \text{with probability $q$.} \end{cases} \]</span></p>
<p>It’s clear from this definition that <span class="math inline">\(X_{n+1}\)</span> (the future) depends on <span class="math inline">\(X_n\)</span> (the present), but, given <span class="math inline">\(X_n\)</span>, does not depend on <span class="math inline">\(X_{n-1}, X_{n-1}, \dots, X_0\)</span> (the past). Thus the Markov property holds, and the simple random walk is a  or .</p>



Note that alternative way to write the simple random walk is to put
<span class="math display">\[\begin{equation} \label{rw}
    X_n = X_0 + \sum_{i=1}^n Z_i , %\tag{$*$}
  \end{equation}\]</span>
<p>where the starting point is <span class="math inline">\(X_0 = 0\)</span> and the increments <span class="math inline">\(Z_1, Z_2, \dots\)</span> are independent and identically distributed (IID) random variables with distribution given by <span class="math inline">\(\mathbb P(Z_i = 1) = p\)</span> and <span class="math inline">\(\mathbb P(Z_i = -1) = q\)</span>. You can check that this means <span class="math inline">\(X_{n+1} = X_n + Z_{n+1}\)</span>, and that this property defines the simple random walk.</p>
<p>In fact, any stochastic process with the form  for some <span class="math inline">\(X_0\)</span> and some distribution for the <span class="math inline">\(Z_i\)</span>s is called a .</p>
<p>Random walks often have state space <span class="math inline">\(\mathcal S = \mathbb Z\)</span>, like the simple random walk, but they could be defined on other state spaces. We could look at higher dimensional simple random walks: in <span class="math inline">\(\mathbb Z^2\)</span>, for example, we could step up, down, left or right with given probabilities. We could even have a continuous state space like <span class="math inline">\(\mathbb R\)</span>, if, for example, the <span class="math inline">\(Z_i\)</span>s had a normal distribution.</p>
<p>We can use this structure to calculate the expectation or variance of any random walk (including the simple random walk).</p>
<p>Let’s start with the expectation. For a random walk <span class="math inline">\((X_n)\)</span> we have <span class="math display">\[ \mathbb E X_n = \mathbb E \left(X_0 + \sum_{i=1}^n Z_i\right) = \mathbb E X_0 + \sum_{i=1}^n \mathbb E Z_i = \mathbb EX_0 + n \mathbb E Z_1 , \]</span> where we’ve used the linearity of expectation, and that the <span class="math inline">\(Z_i\)</span>s are identically distributed.</p>
<p>In the case of the simple random walk, we have <span class="math inline">\(\mathbb E X_0 = 0\)</span>, since we start from <span class="math inline">\(0\)</span> with certainty, and <span class="math display">\[ \mathbb E Z_1 = \sum_{z \in \mathbb Z} z \mathbb P(Z_1 = z) = 1\times p + (-1)\times q = p-q ,\]</span> so <span class="math inline">\(\mathbb EX_n = n(p-q)\)</span>.</p>
<p>If <span class="math inline">\(p &gt; 1/2\)</span>, then <span class="math inline">\(p &gt; q\)</span>, so <span class="math inline">\(\mathbb E X_n\)</span> grows ever bigger over time, while if <span class="math inline">\(p &lt; 1/2\)</span>, then <span class="math inline">\(\mathbb E X_n\)</span> grows ever smaller (that is, negative with larger absolute value) over time. If <span class="math inline">\(p = 1/2 = q\)</span>, which is the case of the simple symmetric random walk, then then the expectation <span class="math inline">\(\mathbb E X_n = 0\)</span> is zero for all time.</p>
<p>Now the variance of a random walk. We have <span class="math display">\[ \Var(X_n) = \Var \left(X_0 + \sum_{i=1}^n Z_i\right) = \Var X_0 + \sum_{i=1}^n \Var Z_i = \Var X_0 + n \Var Z_1 , \]</span> where it was crucial that <span class="math inline">\(X_0\)</span> and all the <span class="math inline">\(Z_i\)</span>s were independent (so we had no covariance terms).</p>
Again, for a simple random walk <span class="math inline">\(\Var X_0 = 0\)</span>, since we always start from <span class="math inline">\(0\)</span>. To calculate the variance, we write
<span class="math display">\[\begin{align*}
  \Var(Z_1) &amp;= \mathbb E Z_1^2 - (\mathbb EZ_1)^2 \\
            &amp;= 1^2 \times p  + (-1)^2 \times q - (p-q)^2 \\
            &amp;= p + q - (p-q)^2 \\
            &amp;= 1 - (2p - 1)^2 \\
            &amp;= 4p - 4p^2 \\
            &amp;= 4pq ,
  \end{align*}\]</span>
<p>where we’ve used that <span class="math inline">\(q = 1-p\)</span>. Hence the variance of the simple random walk is <span class="math inline">\(4pqn\)</span>. Note that (unless <span class="math inline">\(p\)</span> is <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>) the variance grows over time, so it becomes harder and harder to predict where the random walk will be.</p>
<p>The variance of the simple symmetric random walk is <span class="math inline">\(4 \frac12 \frac12 n = n\)</span>.</p>
<p>For large <span class="math inline">\(n\)</span>, we can use a normal approximation for a random walk. Suppose the increments process <span class="math inline">\((Z_n)\)</span> has mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and that the walk starts from <span class="math inline">\(X_0 = 0\)</span>. Then we have <span class="math inline">\(\mathbb E X_n = \mu n\)</span> and <span class="math inline">\(\Var(X_n) = \sigma^2 n\)</span>, so for large <span class="math inline">\(n\)</span> we can use the normal approximation <span class="math inline">\(X_n \approx \mathrm{N}(\mu n, \sigma^2 n)\)</span>. (Note, of course, that the <span class="math inline">\(X_n\)</span> are not independent.) To be more formal, the central limit theorem tells us that, as <span class="math inline">\(n \to \infty\)</span>, we have <span class="math display">\[ \frac{X_n - n\mu}{\sigma \sqrt{n}} \to \mathrm{N}(0,1) . \]</span></p>

<p>In the case of the simple random walk, we can in fact give the exact distribution by writing down a formula for <span class="math inline">\(\mathbb P(X_n = i)\)</span> for any time <span class="math inline">\(n\)</span> and state <span class="math inline">\(i\)</span>.</p>
<p>Recall that, at each of the first <span class="math inline">\(n\)</span> times, we take an upward step with probability <span class="math inline">\(p\)</span>, and otherwise take a downward step. So if we let <span class="math inline">\(Y_n\)</span> be the number of upward steps over the first <span class="math inline">\(n\)</span> times, we see that <span class="math inline">\(Y_n\)</span> has a binomial distribution <span class="math inline">\(Y \sim \text{Bin}(n,p)\)</span>.</p>
<p>Recall that the binomial distribution has probability <span class="math display">\[  \mathbb P(Y_n = k)  = \binom nk p^k (1-p)^{n-k} = \binom nk p^k q^{n-k} , \]</span> for <span class="math inline">\(k = 0,1,\dots, n\)</span>, where <span class="math inline">\(\binom{n}{k}\)</span> is a binomial coefficient `<span class="math inline">\(n\)</span> choose <span class="math inline">\(k\)</span>’.</p>
If <span class="math inline">\(Y_n = k\)</span>, that means we’ve taken <span class="math inline">\(k\)</span> upward steps and <span class="math inline">\(n-k\)</span> downward steps, leaving us at position <span class="math inline">\(k - (n-k) = 2k - n\)</span>. Thus we have that
<span class="math display">\[\begin{equation} \label{bin}
  \mathbb P(X_n = 2k - n) = \mathbb P(Y_n = k) = \binom nk p^k q^{n-k} . %\tag{$**$}
  \end{equation}\]</span>
<p>Note that after an odd number of time steps <span class="math inline">\(n\)</span> we’re always at an odd-numbered state, since <span class="math inline">\(2k - \text{odd} = \text{odd}\)</span>, while after an even number of time steps <span class="math inline">\(n\)</span> we’re always at an even-numbered state, since <span class="math inline">\(2k - \text{even} = \text{even}\)</span>.</p>
<p>Writing <span class="math inline">\(i = 2k - n\)</span>, so <span class="math inline">\(k = (n+i)/2\)</span> and <span class="math inline">\(n-k = (n-i)/2\)</span>, we can rearrange  to see that the distribution for the simple random walk is <span class="math display">\[ \mathbb P(X_n = i) = \binom{n}{(n+i)/2} p^{(n+i)/2} q^{n - (n+i)/2} = \binom{n}{(n+i)/2} p^{(n+i)/2} q^{(n-i)/2} , \]</span> when <span class="math inline">\(n\)</span> and <span class="math inline">\(i\)</span> have the same parity with <span class="math inline">\(-n \leq i \leq n\)</span>, and is <span class="math inline">\(0\)</span> otherwise.</p>
<p>In the special case of the simple symmetric random walk, we have <span class="math display">\[ \mathbb P(X_n = i) = \binom{n}{(n+i)/2} \left(\frac12\right)^{(n+i)/2} \left(\frac12\right)^{(n-i)/2} = \binom{n}{(n+i)/2} 2^{-n} . \]</span></p>


<p>%</p>

<p>Consider the following gambling problem. Alice is gambling against Bob. Alice starts with <span class="math inline">\(\pounds a\)</span> and Bob starts with <span class="math inline">\(\pounds b\)</span>. It will be convenient to write <span class="math inline">\(m = a + b\)</span> for the total amount of money, so Bob starts with <span class="math inline">\(\pounds (m-a)\)</span>. At each step of the game, both players bet <span class="math inline">\(\pounds 1\)</span>; Alice wins <span class="math inline">\(\pounds 1\)</span> off Bob with probability <span class="math inline">\(p\)</span>, or Bob wins <span class="math inline">\(\pounds 1\)</span> off Alice with probability <span class="math inline">\(q\)</span>. The game continues until one player is out of money.</p>
<p>Let <span class="math inline">\(X_n\)</span> denote how much money Alice has after <span class="math inline">\(n\)</span> steps of the game. We can write this as a stochastic process with discrete time <span class="math inline">\(n = \{0,1,2,\dots\} = \mathbb Z_+\)</span> and discrete state space <span class="math inline">\(\mathcal S = \{0,1,\dots,m\}\)</span>, where <span class="math inline">\(X_0 = a\)</span> and, for <span class="math inline">\(n \geq 0\)</span>, we have <span class="math display">\[ X_{n+1} = \begin{cases} X_n + 1 &amp; \text{with probability $p$ if $1\leq X_n \leq m-1$,} \\
                           X_n - 1 &amp; \text{with probability $q$ if $1\leq X_n \leq m-1$,} \\
                           0       &amp; \text{if $X_n = 0$,} \\
                           m       &amp; \text{if $X_n = m$.} \end{cases} \]</span> Note also that the gambler’s ruin process <span class="math inline">\((X_n)\)</span> clearly satisfies the Markov property: the next step <span class="math inline">\(X_{n+1}\)</span> depends on where we are now <span class="math inline">\(X_n\)</span>, but, given that, does not depend on how we got here.</p>
<p>The gambler’s ruin process is exactly like a simple random walk started from <span class="math inline">\(X_0 = a\)</span> except that we have  and <span class="math inline">\(0\)</span> and <span class="math inline">\(m\)</span>, where the game stops because one of the players has `ruined’ – that is, lost all their money. (One can also consider random walks with , that bounce the random walk back into the state space, or  that are absorbing or reflecting at random.)</p>

<p>The gambling game continues until either Alice is ruined (<span class="math inline">\(X_n = 0\)</span>) or Bob is ruined (<span class="math inline">\(X_n = m\)</span>). A natural question to ask is: What is the probability that the game ends in Alice’s ruin?</p>
<p>Let us write <span class="math inline">\(r_i\)</span> for the probability Alice end up ruined if she has <span class="math inline">\(\pounds i\)</span>. Then the probability of ruin for the whole game is <span class="math inline">\(r_a\)</span>, since Alice initially starts with <span class="math inline">\(\pounds a\)</span>. The probability Bob is ruined is <span class="math inline">\(1 - r_a\)</span>, since eventually one of the players must lose.</p>
<p>What can we say about <span class="math inline">\(r_i\)</span>? Clearly we have <span class="math inline">\(r_0 = 1\)</span> and <span class="math inline">\(r_m = 0\)</span>, since this means Alice (<span class="math inline">\(i=0\)</span>) or Bob (<span class="math inline">\(i=m\)</span>) is out of money and is ruined. What about for <span class="math inline">\(1 \leq i \leq m-1\)</span>?</p>
The key is to . That is, we can write
<span class="math display">\[\begin{align*}
\mathbb P(\text{ruin}) &amp;= \mathbb P(\text{win first round}) \, \mathbb P(\text{ruin} \mid \text{win first round}) \\
&amp;\qquad{}+ \mathbb P(\text{lose first round}) \, \mathbb P(\text{ruin} \mid \text{lose first round}) \\
&amp;= p\,\mathbb P(\text{ruin} \mid \text{win first round}) + q \,\mathbb P(\text{ruin} \mid \text{lose first round}) .
\end{align*}\]</span>
<p>Here we have conditioned on whether Alice wins or loses the first round. More formally, we have used the , which says that if the disjoint events <span class="math inline">\(B_1, \dots, B_k\)</span> cover the whole sample space, then <span class="math display">\[ \mathbb P(A) = \sum_{i=1}^k \mathbb P(A \cap B_i) = \sum_{i=1}^k \mathbb P(B_i) \, \mathbb P(A \mid B_i) . \]</span> This idea of conditioning on the first step will be a crucial tool throughout this module.</p>
<p>Note that if Alice wins the first round from having <span class="math inline">\(\pounds i\)</span>, she now has <span class="math inline">\(\pounds (i+1)\)</span>. By the Markov property, we now see that her probability of ruin is <span class="math inline">\(r_{i+1}\)</span>, because it’s as if the game were starting again with Alice having <span class="math inline">\(\pounds (i+1)\)</span> to start with. The Markov property tells us that it doesn’t matter  Alice got to having <span class="math inline">\(\pounds (i+1)\)</span>, it only matters how much she has now. Similarly, if Alice loses the first round, she now has <span class="math inline">\(\pounds (i-1)\)</span>, and the ruin probability is <span class="math inline">\(r_{i-1}\)</span>. Hence we have <span class="math display">\[ r_i = pr_{i+1} + qr_{i-1}. \]</span></p>
<p>Rearranging, and including the , we see that the equation we want to solve is <span class="math display">\[ pr_{i+1} - r_i + qr_{i-1} = 0 \qquad \text{subject to} \qquad r_0 = 1,\ r_m = 0. \]</span> This is a  – and, because the left-hand side is <span class="math inline">\(0\)</span>, we call it a  linear difference equation. We will see how to solve this equation in the next lecture. We will see that, if we set <span class="math inline">\(\rho = q/p\)</span>, then the ruin probability is given by <span class="math display">\[ r_a = \begin{cases} \displaystyle\frac{\rho^a - \rho^m}{1 - \rho^m} &amp; \text{if $\rho \neq 1$,} \\[0.35cm]
           1 - \displaystyle\frac{a}{m} &amp; \text{if $\rho = 1$.} \end{cases} \]</span> Note that <span class="math inline">\(\rho = 1\)</span> is the same as the condition <span class="math inline">\(p = q = 1/2\)</span>.</p>
<p>Imagine Alice is not playing against her opponent Bob, but rather is up against a large casino. In this case, the casino’s capital <span class="math inline">\(\pounds (m-a)\)</span> is typically much bigger than Alice’s <span class="math inline">\(\pounds a\)</span>. We can model this by keeping <span class="math inline">\(a\)</span> fixed taking a limit <span class="math inline">\(m \to \infty\)</span>. Typically, the casino has `an edge’, meaning that <span class="math inline">\(q &gt; p\)</span>, so <span class="math inline">\(\rho &gt; 1\)</span>. In this case, we see that the ruin probability is <span class="math display">\[ \lim_{m \to \infty} r_a = \lim_{m \to \infty} \frac{\rho^a - \rho^m}{1 - \rho^m} = \lim_{m \to \infty} \frac{\rho^a/\rho^m - 1}{1/\rho^m - 1} = \frac{0-1}{0-1} = 1, \]</span> so Alice will be ruined with certainty.</p>
<p>Even with a generous casino that offers an exactly fair game with <span class="math inline">\(p = q\)</span>, so <span class="math inline">\(\rho = 1\)</span>, we have <span class="math display">\[ \lim_{m \to \infty} r_a = \lim_{m \to \infty}\left( 1 - \frac{a}{m} \right) = 1-0 = 1 , \]</span> so, even with this fair game, Alice will still certainly be ruined.</p>

<p>We could also ask for how long we expect the game to last.</p>
We approach this like before. Let <span class="math inline">\(d_i\)</span> be the expected duration of the game when Alice has <span class="math inline">\(\pounds i\)</span>. Our boundary conditions are <span class="math inline">\(d_0 = d_m = 0\)</span>, because <span class="math inline">\(X_n = 0\)</span> or <span class="math inline">\(m\)</span> means that the game is over. Again, we proceed by conditioning on the first step, so
<span class="math display">\[\begin{align*}
\mathbb E(\text{duration}) &amp;= \mathbb P(\text{win first round}) \, \mathbb E(\text{duration} \mid \text{win first round}) \\
&amp;\qquad{}+ \mathbb P(\text{lose first round}) \, \mathbb E(\text{duration} \mid \text{lose first round}) \\
&amp;= p\,\mathbb E(\text{duration} \mid \text{win first round}) + q \,\mathbb E(\text{duration} \mid \text{lose first round}) .
\end{align*}\]</span>
<p>More formally, we’ve used another version of the law of total probability, <span class="math display">\[ \mathbb E(X) = \sum_{i=1}^k \mathbb P(B_i) \, \mathbb E(X \mid B_i) , \]</span> or, alternatively, the  for expectations <span class="math display">\[ \mathbb E(X) = \mathbb E_Y \mathbb E (X \mid Y) = \sum_{y} \mathbb P(Y= y)\, E(X \mid Y = y)\]</span></p>
<p>Now, the expected duration given we win the first round is <span class="math inline">\(1 + d_{i+1}\)</span>. This is because the round itself takes <span class="math inline">\(1\)</span> time step, and then by the Markov property, it’s as if we are starting again from <span class="math inline">\(i+1\)</span>. Similarly, the expected duration given we lose the first round is <span class="math inline">\(1 + d_{i-1}\)</span>. Thus we have <span class="math display">\[ d_i = p(1 + d_{i+1}) + q (1 + d_{i-1}) = 1 + pd_{i+1} + qd_{i-1} . \]</span></p>
<p>Rearranging, and including the boundary conditions, we have another linear difference equation: <span class="math display">\[ pd_{i+1} - d_i + qd_{i-1} = -1 \qquad \text{subject to} \qquad d_0 = 0,\ d_m = 0. \]</span> Because the right-hand side, <span class="math inline">\(-1\)</span>, is nonzero, we call this an  linear difference equation. Again, we’ll see how to solve this in the next lecture, and will find that the solution is given by <span class="math display">\[ d_a = \begin{cases} {\displaystyle \frac{1}{q-p} \left(a - m\frac{1-\rho^a}{1- \rho^m} \right)} &amp; \text{if $\rho \neq 1$,} \\[0.35cm]
\displaystyle a(m-a) &amp; \text{if $\rho = 1$.} \end{cases} \]</span></p>
<p>Thinking again of playing against the casino, with <span class="math inline">\(q &gt; p\)</span>, <span class="math inline">\(\rho &gt; 1\)</span>, and <span class="math inline">\(m \to \infty\)</span>, we see that the expected duration is <span class="math display">\[ \lim_{m\to\infty} d_a = \lim_{m\to\infty} \frac{1}{q-p} \left(a - m\frac{1-\rho^a}{1 - \rho^m} \right)  = \frac{1}{q-p} \left(a - 0 \right) = \frac{a}{q-p} , \]</span> since <span class="math inline">\(\rho^m\)</span> grows much quicker than <span class="math inline">\(m\)</span>. So Alice ruins with certainty, and will take time <span class="math inline">\(a/(q-p)\)</span>, on average.</p>
<p>In the case of the generous casino, though, with <span class="math inline">\(q = p\)</span>, so <span class="math inline">\(\rho = 1\)</span>, we have <span class="math display">\[ \lim_{m\to\infty} d_a =  \lim_{m\to\infty} a(m-a) = \infty .  \]</span> So here, Alice will ruin with certainty, but it may take a very long time until the ruin occurs, since the average duration is infinite.</p>



 are equations that look like
<span class="math display">\[\begin{equation} \label{lde} a_k x_{n+k} + a_{k-1} x_{n+k-1} + \cdots + a_1 x_{n+1} + a_0 x_n = f(n)  \end{equation}\]</span>
<p>for <span class="math inline">\(n = 0,1,\dots\)</span>, where the <span class="math inline">\(a_i\)</span> are given constants, <span class="math inline">\(f(n)\)</span> is a given function, and we want to solve for the sequence <span class="math inline">\((x_n)\)</span>. The equation normally comes with some extra conditions, such as the value of the first few <span class="math inline">\(x_n\)</span>s.</p>
<p>When the right-hand side of  is zero, so <span class="math inline">\(f(n) = 0\)</span>, we say the equation is ; if the right-hand side is nonzero, it is . The number <span class="math inline">\(k\)</span>, where there are <span class="math inline">\(k+1\)</span> terms on the left-hand side, is called the  of the equation; we are mostly interested in second-degree linear difference equations.</p>
<p>A handout, , summarises the techniques we will learn in this lecture, and includes some extra examples.</p>

<p>We start with the homogeneous case, which is simpler.</p>
<p>Consider a homogeneous linear difference equation. We shall use the second-degree example <span class="math display">\[ x_{n+2} - 5x_{n+1} + 6x_{n} = 0 \qquad \text{subject to } x_0 = 4, x_1 = 9 .  \]</span> Here, the conditions on <span class="math inline">\(x_0\)</span> and <span class="math inline">\(x_1\)</span> are , because they tell us how the sequence <span class="math inline">\((x_n)\)</span> starts.</p>
For the moment, we shall put the initial conditions to the side and just worry about the equation <span class="math display">\[ x_{n+2} - 5x_{n+1} + 6x_{n} = 0 . \]</span> We start by guessing there might be a solution of the form <span class="math inline">\(x_n = \lambda^n\)</span> for some constant <span class="math inline">\(\lambda\)</span>. We can find out if there is such a solution by substituting in <span class="math inline">\(x_n = \lambda^n\)</span>, and seeing if there exists a solution for <span class="math inline">\(\lambda\)</span>. For our example, we get <span class="math display">\[ \lambda^{n+2} - 5 \lambda^{n+1} + 6\lambda^n = 0 . \]</span> After cancelling off a common factor of <span class="math inline">\(\lambda^n\)</span>, we get <span class="math display">\[ \lambda^2 - 5 \lambda + 6 = 0 . \]</span> This is called the . For a general homogeneous linear difference equation , the characteristic equation is
<span class="math display">\[\begin{equation} \label {cheq} a_k \lambda^{k} + a_{k-1} \lambda^{k-1} + \cdots + a_1 \lambda + a_0 = 0 . \end{equation}\]</span>
<p>We can now solve the characteristic equation for <span class="math inline">\(\lambda\)</span>. In our example, we can factor the left-hand side to <span class="math inline">\((\lambda - 3)(\lambda - 2) = 0\)</span>, to find the solutions <span class="math inline">\(\lambda = 2\)</span> and <span class="math inline">\(\lambda = 3\)</span>. Thus <span class="math inline">\(x_n = 2^n\)</span> and <span class="math inline">\(x_n = 3^n\)</span> both solve our equation. In fact, since the right-hand side of the equation is <span class="math inline">\(0\)</span>, any linear combination of these two solutions is a solution also, thus we get the  <span class="math display">\[ x_n = A 2^n + B 3^n , \]</span> which is a solution for any values of the constants <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>For a general characteristic equation with distinct roots <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_k\)</span>, the general solution is <span class="math display">\[ x_n = C_1 \lambda_1^n + C_2 \lambda_2^n + \cdots + C_n \lambda_k^n . \]</span> If we have a repeated root – say, <span class="math inline">\(\lambda_1 = \lambda_2 = \cdots = \lambda_r\)</span> is repeated <span class="math inline">\(r\)</span> times – than you can check that a solution is given by <span class="math display">\[ x_n = (D_0 + D_1 n + \cdots + D_{r-1} n^{r-1}) \lambda_1^n , \]</span> which should take its place in the general solution.</p>
Once we have the general solution, we can use the extra conditions to find the values of the constants. In our example, we can use the initial conditions to find out the values of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. We see that
<span class="math display">\[\begin{gather*}
x_0 = A2^0 + B3^0 = A + B = 4 , \\
x_1 = A2^1 + B3^1 = 2A + 3B = 9 .
\end{gather*}\]</span>
<p>We can now solve this pair of simultaneous equations to solve for <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. By subtracting twice the first equation from the second we get <span class="math inline">\(B = 1\)</span>, and substituting this into the first equation we get <span class="math inline">\(A = 3\)</span>. Thus the solution is <span class="math display">\[ x_n = 3\cdot 2^n + 3^n . \]</span></p>
In conclusion, the process here was:


<p>In the last lecture we saw that probability of ruin for the gambler’s ruin process is the solution to <span class="math display">\[ pr_{i+1} - r_i + qr_{i-1} = 0 \qquad \text{subject to} \qquad r_0 = 1,\ r_m = 0 , \]</span> where the extra conditions here are .</p>
<p>The characteristic equation is <span class="math display">\[ p\lambda^2 - \lambda + q = 0 .\]</span> We can solve the characteristic equation by factorising it as <span class="math inline">\((p \lambda - q)(\lambda - 1) = 0\)</span>. (It might take a moment to check this – we’ve used that <span class="math inline">\(p+q=1\)</span>.) So the characteristic equation has roots <span class="math inline">\(\lambda = q/p\)</span>, which we called <span class="math inline">\(\rho\)</span> last time, and <span class="math inline">\(\lambda = 1\)</span>. Note that if <span class="math inline">\(\rho = 1\)</span> (so <span class="math inline">\(p = q = 1/2\)</span>) we have a repeated root, while if <span class="math inline">\(\rho \neq 1\)</span> we have distinct roots, so we’ll need to deal with the two cases separately.</p>
<p>First, the case <span class="math inline">\(\rho \neq 0\)</span>. Since the two roots are distinct, we have the general solution <span class="math display">\[ r_i = A\rho^i + B1^i = A\rho^i + B . \]</span></p>
We can now use the boundary conditions to find <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. We have
<span class="math display">\[\begin{gather*} r_0 = A \rho^0 + B = A+B = 1, \\
                r_m = A \rho^m + B = 0 . \end{gather*}\]</span>
<p>From the first we get <span class="math inline">\(B = 1-A\)</span>, which we substitute into the second to get <span class="math display">\[ A\rho^m + 1 - A = 0 \quad \Rightarrow \quad A = \frac{1}{1-\rho^m} , \]</span> and hence <span class="math display">\[ B = 1 - A = 1 - \frac{1}{1-\rho^m} = - \frac{\rho^m}{1 - \rho^m} . \]</span> Thus the solution is <span class="math display">\[ r_i = \frac{1}{1-\rho^m} \rho^i -  \frac{\rho^m}{1 - \rho^m} = \frac{\rho^i - \rho^m}{1 - \rho^m}  , \]</span> as we claimed last time.</p>
<p>Second, the case <span class="math inline">\(\rho = 1\)</span>. Now we have a repeated root <span class="math inline">\(\lambda = 1\)</span>, so the general solution is <span class="math display">\[ r_i = (A + Bi) 1^i = A+Bi . \]</span></p>
Again, we use the boundary conditions, to get
<span class="math display">\[\begin{gather*} r_0 = A + B\cdot 0 = A = 1, \\
r_m = A + Bm = 0 , \end{gather*}\]</span>
<p>and we immediately see that <span class="math inline">\(A = 1\)</span> and <span class="math inline">\(B = -1/m\)</span>. Thus the solution is <span class="math display">\[ r_i = 1 - \frac{1}{m}i = 1 - \frac{i}{m} , \]</span> as claimed.</p>

Solving inhomogeneous linear difference equations requires three steps:

<p>This idea works because adding a solution to the homogeneous equation to the left-hand side adds zero to the right-hand side.</p>
<p>Let’s work through the example <span class="math display">\[ x_{n+2} - 5x_{n+1} + 6x_{n} = 2 \qquad \text{subject to } x_0 = 4, x_1 = 9 . \]</span></p>
<p>We already know from earlier that the general solution to the homogeneous equation <span class="math inline">\(x_{n+2} - 5x_{n+1} + 6x_{n} - 0\)</span> (with a zero on the right-hand side) is <span class="math display">\[ x_n = A2^n + B3^n . \]</span></p>
<p>We now need to find a  – that is, any solution – to our new inhomogeneous equation. A general hint here is to guess a solution with the same `shape’ as the right-hand side. For example, if the right-hand side is a polynomial of degree <span class="math inline">\(d\)</span>, try a polynomial of degree <span class="math inline">\(d\)</span>. Here our right-hand side is a constant (<span class="math inline">\(2\)</span>), so we should try a constant. Substituting in <span class="math inline">\(x_n = C\)</span> for all <span class="math inline">\(n\)</span> gives us <span class="math inline">\(C - 5C + 6C = 2\)</span>, thus <span class="math inline">\(2C = 2\)</span> and <span class="math inline">\(C = 1\)</span>, giving a particular solution <span class="math inline">\(x_n = 1\)</span>. The general solution to the inhomogeneous equation is therefore <span class="math display">\[ x_n = 1 + A2^n + B3^n . \]</span></p>
Again, we use the initial conditions to get the constants <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. We have
<span class="math display">\[\begin{gather*}
x_0 = 1 + A2^0 + B3^0 = 1+ A + B = 4 , \\
x_1 = 1 + A2^1 + B3^1 = 1+ 2A + 3B = 9 ,
\end{gather*}\]</span>
<p>and we can easily check this gives <span class="math inline">\(A = 1, B = 2\)</span>. Thus the solution is <span class="math display">\[ x_n = 1 + 1\cdot 2^n + 2 \cdot 3^n = 1 + 2^n + 2 \cdot 3^n . \]</span></p>

<p>From last time, the expected duration of the gambler’s ruin game solves <span class="math display">\[ pd_{i+1} - d_i + qd_{i-1} = -1 \qquad \text{subject to} \qquad d_0 = 0,\ d_m = 0. \]</span> As before, we divide cases based on whether or not <span class="math inline">\(\rho = 1\)</span>.</p>
<p>First, the case <span class="math inline">\(\rho \neq 1\)</span>. We already know that the general solution to the homogeneous equation is <span class="math display">\[ d_i =  A \rho^i + B . \]</span></p>
Now we need a particular solution. It’s tempting to guess a constant <span class="math inline">\(C\)</span> for a particular solution, but we know that constants solve the homogeneous equation, so will have right-hand side <span class="math inline">\(0\)</span>, not <span class="math inline">\(-1\)</span>. The next best guess is one degree up: let’s try <span class="math inline">\(x_i = Ci\)</span>. This gives
<span class="math display">\[\begin{align*}
  -1 &amp;= pC(i+1) - Ci + qC(i-1)\\
     &amp;= C(pi + p - i + qi - q) \\
     &amp;= C\big((p+q-1)i + (p-q)\big) \\
     &amp;= C(p-q) ,
  \end{align*}\]</span>
<p>giving <span class="math inline">\(C = -1/(p-q) = 1/(q-p)\)</span>. The general solution to the inhomogeneous equation is <span class="math display">\[ d_i = \frac{i}{q-p} + A \rho^i + B .  \]</span></p>
Then to find the constants, we have
<span class="math display">\[\begin{gather*} r_0 = \frac{0}{q-p} + A \rho^0 + B = A+B = 0, \\
                  r_m = \frac{m}{q-p} + A \rho^m + B = 0 , \end{gather*}\]</span>
<p>which gives <span class="math display">\[ A = -B = \frac{1}{q-p} \cdot \frac{m}{1 - \rho^m} . \]</span> The solution is <span class="math display">\[ d_i = \frac{i}{q-p} + \frac{1}{q-p} \cdot \frac{m}{1 - \rho^m} \rho^i - \frac{1}{q-p} \cdot \frac{m}{1 - \rho^m} =  \frac{1}{q-p} \left(i - m\frac{1-\rho^i}{1- \rho^m} \right) . \]</span></p>
<p>Second, the case <span class="math inline">\(\rho = 1\)</span>, so <span class="math inline">\(p = q = 1/2\)</span>. We already know that the general solution to the homogeneous equation is <span class="math display">\[ d_i =  A + Bi . \]</span></p>
We need a particular solution. Since both constants and linear terms solve the homogeneous equation, we’ll have to go up another degree and try <span class="math inline">\(x_i = Ci^2\)</span>. This gives
<span class="math display">\[\begin{align*}
    -1 &amp;= \frac12 C(i+1)^2 - Ci^2 + \frac 12 C(i-1)^2 \\
       &amp;= \frac12 C(i^2 + 2i + 1 - 2i^2 + i^2 - 2i + 1) \\
       &amp;= \frac12 C\big((1-2+1)i^2 + (2-2)i + (1+1)\big) \\
       &amp;=C ,
\end{align*}\]</span>
<p>so the general solution to the inhomogeneous equation is <span class="math display">\[ d_i = -i^2 + A + Bi .  \]</span></p>
Then to find the constants, we have
<span class="math display">\[\begin{gather*} d_0 = -0i^2 + A + B\cdot0 = A = 0, \\
                  d_m = -m^2 + A + Bm = 0 , \end{gather*}\]</span>
<p>giving <span class="math inline">\(A = 0, B = m\)</span>. The solution is <span class="math display">\[ d_i = -i^2 + 0 + mi = i(m-i) .\]</span></p>

<p> </p>
<pre><code>\addcontentsline{toc}{subsection}{\emph{Handout: How to solve linear difference equations}}

\begin{center}
    { \texttt{MATH2750} Introduction to Markov Processes (2018--19)}
    
    \vspace{18pt}
    
    {\LARGE \textbf{How to solve \\[0.2cm] linear difference equations}}
    
    \vspace{16pt}</code></pre>
<p>\end{center}</p>
<p>This handout accompanies . We summarise the techniques from the lecture, and give some extra examples.</p>

<p> are of the form <span class="math display">\[  a_k x_{n+k} + a_{k-1} x_{n+k-1} + \cdots + a_1 x_{n+1} + a_0 x_n = 0 \]</span> with <span class="math inline">\(0\)</span> on the right hand side. Here, the <span class="math inline">\((a_i)\)</span> are constants, and we want to solve for <span class="math inline">\((x_n)\)</span>. We are given extra conditions, such as initial conditions (the values of <span class="math inline">\(x_0, x_1, \dots\)</span>).</p>

<p> </p>

<p>The characteristic equation is <span class="math display">\[  a_k \lambda^{k} + a_{k-1} \lambda^{k-1} + \cdots + a_1 \lambda + a_0 = 0 .\]</span></p>
<p>If the solutions <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_k\)</span> are distinct, then the general solution is <span class="math display">\[ x_n = C_1 \lambda_1^n + C_2 \lambda_2^n + \cdots + C_k\lambda_k^n , \]</span> for constants <span class="math inline">\(C_1, C_2, \dots, C_k\)</span>.</p>
<p>If we have a repeated root – say, <span class="math inline">\(\lambda_1 = \lambda_2 = \cdots = \lambda_r\)</span> is repeated <span class="math inline">\(r\)</span> times – than a solution is given by <span class="math display">\[ x_n = (D_0 + D_1 n + \cdots + D_{r-1} n^{r-1}) \lambda_1^n , \]</span> which should take its place in the general solution.</p>

<p> </p>

<p>The extra conditions can be substituted into the general solution. This gives a number of simultaneous equations which can be solved to find the values of the constants <span class="math inline">\(C_1, C_2, \dots, C_k\)</span>.</p>
<p>%</p>
<p>  <span class="math display">\[ x_{n+2} - x_{n+1} - 6x_n = 0 \qquad \text{\emph{subject to} \quad $x_0 = 3$,\quad $x_1 = 4$.} \]</span></p>

<p> The characteristic equation is <span class="math display">\[ \lambda^2 - \lambda - 6 = 0 . \]</span> We can solve this by factorising it as <span class="math inline">\((\lambda - 3) (\lambda + 2) = 0\)</span>, to find the solutions <span class="math inline">\(\lambda_1 = -2\)</span> and <span class="math inline">\(\lambda_2 = 3\)</span>. Thus the general solution is <span class="math display">\[ x_n = A(-2)^n + B3^n . \]</span> (We’ve used the more convenient <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> rather than <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>.)</p>

 Substituting the initial conditions into the general solution, we have
<span class="math display">\[\begin{align*}
x_0 &amp;= A(-2)^0 + B3^0 = A + B = 3 \\
x_1 &amp;= A(-2)^1 + B3^1 = -2A + 3B = 4 .
\end{align*}\]</span>
<p>We can add twice the first equation from the second to get <span class="math inline">\(5B = 10\)</span>, so <span class="math inline">\(B=2\)</span>. We can substitute this into the first equation to get <span class="math inline">\(A = 1\)</span>.</p>
<p>The solution is therefore <span class="math display">\[ x_n = 1\cdot(-2)^n + 2 \cdot 3^n = \cdot(-2)^n + 2 \cdot 3^n . \]</span></p>

<p>  <span class="math display">\[ x_{n+2} + 4x_{n+1} +4x_n = 0 \qquad \text{\emph{subject to} \quad $x_0 = 2$,\quad $x_1 = -6$.} \]</span></p>

<p> The characteristic equation is <span class="math display">\[ \lambda^2 + 4\lambda + 4 = 0 . \]</span> We can solve this by factorising it as <span class="math inline">\((\lambda + 2)^2 = 0\)</span>, to find a repeated root <span class="math inline">\(\lambda_1 = \lambda_2 = -2\)</span>. Thus the general solution is <span class="math display">\[ x_n = (A + Bn) (-2)^n . \]</span></p>

 Substituting the initial conditions into the general solution, we have
<span class="math display">\[\begin{align*}
x_0 &amp;= (A + B0)(-2)^0 = A = 2 \\
x_1 &amp;= (A + B1)(-2)^1 = -2A - 2B = -6 .
\end{align*}\]</span>
<p>The first immediately gives <span class="math inline">\(A = 2\)</span>, and substituting this into the second equation gives <span class="math inline">\(B = 1\)</span>.</p>
<p>The solution is therefore <span class="math display">\[ x_n = (2 + n)(-2)^n . \]</span></p>


<p> are of the form <span class="math display">\[  a_k x_{n+k} + a_{k-1} x_{n+k-1} + \cdots + a_1 x_{n+1} + a_0 x_n = f(n) \]</span> with a nonzero right hand side <span class="math inline">\(f(n)\)</span>. As before, we are given extra conditions, such as initial conditions (the values of <span class="math inline">\(x_0, x_1, \dots\)</span>).</p>
<p>  </p>
<p>As before, the characteristic equation is <span class="math display">\[  a_k \lambda^{k} + a_{k-1} \lambda^{k-1} + \cdots + a_1 \lambda + a_0 = 0 .\]</span> We form the general solution to the homogeneous equation in the usual way: if the solutions <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_k\)</span> are distinct, then the general solution is <span class="math display">\[ x_n = C_1 \lambda_1^n + C_2 \lambda_2^n + \cdots + C_k\lambda_k^n , \]</span> for constants <span class="math inline">\(C_1, C_2, \dots, C_k\)</span>, while we require adjustments for repeated roots.</p>
<p>  </p>
A <code>particular solution' is any solution of the inhomogeneous equation. We can usually find such a solution by guessing the</code>shape’ of the solution to be the same as the right hand side <span class="math inline">\(f(n)\)</span>. A few suggestions are

<p>After guessing the shape, we can substitute the guess into the inhomogeneous linear difference equation to find the value of any constants in our guess.</p>
<p>  </p>
<p>As before, the extra conditions can be substituted into the general solution to give a number of simultaneous equations which can be solved to find the values of the constants <span class="math inline">\(C_1, C_2, \dots, C_k\)</span>.</p>

<p>  <span class="math display">\[ 10 x_{n+2} - 7x_{n+1} + x_n = 8 \qquad \text{\emph{subject to} \quad $x_0 = 0$,\quad $x_1 = \tfrac{13}{10} $.} \]</span></p>

<p> The characteristic equation is <span class="math display">\[ 10\lambda^2 - 7\lambda + 1 = 0 . \]</span> We can solve this by factorising it as <span class="math display">\[ (2\lambda - 1) (5\lambda - 1) = 0 , \]</span> to find the solutions <span class="math inline">\(\lambda_1 = \frac12\)</span> and <span class="math inline">\(\lambda_2 = \frac15\)</span>. Thus the general solution of the homogeneous equation is <span class="math display">\[ x_n = A\left(\frac12\right)^n + B\left(\frac15\right)^n . \]</span></p>

<p> Since the right hand side of the inhomogeneous equation is a constant, we guess a constant particular solution with shape <span class="math inline">\(x_n = C\)</span>. Substituting in this guess, we get <span class="math display">\[ 10C - 7C + C = 4C = 8 \]</span> with solution <span class="math inline">\(C=2\)</span>. Thus a particular solution is <span class="math inline">\(x_n = 2\)</span>, and the general solution to the inhomogeneous equation is <span class="math display">\[ x_n = 2 + A\left(\frac12\right)^n + B\left(\frac15\right)^n . \]</span></p>

 Substituting the initial conditions into the general solution, we have
<span class="math display">\[\begin{align*}
x_0 = 2 + A\left(\frac12\right)^0 + B\left(\frac15\right)^0 = 2 + A + B = 0 \quad &amp;\Rightarrow \quad A + B = -2 \\
x_1 = 2 + A\left(\frac12\right)^1 + B\left(\frac15\right)^1 = 2 + A\frac12 + B\frac15 = \frac{13}{10} \quad &amp;\Rightarrow \quad 5A + 2B = -7.
\end{align*}\]</span>
<p>We can subtract twice the first equation from the second to get <span class="math inline">\(3A = -3\)</span>, so <span class="math inline">\(A = -1\)</span>. We can substitute this into the second equation to get <span class="math inline">\(B = -1\)</span>.</p>
<p>The solution is therefore <span class="math display">\[ x_n = 2 - \left(\frac12\right)^n - \left(\frac15\right)^n . \]</span></p>



<p>So far we’ve seen a a few examples of stochastic processes in discrete time and discrete space with the Markov memoryless property. Now we will develop the theory more generally.</p>
<p>%Recall from Lecture 1 the definition of the Markov property: a stochastic process <span class="math inline">\((X_n)\)</span> has the Markov property if, for all times <span class="math inline">\(n\)</span> and all states <span class="math inline">\(i_0, i_1, \dots i_n i_{n+1} \in \mathcal S\)</span> we have %<span class="math display">\[     \mathbb P(X_{n+1}=i_{n+1} \mid X_{n}=i_{n}, \dots,X_1 = i_1, X_0=i_0)=\mathbb P(X_{n+1}=i_{n+1} \mid X_{n}=i_{n}) . \]</span></p>
<p>To define a so-called `Markov chain’, we first need to say where we start from, and second what the probabilities of transitions from one state to another are.</p>
<p>In our examples of the simple random walk and gambler’s ruin we specified the start point <span class="math inline">\(X_0\)</span> exactly, but we could pick the start point at random according to some distribution <span class="math inline">\(\lambda_i = \mathbb P(X_0 = i)\)</span>.</p>
<p>After that, we want to know the transition probabilities <span class="math inline">\(\mathbb P(X_{n+1} = j \mid X_n = i)\)</span> for <span class="math inline">\(i,j \in \mathcal S\)</span>. Note that, because of the Markov property, the transition probability only needs to condition the state we’re in now <span class="math inline">\(X_n = i\)</span>, and not on the whole history of the process.</p>
<p>%If we then want to know the probability that after the first step we are at state <span class="math inline">\(j\)</span>, we can write %<span class="math display">\[ \mathbb P(X_1 = j) = \sum_{i \in \mathcal S} \lambda_i \mathbb P(X_1 = j \mid X_0 = i) , \]</span> %by conditioning on the start point. Note that, given the <span class="math inline">\(\lambda_i\)</span>s, we now only need to know the transition probabilities <span class="math inline">\(\mathbb P(X_1 = j \mid X_0 = i)\)</span>.</p>
<p>%Continuing like this, we have for step <span class="math inline">\(2\)</span>, %<span class="math display">\[ \mathbb P(X_2 = k) = \sum_{i,j \in \mathcal S} \lambda_i \mathbb P(X_1 = j \mid X_0 = i) \mathbb P(X_2 = j \mid X_1 = k). \]</span> %Note that by the Markov property, the final term only depends on <span class="math inline">\(X_1\)</span> and not on <span class="math inline">\(X_0\)</span> as well.</p>
<p>%Thus, if we want to know about a discrete-time Markov process, all we need to know about is the initial distribution <span class="math inline">\(\lambda_i = \mathbb P(X_0 = i)\)</span> and the transition probabilities <span class="math inline">\(\mathbb P(X_{n+1} = j \mid X_n = i)\)</span>.</p>
<p>In the case of the simple random walk, for example, we had initial distribution <span class="math display">\[ \lambda_i = \mathbb P(X_0 = i) = \begin{cases} 1 &amp; \text{if $i = 0$} \\ 0 &amp; \text{otherwise} \end{cases} \]</span> and transition probabilities <span class="math display">\[ \mathbb P(X_{n+1} = j \mid X_n = i) = \begin{cases} p &amp; \text{if $j = i+1$} \\ q &amp; \text{if $j = i-1$} \\ 0 &amp; \text{otherwise.} \end{cases} \]</span> Note that in this case (and the case of the gambler’s ruin), the transition probabilities <span class="math inline">\(\mathbb P(X_{n+1} = j \mid X_n = i)\)</span> don’t depend on <span class="math inline">\(n\)</span>; in other words, the transition probabilities stay the same over time. A Markov process with this property is called , and we will always consider time homogeneous processes from now on.</p>
<p>Let’s write <span class="math inline">\(p_{ij} = \mathbb P(X_{n+1} = j \mid X_n = i)\)</span> for the transition probabilities, which are independent of <span class="math inline">\(n\)</span>. Note that we must have <span class="math inline">\(p_{ij} \geq 0\)</span>, since it is a probability, and <span class="math inline">\(\sum_j p_{ij} = 1\)</span>, as this is the sum of the probabilities of all the places you can move to from state <span class="math inline">\(i\)</span>.</p>

<p>When the state space is finite (and even sometimes when it’s not), it’s convenient to write the transition probabilities <span class="math inline">\((p_{ij})\)</span> as a matrix <span class="math inline">\(\mathsf P\)</span>, called the , whose <span class="math inline">\((i,j)\)</span>th entry is <span class="math inline">\(p_{ij}\)</span>. Then the condition that <span class="math inline">\(\sum_j p_{ij} = 1\)</span> is the condition that the rows of <span class="math inline">\(\mathsf P\)</span> add up to <span class="math inline">\(1\)</span>.</p>
<p>From this, we can calculate, for example, <span class="math display">\[ \mathbb P(X_1 = j, X_0 = i) = \lambda_i p_{ij} , \]</span> as we must start from <span class="math inline">\(i\)</span> and then move to <span class="math inline">\(j\)</span>. Another example would be <span class="math display">\[ \mathbb P(X_{n+2} = j, X_{n+1} = k \mid X_n = i) = p_{ik}p_{kj} , \]</span> as we must jump from <span class="math inline">\(i\)</span> to <span class="math inline">\(k\)</span>, then <span class="math inline">\(k\)</span> to <span class="math inline">\(j\)</span>.</p>

<p>Consider a simple two-state Markov chain with state space <span class="math inline">\(\mathcal S = \{0,1\}\)</span> and transition matrix <span class="math display">\[ \mathsf P = \begin{pmatrix} p_{00} &amp; p_{01} \\ p_{10} &amp; p_{11} \end{pmatrix} = \begin{pmatrix} 1-\alpha &amp; \alpha \\ \beta &amp; 1-\beta \end{pmatrix}  \]</span> for some <span class="math inline">\(0 \leq \alpha, \beta \leq 1\)</span>. Note that the rows of <span class="math inline">\(\mathsf P\)</span> add up to <span class="math inline">\(1\)</span>.</p>
<p>We can illustrate <span class="math inline">\(\mathsf P\)</span> by a , where the blobs are the states and the arrows give the transition probabilities. (We don’t draw the arrow if <span class="math inline">\(p_{ij} = 0\)</span>.) In this case, our transition diagram looks like this:</p>


<p>We can use this as a simple model of a broken printer, for example. If the printer is broken (state <span class="math inline">\(0\)</span>) on one day, then with probability <span class="math inline">\(\alpha\)</span> it will be fixed (state <span class="math inline">\(1\)</span>) by the next day; while if it is working (state <span class="math inline">\(1\)</span>), then with probability <span class="math inline">\(\beta\)</span> it will have broken down (state <span class="math inline">\(0\)</span>) by the next day.</p>


<p>In the above example, we calculated a two-step transition probability <span class="math inline">\(p^{(2)}_{ij} = \mathbb P (X_{n+2} = j \mid X_n = i)\)</span> by conditioning on the first step. That is, by considering all the possible intermediate steps <span class="math inline">\(k\)</span>, we have <span class="math display">\[ p^{(2)}_{ij} = \sum_{k\in\mathcal S} \mathbb P (X_{n+1} = k \mid X_n = i)\mathbb P (X_{n+2} = j \mid X_{n+1} = k) = \sum_{k\in\mathcal S} p_{ik}p_{kj} . \]</span></p>
<p>Note that this is exactly the formula for multiplying the matrix <span class="math inline">\(\mathsf P\)</span> with itself. In other words, <span class="math inline">\(p^{(2)}_{ij} = \sum_{k} p_{ik}p_{kj}\)</span> is the <span class="math inline">\((i,j)\)</span>th entry of the matrix <span class="math inline">\(\mathsf P^2 = \mathsf{PP}\)</span>. If we write <span class="math inline">\(\mathsf P^{(2)} = (p^{(2)}_{ij})\)</span> for the matrix of two-step transition probabilities, we have <span class="math inline">\(\mathsf P^{(2)} = \mathsf P^2\)</span>.</p>
<p>More generally, we see that this rule holds over multiple steps, provided we sum over all the possible paths <span class="math inline">\(i\to k_1 \to k_2 \to \cdots \to k_{n-1} \to j\)</span> of length <span class="math inline">\(n\)</span> from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>.</p>

<p>(If you want to be completely rigorous, try proving this by induction on <span class="math inline">\(n\)</span>.)</p>

<p>The so-called  follow immediately from this.</p>

In our two-state broken printer example above, the matrix two-state transition probabilities is given by
<span class="math display">\[\begin{align*}
\mathsf P^{(2)} = \mathsf P^2 &amp;=  \begin{pmatrix} 1-\alpha &amp; \alpha \\ \beta &amp; 1-\beta \end{pmatrix}  \begin{pmatrix} 1-\alpha &amp; \alpha \\ \beta &amp; 1-\beta \end{pmatrix} \\
&amp;=  \begin{pmatrix} (1-\alpha)^2 + \alpha\beta &amp; (1-\alpha)\alpha + \alpha(1-\beta) \\ \beta(1-\alpha) + (1-\beta)\beta &amp; \beta\alpha + (1-\beta)^2 \end{pmatrix} ,
\end{align*}\]</span>
<p>where the bottom right entry is what we calculated earlier.</p>
<p>It’s also convenient to consider the initial distribution <span class="math inline">\(\boldsymbol\lambda = (\lambda_i)\)</span> as a row vector. The first-step distribution is given by <span class="math display">\[ \mathbb P(X_1 = j) = \sum_{i \in \mathcal S} \lambda_i p_{ij} , \]</span> by conditioning on the start point. This is exactly the <span class="math inline">\(j\)</span>th element of the vector–matrix multiplication <span class="math inline">\(\boldsymbol\lambda \mathsf P\)</span>. More generally, the row vector of of probabilities after <span class="math inline">\(n\)</span> steps is given by <span class="math inline">\(\boldsymbol\lambda \mathsf P^n\)</span>.</p>



<p>In this lecture we’ll set up three simple models for an insurance company that can be analysed using ideas about Markov chains.</p>

A motor insurance company puts policy holders into three categories:

<p>New policy holders start with no discount (state <span class="math inline">\(0\)</span>). Following a year with no insurance claims, policy holders move up one level of discount. If they start the year in state <span class="math inline">\(2\)</span> and make no claim, they remain in state <span class="math inline">\(2\)</span>. Following a year with at least one claim, they move down one level of discount. If they start the year in state <span class="math inline">\(0\)</span> and make at least one claim, they remain in state <span class="math inline">\(0\)</span>. The insurance company believes that probability that a motorist has a claim free year is <span class="math inline">\(3/4\)</span>.</p>
We can model this directly as a Markov chain:


<p>The transition probability and transition diagram of the Markov chain are:</p>
<p><span class="math display">\[ \mathsf P = \begin{pmatrix} \frac14 &amp; \frac34 &amp; 0 \\[2px] \frac14 &amp; 0 &amp; \frac34 \\[2px] 0 &amp; \frac14 &amp; \frac34 \end{pmatrix} . \]</span></p>


<p>We want to find the <span class="math inline">\(3\)</span>-step transition probability <span class="math display">\[
p_{02}^{(3)} = \mathbb P(X_{3} = 2 \mid X_0=0) .
\]</span> We can find this by summing over all paths <span class="math inline">\(0 \to k_1 \to k_2 \to 2\)</span>. There are two such paths, <span class="math inline">\(0 \to 0 \to 1 \to 2\)</span> and <span class="math inline">\(0 \to 1 \to 2 \to 2\)</span>. Thus <span class="math display">\[ p_{02}^{(3)} = p_{00}p_{01}p_{12} + p_{01}p_{12}p_{22} = \frac14 \frac34 \frac34 + \frac34 \frac34 \frac34 = \frac{36}{64} = \frac{9}{16} . \]</span></p>
<p>Alternatively, we could directly calculate all the <span class="math inline">\(3\)</span>-step transition probabilities by the matrix method, to get <span class="math display">\[ \mathsf P^{(3)} = \mathsf P^3 = \mathsf{PPP} = \frac{1}{64} \begin{pmatrix} 7 &amp; 21 &amp; 36 \\ 7 &amp; 12 &amp; 45 \\ 4 &amp; 15 &amp; 45 \end{pmatrix} .\]</span> (You can check this yourself.) The desired <span class="math inline">\(p_{02}^{(3)}\)</span> is the top right entry <span class="math inline">\(36/64 = 9/16\)</span>. %</p>

<p>According to a different model, a motorist’s <span class="math inline">\(n\)</span>th year of driving is either accident free, or has exactly one accident. (The model does not allow for more than one accident in a year.) Let <span class="math inline">\(Y_n\)</span> be a random variable so that, <span class="math display">\[
Y_n=\begin{cases}
0&amp;\textnormal{ if the motorist has no accident,}\\
1&amp;\textnormal{ if the motorist has one accident.}
\end{cases}
\]</span> This defines a stochastic process <span class="math inline">\((Y_n)\)</span> with finite state space <span class="math inline">\(\mathcal{S}=\{0,1\}\)</span> and discrete time <span class="math inline">\(n = 1,2,3,\dots\)</span>.</p>
<p>The probability of an accident in year <span class="math inline">\(n+1\)</span> is modelled as a function of the total number of previous accidents over a function of the number of years in the policy; that is, <span class="math display">\[
\mathbb P(Y_{n+1}= 1 \mid Y_n=y_{n},\dots ,Y_2=y_{2},Y_1=y_{1} )=\frac{f(y_1+y_2+\cdots +y_n)}{g(n)},
\]</span> where <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are non-negative increasing functions with <span class="math inline">\(0\leq f(m)\leq g(m)\)</span> for all <span class="math inline">\(m\)</span>.</p>
<p>Unfortunately <span class="math inline">\((Y_n)\)</span> is  a Markov chain – it’s clear that <span class="math inline">\(Y_{n+1}\)</span> depends not only on <span class="math inline">\(Y_n\)</span>, the number accidents this year, but the entire history <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span>.</p>
However, we have a cunning work-around. Define <span class="math inline">\(X_n=\sum_{i=1}^n Y_i\)</span> to be the total number of accidents up to year <span class="math inline">\(n\)</span>. Then <span class="math inline">\((X_n)\)</span>  a Markov chain. In fact, we have
<span class="math display">\[\begin{align*}
    \mathbb P(X_{n+1}={}&amp;{}x_{n}+1\mid X_n=x_n, \dots, X_2=x_2, X_1=x_1)\\
    &amp;=\mathbb P(Y_{n+1}=y_{n+1}\mid Y_n=x_n - x_{n-1}, \dots Y_2=x_2-x_1, Y_1=x_1)\\
    &amp;=\frac{f\big((x_n-x_{n-1}) +\cdots +(x_2-x_1) + x_1\big)}{g(n)}\\
    &amp;=\frac{f(x_n)}{g(n)},
\end{align*}\]</span>
<p>which clearly depends only on <span class="math inline">\(x_n\)</span>. Thus we can use Markov chain techniques on <span class="math inline">\((X_n)\)</span> to lean about the non-Markov process <span class="math inline">\((Y_n)\)</span></p>
<p>Let’s think about the conditions we placed on this model. First, the condition <span class="math inline">\(0 \leq f(m) \leq g(m)\)</span> ensures that the transition probabilities are between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> (since <span class="math inline">\(X_n\)</span> is between <span class="math inline">\(0\)</span> and <span class="math inline">\(n\)</span>). Our probabilities should lie in the range <span class="math inline">\([0,1]\)</span>. Since <span class="math inline">\(\sum_{i=1}^n y_i\leq n\)</span>, this is where we use <span class="math inline">\(0\leq f(m)\leq g(m)\)</span>. Second, the condition that <span class="math inline">\(f\)</span> is increasing means that between drivers who have been driving the same number of years, we think the more accident-prone in the past is more likely to have an accident in the future. The condition that <span class="math inline">\(g\)</span> is increasing means that between drivers who have had the same number of accidents, we think the one who has spread those accidents over a longer period of time is less likely to have accidents in the future.</p>

<p>Sometimes, we are presented with a stochastic process which is not a Markov chain, but by altering the state space <span class="math inline">\(\mathcal{S}\)</span> we can sometimes end up with a process which is a Markov chain. As such, it is important to think carefully about choice of state space. To see this we will return to the no-claims discount example.</p>
<p>Suppose now we have an model with four levels of discount:</p>


<p>If a year is accident free then your discount increases one level, to a maximum of <span class="math inline">\(60\%\)</span>. If the year is not accident free, then the discount decreases, by one level if the year previous to that was accident free, but by two levels if the previous year was not accident free either. As before, the insurance company believes that probability that a motorist has a claim free year is <span class="math inline">\(3/4\)</span>.</p>
<p>We might consider the most natural example of a state space, where the states are discount levels; say, <span class="math inline">\(\mathcal{S}=\{0,1,2,3\}\)</span>. But this is not a Markov chain, since if a policy holder has an accident, we need to know about the past in order to determine probabilities for future states.</p>
However, we can be clever again, this time in the choice of our state space. Instead, let the following be our states:

<p>Now this is a Markov chain. Under the old assumption of <span class="math inline">\(25\%\)</span> of drivers having an accident each year, the transition matrix is <span class="math display">\[
\mathsf P=\begin{pmatrix}
0.25 &amp; 0.75 &amp; 0 &amp; 0 &amp; 0\\
0.25 &amp; 0 &amp; 0.75 &amp; 0 &amp; 0\\
0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0.75\\
0.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0.75\\
0 &amp; 0 &amp; 0 &amp; 0.25 &amp; 0.75\end{pmatrix}.
\]</span> The transition diagram is shown below. (We don’t draw arrows with probability~<span class="math inline">\(0\)</span>.)</p>




<p>If we have a large complicated Markov chain, it can be useful to split it up into smaller pieces that can be studied separately. The idea is to split to states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> into different pieces if we can’t move from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> and then back again after some number of steps.</p>

<p>Note that the condition <span class="math inline">\(p_{ij}^{(n)}&gt;0\)</span> means that, starting from <span class="math inline">\(i\)</span>, there’s a positive chance that we’ll get to <span class="math inline">\(j\)</span> at some point in the future – hence the term `accessible’.</p>


<p>A fact you may remember about equivalence relations is that an equivalence relation partitions the space <span class="math inline">\(\mathcal S\)</span> into equivalence classes. That is, every <span class="math inline">\(i\)</span> is in exactly one equivalence class, and that class is the set of <span class="math inline">\(j\)</span> such that <span class="math inline">\(i \leftrightarrow j\)</span>. In this context, we call these .</p>





<p>When we discussed the simple random walk, we noted that it alternates between even-numbered and odd-numbered states. This `periodic’ behaviour is important to understand if we want to know what will happen to the Markov chain in the future.</p>
<p>The idea is this: List the number of steps for all possible paths starting and ending in the state. Then the period is the greatest common divisor of the integers in this list.</p>




<p>%</p>
<p>You may have noticed in these examples that, within a communicating class, every state has the same period.</p>

<p>In particular, in an irreducible Markov chain, all states have the same period <span class="math inline">\(d\)</span>. We say that an irreducible Markov chain is  if <span class="math inline">\(d&gt;1\)</span> and  if <span class="math inline">\(d=1\)</span>.</p>




<p>In Lectures 3 and 4, we used conditioning on the first step to find the ruin probability and expected duration for the gambler’s ruin problem. Here, we develop those ideas for general Markov chains.</p>


<p>For the gambler’s ruin problem, we found equations for hitting probabilities and expected hitting times by conditioning on the first step and solving the resulting equations. We do the same here for other Markov chains.</p>

<p>Substituting the second equation into the first, we get the solution <span class="math inline">\(\frac45 h_{12} = \frac1{10} + \frac25\)</span>, so <span class="math inline">\(h_{12} = \frac38 = 0.375\)</span>.</p>
<p>It is recommended to derive equations for hitting probabilities from first principles by conditioning on the first step. However, we can state what the general formula is: we have <span class="math display">\[ h_{iA} = \begin{cases} \displaystyle\sum_{j \in \mathcal S} p_{ij} h_{jA} &amp; \text{if $i \not\in A$} \\
1 &amp; \text{if $i \in A$.} \end{cases} \]</span> It can be shown that, if these equations have multiple solutions, that the hitting probabilities are in fact the smallest non-negative solutions.</p>

<p>Similarly, we have a general formula <span class="math display">\[ \eta_{iA} = \begin{cases} 1 + \displaystyle\sum_{j \in \mathcal S} p_{ij} \eta_{jA} &amp; \text{if $i \not\in A$} \\
0 &amp; \text{if $i \in A$.} \end{cases} \]</span> Again, if we have multiple solutions, that the hitting probabilities are the smallest non-negative solutions.</p>
<p>Note that, under the definitions above, the hitting probability and time for a state to itself is always <span class="math inline">\(h_{ii} = 1\)</span> and <span class="math inline">\(\eta_{ii} = 0\)</span>, as we’re `already there’. In this case, it can be interesting to look instead at the random variable representing the return time, <span class="math display">\[ M_i = \min \big\{n \in \{1,2,\dots\} : X_n = i  \big\} . \]</span> We then have the  <span class="math display">\[ m_{i} = \mathbb P(X_n = i  \text{ for some $n \geq 1$} \mid X_0 = i) = \mathbb P(M_i &lt; \infty \mid X_0 = i) ,  \]</span> and  <span class="math display">\[ \mu_{i} = \mathbb E(M_i \mid X_0 = i) .  \]</span> Note that these only consider times <span class="math inline">\(n = 1, 2, \dots\)</span> not including <span class="math inline">\(n = 0\)</span>.</p>
<p>By conditioning on the first step, it’s clear that we have the equations <span class="math display">\[
      m_i = \sum_{j \in \mathcal S} p_{ij}h_{ji} , \qquad
      \mu_i = 1 + \sum_{j \in \mathcal S} p_{ij}\eta_{ji}.
  \]</span> We take the minimal non-negative solution again.</p>

<p>We now turn to hitting probabilities for the simple random walk, which goes up with probability <span class="math inline">\(p\)</span> and down with probability <span class="math inline">\(q = 1-p\)</span>. Without loss of generality, we look at <span class="math inline">\(h_{i0}\)</span>, the probability the random walk hits <span class="math inline">\(0\)</span> starting from <span class="math inline">\(i \in \mathbb Z\)</span>.</p>
<p>We start with the case that <span class="math inline">\(i \geq 0\)</span>. Clearly we have <span class="math inline">\(h_{00} = 1\)</span>. By conditioning on the first step, we have <span class="math display">\[ h_{i0} = ph_{i+1\, 0} + qh_{i-1\, 0} .  \]</span></p>

<p>We recognise this equation from the gambler’s ruin problem. When <span class="math inline">\(p \neq \frac12\)</span>, the general solution is <span class="math inline">\(h_{i0} = A + B\rho^i\)</span>, where <span class="math inline">\(\rho = q/p \neq 0\)</span>. The initial condition <span class="math inline">\(h_{00} = 1\)</span> gives <span class="math inline">\(A = 1-B\)</span>, so we have a family of solutions <span class="math inline">\(h_{i0} = 1 + B(\rho^i - 1)\)</span>.</p>
<p>In the gambler’s ruin problem we had another boundary condition to find <span class="math inline">\(A\)</span>. Here we have no other conditions, but we can use the minimality condition that the hitting probabilities are the smallest non-negative solution to the equation.</p>
<p>When <span class="math inline">\(\rho &gt; 1\)</span>, so <span class="math inline">\(p &lt; \frac12\)</span>, the term <span class="math inline">\(\rho^i\)</span> tends to infinity, so the minimal non-negative solution will have to take <span class="math inline">\(B = 0\)</span>, to get <span class="math inline">\(h_{i0} = 1\)</span>, meaning we hit <span class="math inline">\(0\)</span> with certainty.</p>
<p>When <span class="math inline">\(\rho &lt; 1\)</span>, so <span class="math inline">\(p &gt; \frac12\)</span>, we have that <span class="math inline">\(\rho^i - 1\)</span> is negative, so to get a small solution we want <span class="math inline">\(B\)</span> as large as possible. But keeping the solution non-negative limits us to <span class="math inline">\(B \leq 1\)</span>, so the minimality condition is achieved at <span class="math inline">\(B = 1\)</span>. The solution is <span class="math inline">\(h_{i0} = 1 + (\rho^i - 1) = \rho^i\)</span>.</p>
<p>For <span class="math inline">\(\rho = 1\)</span>, so <span class="math inline">\(p = \frac12\)</span>, we recall the general solution <span class="math inline">\(h_{i0} = A + Bi\)</span>, and the condition gives <span class="math inline">\(A = 1\)</span>. Minimality then requires <span class="math inline">\(B = 0\)</span>, so we have <span class="math inline">\(h_{i0} = 1\)</span>.</p>
<p>In conclusion, for <span class="math inline">\(i &gt; 0\)</span>, the hitting probabilities are given by <span class="math display">\[ h_{i0} = \begin{cases} \left(\displaystyle\frac{q}{p}\right)^i &amp; \text{if $p &gt; \frac12$} \\ \ 1 &amp; \text{if $p \leq \frac12$.} \end{cases} \]</span> For <span class="math inline">\(i &lt; 0\)</span>, we can get the result by swapping the role of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, and treating the positive value <span class="math inline">\(-i\)</span>.</p>
<p>What about the return time to <span class="math inline">\(0\)</span> (or, by symmetry, to any state <span class="math inline">\(i \in \mathbb Z\)</span>)? By conditioning on the first step, <span class="math inline">\(m_0 = ph_{1\,0} + qh_{-1\,0}\)</span>. We then have <span class="math display">\[ m_0 = \begin{cases} p + q = 1 &amp; \text{if $p = \frac12$} \\[0.5ex]
p \displaystyle\frac qp + q = 2q &lt; 1 &amp; \text{if $p &gt; \frac12$} \\[1ex]
p + q \displaystyle\frac pq = 2p &lt; 1 &amp; \text{if $p &lt; \frac12$.}\end{cases} \]</span> So for the simple symmetric random walk (<span class="math inline">\(p = \frac12\)</span>) we have <span class="math inline">\(m_i = 1\)</span> and are certain to return to <span class="math inline">\(0\)</span> again and again, while for <span class="math inline">\(p \neq \frac12\)</span>, we have <span class="math inline">\(m_i &lt; 1\)</span> and we might never return.</p>
<p>The expected hitting times <span class="math inline">\(\eta_{i0}\)</span> for <span class="math inline">\(i&gt;0\)</span> can be calculated too. For <span class="math inline">\(p &gt; \frac12\)</span> we have <span class="math inline">\(\eta_{i0} = \infty\)</span>, as the walk might not hit <span class="math inline">\(0\)</span> at all. For <span class="math inline">\(p &lt; \frac12\)</span> we have <span class="math display">\[ \eta_{i0} = 1 + p\eta_{i+1\,0} + q\eta_{i-1\,0} , \]</span> with <span class="math inline">\(\eta_{00} = 0\)</span>. This has solution <span class="math inline">\(\eta_{i0} = i/(q-p) + \rho^i(1-A)\)</span>. The minimality conditions require <span class="math inline">\(A = 1\)</span>, so <span class="math inline">\(\eta_{i0} = i/(q-p)\)</span>. When <span class="math inline">\(p = \frac12\)</span>, the solution to the equation is <span class="math inline">\(\eta_{i0} = i(B-i)\)</span>, and non-negativity demands <span class="math inline">\(B = \infty\)</span>, to give <span class="math inline">\(\eta_{i0} = \infty\)</span>.</p>
<p>It follows that the expected return time is <span class="math inline">\(\mu_0 = \infty\)</span> for all <span class="math inline">\(p\)</span>. For <span class="math inline">\(p = \frac12\)</span>, while we always return to <span class="math inline">\(0\)</span>, it may take a very long time.</p>



When thinking about the long-run behaviour of Markov chains, there are two different types of states:

<p>The official definition is as follows; we will show that other properties follow from this.</p>



<p>Note that here <span class="math inline">\(\sum_{n=1}^\infty p_{ii}^{(n)}\)</span> is the expected number of returns to <span class="math inline">\(i\)</span> starting from <span class="math inline">\(i\)</span>.</p>




<p>We could find whether each state is transient or recurrent by calculating (or bounding) all the return probabilities <span class="math inline">\(m_i\)</span>, using the methods for the last lecture. But the following two theorems will give some convenient short-cuts.</p>

<p>For this reason, we can refer to communicating classes as recurrent classes and transient classes. If a Markov chain is irreducible, we can refer to it as a recurrent or transient Markov chain.</p>


<p>This theorem completely categories the transience and recurrence of classes, with rare exception of infinite closed classes, which can require further examination.</p>

<p>Going back to the previous example, we see that the class <span class="math inline">\(\{5,6,7\}\)</span> is closed and therefore recurrent, and class <span class="math inline">\(\{1,2,3,4\}\)</span> is not closed and therefore transient.</p>

<p>It can be useful to further divide recurrent classes, where the return probability <span class="math inline">\(m_i = 1\)</span>, depending on whether the expected return time <span class="math inline">\(\mu_i\)</span> is finite or not.</p>

The following facts are not difficult to prove:

<p>In the last lecture, we looked at return times for the simple random walk, which is an irreducible Markov chain. We saw that <span class="math display">\[ m_i = \begin{cases}
2p &amp; \text{if $p &lt; \frac12$} \\
1 &amp; \text{if $p = \frac12$} \\
2q &amp; \text{if $p &gt; \frac12$.} \
\end{cases} \]</span> and that <span class="math inline">\(\mu_i = \infty\)</span> always. We see that, for <span class="math inline">\(p \neq \frac12\)</span>, the simple random walk is transient, while for <span class="math inline">\(p = \frac12\)</span> the simple symmetric random walk is null recurrent.</p>
<p>We can also consider the simple symmetric random walk in <span class="math inline">\(d\)</span>-dimensions, on <span class="math inline">\(\mathbb Z^d\)</span>. At each step we pick one of the coordinates and increase or decrease it by one; each of the <span class="math inline">\(2d\)</span> possibilities having probability <span class="math inline">\(1/2d\)</span>. We have seen that for <span class="math inline">\(d=1\)</span> this is null recurrent, and it is null recurrent for <span class="math inline">\(d = 2\)</span> also, but for <span class="math inline">\(d \geq 3\)</span> it is transient.</p>



<p>Consider the two-state `broken printer’ Markov chain from Lecture 5.</p>


Suppose we start the chain from the initial distribution <span class="math display">\[ \lambda_0 = \mathbb P(X_0 = 0) = \frac{\beta}{\alpha+\beta} \qquad \lambda_1 = \mathbb P(X_0 = 1) = \frac{\alpha}{\alpha+\beta} . \]</span> (You may recognise this from Question 1 on Problem Sheet 3.) What’s the distribution after step 1? By conditioning on the initial position, we have
<span class="math display">\[\begin{align*}
  \mathbb P(X_1 = 0) &amp;= \lambda_0 p_{00} + \lambda_1 p_{10} = \frac{\beta}{\alpha+\beta}(1-\alpha) + \frac{\alpha}{\alpha+\beta}\beta = \frac{\beta}{\alpha+\beta} ,\\
  \mathbb P(X_1 = 1) &amp;= \lambda_0 p_{01} + \lambda_1 p_{11} = \frac{\beta}{\alpha+\beta}\alpha + \frac{\alpha}{\alpha+\beta}(1-\beta) = \frac{\alpha}{\alpha+\beta} .
\end{align*}\]</span>
<p>So we’re still in the same distribution we started in. By repeating the same calculation, we’re still going to be in this distribution after step 2, and step 3, and forever.</p>
<p>More generally, if we start from a state given by a distribution <span class="math inline">\(\boldsymbol \pi = (\pi_i)\)</span>, then after step 1 the probability we’re in state <span class="math inline">\(j\)</span> is <span class="math inline">\(\sum_i \pi_i p_{ij}\)</span>. So if <span class="math inline">\(\pi_j = \sum_i \pi_i p_{ij}\)</span>, we stay in this distribution forever. In matrix form, this is <span class="math inline">\(\boldsymbol \pi = \boldsymbol \pi\mathsf P\)</span>. (Remember that <span class="math inline">\(\boldsymbol \pi\)</span> is a  vector.) We call such a distribution a stationary distribution.</p>


<p>Let’s try an example. Consider the no-claims discount Markov chain from Lecture 6 with state space <span class="math inline">\(\mathcal S=\{1,2,3\}\)</span> and transition matrix <span class="math display">\[ \mathsf P =\begin{pmatrix}
    \tfrac14 &amp;\tfrac34 &amp; 0\\[2px]
    \tfrac14 &amp;0 &amp; \tfrac34\\[2px]
    0 &amp;\tfrac14 &amp; \tfrac34\\
    \end{pmatrix} .\]</span></p>
We want to find a stationary distribution <span class="math inline">\(\boldsymbol \pi\)</span>, which must solve the equation <span class="math inline">\(\boldsymbol \pi =\boldsymbol \pi\mathsf P\)</span>, which is <span class="math display">\[ \begin{pmatrix} \pi_1 &amp; \pi_2 &amp; \pi_3 \end{pmatrix}  = \begin{pmatrix} \pi_1 &amp; \pi_2 &amp; \pi_3 \end{pmatrix}  \begin{pmatrix}
    \tfrac14 &amp;\tfrac34 &amp; 0\\[2px]
    \tfrac14 &amp;0 &amp; \tfrac34\\[2px]
    0 &amp;\tfrac14 &amp; \tfrac34\\
    \end{pmatrix} .\]</span> Writing out the equations coordinate at a time, we have
<span class="math display">\[\begin{align*}
    \pi_1 &amp;= \tfrac14\pi_1+\tfrac14\pi_2 , \\[2px]
    \pi_2 &amp;= \tfrac34\pi_1+\tfrac14\pi_3 , \\[2px]
    \pi_3 &amp;= \tfrac34\pi_2+\tfrac34\pi_3 . 
    \end{align*}\]</span>
<p>Since <span class="math inline">\(\boldsymbol\pi\)</span> must be a distribution, we also have the normalising condition <span class="math display">\[ \pi_1+\pi_2+\pi_3=1 . \]</span></p>
<pre><code>The way to solve these equations is first to solve for all the variables $\pi_i$ in terms of a convenient $\pi_{j}$ (called the \emph{working variable}) and then substitue all of these expressions into the normalising condition to find a value for $\pi_{j}$.

Choosing $\pi_2$ as our working variable, the first and third equations (we won&#39;t need the second equation) can be solved in terms of $\pi_2$, giving 
\[ \pi_1=\tfrac13\pi_2 \qquad \pi_3=3\pi_2 . \]

The normalising condition gives
\[
\pi_1+\pi_2+\pi_3 = \tfrac13\pi_2+\pi_2+3\pi_2 = \tfrac{13}{3} \pi_2 = 1 .
\]</code></pre>
<p>So <span class="math inline">\(\pi_2 = 3/13\)</span>, and the solution is <span class="math display">\[ \boldsymbol \pi = (\pi_1\quad \pi_2\quad \pi_3) = \left(\tfrac{1}{13}\quad \tfrac{3}{13}\quad\tfrac{9}{13}\right). \]</span></p>
<p>%</p>
The method we used in the example can be summarised as follows:

<p>It is good practice to use the equation discarded earlier to verify that the calculated solution is indeed correct.</p>
<p>Another example is given in the handout .</p>

<p>Two natural questions to ask are the following: Does a stationary distribution always exist? If a stationary distribution does exists, is there only one, or can there be many stationary distributions?</p>
<p>The answer is given by the following very important theorem.</p>

<p>We won’t prove this theorem here, but see Norris, , Section 1.7 or Grimmett and Stirzaker, , Section 6.4 for details.</p>
<p>In our no-claims discount example, the chain was irreducible and, like all finite-state irreducible chains, positive recurrent. Thus the stationary distribution <span class="math inline">\(\boldsymbol\pi\)</span> we found is the unique stationary distribution for that chain. Once we have the stationary distribution <span class="math inline">\(\boldsymbol\pi\)</span>, we get the expected return times <span class="math inline">\(\mu_i = 1/\pi_i\)</span> for free: the expected return times are <span class="math inline">\(m_1 = 13\)</span>, <span class="math inline">\(m_2 = \frac{13}{3} = 4.33\)</span>, <span class="math inline">\(m_3 = \frac{13}{9} = 1.44\)</span>.</p>

Note the condition in Theorem  that the Markov chain is irreducible. What if the Markov chain has more than one communicating class? We can work out what must happen from the theorem:




<pre><code>\addcontentsline{toc}{subsection}{\emph{Handout: How to find stationary distributions}}

\begin{center}
    { \texttt{MATH2750} Introduction to Markov Processes (2018--19)}
    
    \vspace{18pt}
    
    {\LARGE \textbf{How to find \\[0.23cm] stationary distributions}}
    
    \vspace{16pt}</code></pre>
<p>\end{center}</p>
<p>This handout accompanies . We summarise the techniques from the lecture, and give an extra examples.</p>

<p>Let <span class="math inline">\((X_n)\)</span> be a Markov chain on a state space <span class="math inline">\(\mathcal S\)</span> with transition matrix <span class="math inline">\(\mathsf P\)</span>. Let <span class="math inline">\(\boldsymbol \pi = (\pi_i)\)</span> be a distribution on <span class="math inline">\(\mathcal S\)</span>, in that <span class="math inline">\(\pi_i \geq 0\)</span> for all <span class="math inline">\(i \in \mathcal S\)</span> and <span class="math inline">\(\sum_{i \in \mathcal S} \pi_i = 1\)</span>. We call <span class="math inline">\(\boldsymbol \pi\)</span> a  if <span class="math display">\[ \pi_j = \sum_{i\in \mathcal S} \pi_i p_{ij} \quad \text{for all $j \in \mathcal S$.} \]</span></p>
<p>If we write <span class="math inline">\(\boldsymbol\pi\)</span> as a row vector the condition is that <span class="math inline">\(\boldsymbol \pi = \boldsymbol \pi\mathsf P\)</span>.</p>

In the lecture we stated the following theorem for irreducible Markov chains:


The way to find a stationary distribution is as follows:



<p> <span class="math display">\[ \mathsf P = \begin{pmatrix} \tfrac12 &amp; \tfrac14&amp; \frac14 \\[0.5ex]
                   \tfrac14&amp; \frac12&amp; \frac14 \\[0.5ex]
                   0       &amp; \frac14 &amp; \frac34 \end{pmatrix} . \]</span> </p>

 Writing out <span class="math inline">\(\boldsymbol \pi = \boldsymbol \pi\mathsf P\)</span>, we get
<span class="math display">\[\begin{align*}
\pi_1 &amp;= \tfrac12 \pi_1 + \tfrac14\pi_2 \\
\pi_2 &amp;= \tfrac14\pi_1 + \tfrac12\pi_2 + \tfrac14\pi_3 \\
\pi_3 &amp;= \tfrac14\pi_1 + \tfrac14\pi_2 + \tfrac34\pi_3 .
\end{align*}\]</span>
<p>We discard the third equation.</p>
<p> We choose <span class="math inline">\(\pi_1\)</span> as our working variable. From the first equation we get <span class="math inline">\(\pi_2 = 2\pi_1\)</span>. From the second equation we get <span class="math inline">\(\pi_3 = 2\pi_2 - \pi_1\)</span>, and substituting the previous <span class="math inline">\(\pi_2 = 2\pi_1\)</span> into this, we get <span class="math inline">\(\pi_3 = 3\pi_1\)</span>.</p>
<p> The normalising condition is <span class="math display">\[ \pi_1 + \pi_2 + \pi_3 = \pi_1 + 2\pi_1 + 3\pi_1 = 6\pi_1 = 1 . \]</span> Therefore <span class="math inline">\(\pi_1 = \frac16\)</span>. Substituting this into our previous expressions, we get that <span class="math inline">\(\pi_2 = 2\pi_1 = \frac13\)</span> and <span class="math inline">\(\pi_3 = 3\pi_1 = \frac12\)</span>.</p>
<p>Thus the solution is <span class="math display">\[ \boldsymbol\pi = \left( \tfrac16 \quad \tfrac13 \quad \tfrac12 \right) . \]</span></p>
<p>We can check our answer with the discarded third equation: we get <span class="math display">\[ \tfrac12 = \pi_3 = \tfrac14\pi_1 + \tfrac14\pi_2 + \tfrac34\pi_3 = \tfrac14\tfrac16 + \tfrac14\tfrac13+\tfrac34\tfrac12 = \tfrac1{24} + \tfrac2{24} + \tfrac{9}{24} = \tfrac{12}{24} = \tfrac{1}{2} , \]</span> as desired.</p>
<p> Find stationary distributions for the following two transition matrices: <span class="math display">\[ \begin{pmatrix} \tfrac13 &amp; \tfrac23 &amp; 0 \\[0.5ex]
                   \tfrac16 &amp; \tfrac13 &amp; \frac12 \\[0.5ex]
                   0        &amp; \tfrac13 &amp; \frac23  \end{pmatrix}, \qquad\qquad
   \begin{pmatrix} \tfrac13 &amp; \tfrac13 &amp; \frac13 \\[0.5ex]
                   \tfrac14 &amp; \tfrac12 &amp; \frac14 \\[0.5ex]
                   \tfrac16 &amp; \tfrac13 &amp; \frac12  \end{pmatrix} . \]</span></p>
<p> <span class="math inline">\((\tfrac{1}{11}, \frac{4}{11}, \frac{6}{11})\)</span> and <span class="math inline">\((\frac{6}{25}, \frac{10}{25}, \frac{9}{25})\)</span>.</p>

<p> </p>



<p>In this lecture we’re interested in what happens to a Markov chain <span class="math inline">\((X_n)\)</span> in the long-run – that is, when <span class="math inline">\(n\)</span> tends to infinity.</p>

<p>One thing that could happen is that the Markov chain could tend towards an <code>equilibrium distribution'. Further, that long-term equilibrium might not depend on the initial distribution, but that initial behaviour might eventually almost disappear, exhibiting a</code>lack of memory’ property.</p>

<p>It’s clear there can be at most one equilibrium distribution, but will there be one at all? The following is the most important result in this course.</p>

<p>Note the three conditions for convergence to an equilibrium distribution: irreducibility, aperiodicity, and positive recurrence. (Recall that an irreducible Markov chain is aperiodic if it has period <span class="math inline">\(1\)</span>.)</p>
<p>Consider a irreducible, aperiodic, positive recurrent Markov chain. Taking the initial distribution to be starting in state <span class="math inline">\(i\)</span> with certainty, the limit theorem tells us that <span class="math inline">\(p_{ij}^{(n)} \to \pi_j\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. In particular, the <span class="math inline">\(n\)</span>-step transition matrix will have the limiting value <span class="math display">\[ \lim_{n \to \infty} \mathsf P^{(n)} = \begin{pmatrix}
     \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N \\
     \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N \\
     \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
     \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N \end{pmatrix} , \]</span> where each row is identical.</p>
<p>We will not prove the limit theorem here; see Norris, , Section 1.8 or Grimmett and Stirzaker, , Section 6.4 for details. We can, however, prove the following simpler result.</p>


<p>Given this result it’s clear that an irreducible Markov chain cannot have an equilibrium distribution if it is null recurrent or transient, as it doesn’t even have a stationary distribution.</p>
<p>%This tells us that, if there is an equilibrium distribution then it is stationary. But will there be an equilibrium distribution?</p>







<p>The limit theorem looked at the limit of <span class="math inline">\(\mathbb P(X_n = j)\)</span>, the probability that the Markov chain is in state <span class="math inline">\(j\)</span> at some specific time <span class="math inline">\(n\)</span> a long time in the future. We could also look at the long-run amount of time spent in state <span class="math inline">\(j\)</span>; that is, averaging the behaviour over a long time period.</p>
<p>Let us write <span class="math display">\[ V_j(N) = \# \big\{ n \in \{0,1,\dots, N\} : X_n = j \} \]</span> for the total number of visits to state <span class="math inline">\(j\)</span> by time <span class="math inline">\(N\)</span>. Then we can interpret <span class="math inline">\(V_j(n)/n\)</span> to be the proportion of time up to time <span class="math inline">\(n\)</span> spent in state <span class="math inline">\(j\)</span>, and its limiting value (if it exists) to be the long-run proportion of time n state <span class="math inline">\(j\)</span>.</p>

<p>Note that, because we are averaging over a long-time period, we do not need the condition that the Markov chain is aperiodic.</p>
<p>This theorem is not difficult to prove using the law of large numbers. For completeness, we state that `almost sure’ convergence means that <span class="math inline">\(\mathbb P(V_j(n)/n \to 1/\mu_j) = 1\)</span>, although the precise definition is not important for us here.</p>








<p>%}</p>

<p></p>




<p>In the next three lectures we’ll be considering the Poisson process, a continuous time discrete space process with the Markov property.</p>

<p>You should remember the Poisson distribution. Recall that a discrete random variable <span class="math inline">\(X\)</span> has a  with rate <span class="math inline">\(\lambda\)</span>, written <span class="math inline">\(X \sim \Po(\lambda)\)</span>, if its probability mass function is <span class="math display">\[ \mathbb P(X = n) = \ee^{-\lambda} \frac{\lambda^n}{n!} \qquad n = 0,1,2,\dots. \]</span></p>
<p>The Poisson distribution is often used to model the number of `arrivals’ in a fixed amount of time – for example, the number of calls to a call centre in one hour, the number of claims to an insurance company in one year, or the number of particles decaying from a large amount of radioactive material in one second.</p>
You are reminded of the following properties of the Poisson distribution:


<p>Suppose that, instead of just modelling the number of arrivals in a fixed amount of time, we want to model the total number of arrivals as it changes over time. This will be a stochastic process with discrete state space <span class="math inline">\(\mathcal S = \mathbb Z_+ = \{0,1,2,\dots\}\)</span> and continuous time <span class="math inline">\(\mathbb R_+ = [0,\infty)\)</span>. In continuous time, we will normally write stochastic processes as <span class="math inline">\((X(t))\)</span>, with the time variable being a <span class="math inline">\(t\)</span> in brackets (rather than a subscript <span class="math inline">\(n\)</span>, as we had in discrete time).</p>
Suppose calls arrive at a call centre at a rate of <span class="math inline">\(\lambda = 100\)</span> an hour. The following assumptions seem reasonable:

<p>These properties will define the Poisson process.</p>





<p>The following theorem shows that the sum of two Poisson processes is itself a Poisson process. </p>
<p>%</p>




<p>We also have marked Poisson processes, which can be thought of as the opposite to the summed process.</p>







<p>Last time, we introduced the Poisson process by saying that the random number of arrivals in fixed time follows a Poisson distribution. Another way of looking at the Poisson process is to look at the random amount of of time for a fixed number of arrivals. For this, we’ll need the exponential distribution.</p>

<p>We start by recalling the exponential distribution. Recall that we say a continuous random variable <span class="math inline">\(T\)</span> has the  with rate <span class="math inline">\(\lambda\)</span>, and write <span class="math inline">\(T \sim \Exp(\lambda)\)</span>, if it has the probability distribution function <span class="math inline">\(f(t) = \lambda \ee^{-\lambda t}\)</span> for <span class="math inline">\(t \geq 0\)</span>.</p>
You are reminded of the following facts about the exponential distribution:

<p>The following memoryless property will of course be important in a course about memoryless Markov processes.</p>

<p>Suppose we are waiting an exponentially distributed time for an alarm to go off. No matter how long we’ve been waiting for, the remaining time to wait is still exponentially distributed, with the same parameter. Hence `memoryless’.</p>

<p>The following property will be important later on in the course.</p>

<p>The proof of this is an exercise on Problem Sheet 7.</p>
<p>%\begin{proof} %For the first part, we note that the minimum <span class="math inline">\(T\)</span> will only be larger than some <span class="math inline">\(t\)</span> if all of the <span class="math inline">\(T_i\)</span>s are larger than that <span class="math inline">\(t\)</span>. Hence we have %<span class="math display">\[ \mathbb P(T &gt; t) =  \mathbb P(T_1 &gt; t) \cdots \mathbb P(T_n &gt; t) = \ee^{-\lambda_1t} \cdots \ee^{-\lambda_nt} = \ee^{-(\lambda_1 + \cdots + \lambda_n)t} ,  \]</span> %which is the tail probability of the desired exponential distribution.</p>
<p>%The second part is an exercise on Problem Sheet 7. %\end{proof}</p>

<p>Exponential distributions are often used to model <code>waiting times' -- for example, the amount of time until a light bulb breaks, or the times between busses arriving. We often use the term</code>holding time’ to refer to the time between consecutive arrivals.</p>
When modelling a process <span class="math inline">\((X(t))\)</span> counting many arrivals at rate <span class="math inline">\(\lambda\)</span>, we might model the process like this: after waiting an <span class="math inline">\(\Exp(\lambda)\)</span> amount of time, an arrival appears. After another <span class="math inline">\(\Exp(\lambda)\)</span> amount of time, another arrival appears. And so on.

<p>It turns out that the process as described above is also the Poisson process!</p>







<p>We previously saw the Markov `memoryless’ property in discrete time. The equivalent definition in continuous time is the following.</p>

<p>In other words, the state of the process at some point <span class="math inline">\(t_{n+1}\)</span> in the future depends on where we are now <span class="math inline">\(t_n\)</span>, but, given that, does not depend on any collection of previous times <span class="math inline">\(t_0, t_1, \dots, t_{n-1}\)</span>.</p>
<p>The Poisson process does indeed have the Markov property, since, by the property of independent increments, we have that <span class="math display">\[ \mathbb P\big( X(t_{n+1}) = x_{n+1} \mid X(t_n) = x_n, \dots, X(t_0) = x_0 \big) = \mathbb P\big( X(t_{n+1}) -X(t_n) = x\big) , \]</span> where <span class="math inline">\(x = x_{n+1}-x_n\)</span>. By Poisson increments, this has probability <span class="math display">\[ \ee^{-\lambda(t_{n+1}-t_n)} \frac{\big(\lambda(t_{n+1}-t_n)\big)^x}{x!} . \]</span></p>
<p>Alternatively, by the memoryless property of the exponential distribution, we see that we can restart the holding time from <span class="math inline">\(t_n\)</span> and still have this and all future holding times exponentially distributed.</p>




<p>We have seen two definitions of the Poisson process so far: one in terms of increments having a Poisson distribution, and one in therms of holding times having an exponential distribution. Here we will see another definition, by looking at what happens in a very small time period <span class="math inline">\(\tau\)</span>. Advantages of this approach include that does not require direct assumptions on the distributions involved, so may seem more <code>natural' or</code>inevitable’, and that it shows links between Markov processes and differential equations. A disadvantage is that it’s normally easier to do calculations with the Poisson or exponential definitions.</p>

Let <span class="math inline">\((X(t))\)</span> be a stochastic processes counting some arrivals over time. Consider the number of arrivals in a very small time period <span class="math inline">\(\tau\)</span>, which is <span class="math inline">\(X(t+\tau) - X(t)\)</span>. The following (seemingly) weak assumptions seem justified:

<p>To write this down formally in maths, we will consider <span class="math inline">\(\mathbb P(X(t+\tau) - X(t) = j)\)</span> as the length of the time period <span class="math inline">\(\tau\)</span> tends to zero.</p>
It will be helpful to use <code>little-$o$' notation. Here $o(\tau)$ means a term that is of</code>lower order’ compared to <span class="math inline">\(\tau\)</span>. Specifically, <span class="math inline">\(o(\tau)\)</span> stands for a function of <span class="math inline">\(\tau\)</span> that tends to <span class="math inline">\(0\)</span> as <span class="math inline">\(\tau \to 0\)</span> even when divided by <span class="math inline">\(\tau\)</span>. That is, <span class="math display">\[ f(\tau) = o(\tau) \qquad \iff \qquad \frac{f(\tau)}{\tau} \to 0 \quad \text{as $\tau \to 0$.}  \]</span> Then our suggestions above translates to
<span class="math display">\[\begin{equation} \mathbb P \big(X(t+\tau) - X(t) = j\big) = \begin{cases} 1 - \lambda \tau + o(\tau) &amp; \text{if $j = 0$,} \\
\lambda\tau + o(\tau) &amp; \text{if $j = 1$,} \\
o(\tau) &amp; \text{if $j \geq 2$.} \end{cases} \tag{$*$} \end{equation}\]</span>
<p>as <span class="math inline">\(\tau \to 0\)</span>.</p>
<p>It will not be surprising to learn that we have defined the Poisson process with rate <span class="math inline">\(\lambda\)</span>.</p>

<p>We will prove this in a moment. First let’s see an example of its use.</p>

<p>We mentioned in Lecture 12 the sum of two Poisson processes: that if <span class="math inline">\((X(t))\)</span> is a Poisson process with rate <span class="math inline">\(\lambda\)</span> and <span class="math inline">\((Y(t))\)</span> is a Poisson process with rate <span class="math inline">\(\mu\)</span> are independent, then <span class="math inline">\((Z(t))\)</span> where <span class="math inline">\(Z(t) = X(t) + Y(t)\)</span> is a Poisson process with rate <span class="math inline">\(\lambda + \mu\)</span>.</p>
<p>On Problem Sheet 7, Question 3, you will prove this from the Poisson increments definition. But it’s perhaps easier using the infinitesimals.</p>
We have
<span class="math display">\[\begin{align*}\mathbb P\big( Z(t+\tau) - Z(t) = 0 \big) &amp;= \mathbb P\big( X(t+\tau) - X(t) = 0 \big) \, \mathbb P\big( Y(t+\tau) - Y(t) = 0 \big) \\ &amp;=\big(1 - \lambda \tau + o(\tau) \big)\big(1 - \mu \tau + o(\tau) \big) \\
&amp;= 1 - (\lambda + \mu)\tau + o(\tau) , \end{align*}\]</span>
since <span class="math inline">\(Z\)</span> does not increase provided neither <span class="math inline">\(X\)</span> nor <span class="math inline">\(Y\)</span> increase. We also have
<span class="math display">\[\begin{align*}\mathbb P\big( Z(t+{}&amp;\tau) - Z(t) = 1 \big) \\
&amp;= \mathbb P\big( X(t+\tau) - X(t) = 1 \big) \, \mathbb P\big( Y(t+\tau) - Y(t) = 0 \big) \\ &amp;\qquad {}+ \mathbb P\big( X(t+\tau) - X(t) = 0 \big) \, \mathbb P\big( Y(t+\tau) - Y(t) = 1 \big) \\
&amp;= \big(\lambda \tau + o(\tau) \big)\big(1 - \mu \tau + o(\tau) \big) + \big(1 - \lambda \tau + o(\tau) \big)\big(\mu \tau + o(\tau) \big) \\
&amp;= (\lambda + \mu)\tau + o(\tau) , \end{align*}\]</span>
<p>since <span class="math inline">\(Z\)</span> increases by <span class="math inline">\(1\)</span> if either <span class="math inline">\(X\)</span> increases by <span class="math inline">\(1\)</span> and <span class="math inline">\(Y\)</span> stays fixed or vice versa. We used here that terms in <span class="math inline">\(\tau^2\)</span> are <span class="math inline">\(o(\tau)\)</span>. Finally, since probabilities must add up to <span class="math inline">\(1\)</span>, we must have <span class="math display">\[ \mathbb P\big( Z(t+\tau) - Z(t) \geq 2 \big) =  o(\tau) . \]</span></p>
<p>But what we have written down is precisely the inifinitesimals definition of a Poisson process with rate <span class="math inline">\(\lambda + \mu\)</span>.</p>

<p>If we compare the theorem to the definition of the Poisson process in terms of Poisson increments, we see that properties 1 and 3 are the same. We only need to check property 2, that <span class="math inline">\(X_{t+s} - X(s)\)</span> is a Poisson distribution with rate <span class="math inline">\(\lambda t\)</span>.</p>
<p>Since it’s clear the increments as defined in <span class="math inline">\((*)\)</span> are stationary, it will suffice to consider <span class="math inline">\(s = 0\)</span>. So we need to show that <span class="math inline">\(X(t) \sim \text{Po}(\lambda t)\)</span>.</p>
<p>Write <span class="math inline">\(p_j(t) = \mathbb P(X(t) = j)\)</span>. Then for <span class="math inline">\(j \geq 1\)</span> we have <span class="math display">\[ p_j(t + \tau) = \big(1 - \lambda\tau + o(\tau)\big)p_j(t) + \big(\lambda\tau + o(\tau)\big) p_{j-1}(t) + o(\tau) , \]</span> since we get to <span class="math inline">\(j\)</span> either by staying at <span class="math inline">\(j\)</span> or moving up from <span class="math inline">\(j-1\)</span>, all other highly unlikely possibilities being absorbed into the <span class="math inline">\(o(\tau)\)</span>. In order to deal with <code>increments' it will be convenient to take a $p_{j}(t)$ to the left-hand side and rearrange, to get \[ p_j(t + \tau) - p_j(t) = -\lambda\tau p_j(t) + \lambda\tau p_{j-1}(\tau) + o(\tau) , \] where</code>constant times <span class="math inline">\(o(\tau)\)</span>’ is itself <span class="math inline">\(o(\tau)\)</span>.</p>
<p>The only way to deal with the <span class="math inline">\(o(\tau)\)</span> term, given its definition, is to divide everything by <span class="math inline">\(\tau\)</span> and send <span class="math inline">\(\tau \to 0\)</span>. Dividing by <span class="math inline">\(\tau\)</span> gives <span class="math display">\[ \frac{p_j(t + \tau) - p_j(t)}{\tau} = -\lambda p_j(t) + \lambda p_{j-1}(\tau) + \frac{o(\tau)}{\tau} . \]</span> Sending <span class="math inline">\(\tau\)</span> to <span class="math inline">\(0\)</span>, the term <span class="math inline">\(o(\tau)/\tau\)</span> tends to <span class="math inline">\(0\)</span> and vanishes, while we recognise the limit of the left-hand side as being the derivative <span class="math inline">\(\mathrm d p_j(t) / \mathrm d t = p&#39;_j(t)\)</span>. </p>
<p>This leaves us with the differential equation <span class="math display">\[ p&#39;_j(t) = -\lambda p_j(t) + \lambda p_{j-1}(t) . \]</span> We also have the initial condition <span class="math inline">\(p_j(0) = 0\)</span> for <span class="math inline">\(j \geq 1\)</span>, since we start at <span class="math inline">\(0\)</span>, not at any <span class="math inline">\(j \geq 1\)</span>.</p>
<p>We have to deal with the case <span class="math inline">\(j = 0\)</span> separately. This is similar but a bit easier. We have <span class="math display">\[ p_0(t + \tau) = \big(1 - \lambda\tau + o(\tau)\big)p_0(t) , \]</span> since we must previously been at <span class="math inline">\(0\)</span>, then seen no arrivals. After rearranging and sending <span class="math inline">\(\tau \to 0\)</span>, we get the differential equation <span class="math display">\[ p&#39;_0(t) = -\lambda p_0(t) \]</span> with the initial condition <span class="math inline">\(p_0(0) = 1\)</span>, since we always start at <span class="math inline">\(0\)</span>.</p>
In summary, we have derived the following equations:
<span class="math display">\[\begin{align*}
   p&#39;_0(t) &amp;= -\lambda p_0(t) &amp; \hspace{-1.5cm} p_0(0) &amp; = 1 ,\\
   p&#39;_j(t) &amp;= -\lambda p_j(t) + \lambda p_{j-1}(t) &amp; \hspace{-1.5cm} p_j(0) &amp;= 0 \quad \text{for $j \geq 1$} .
\end{align*}\]</span>
<p>These are often referred to as the .</p>
<p>We claim that the solution to these equations is the Poisson distribution <span class="math display">\[ p_j(t) = \ee^{-\lambda t} \frac{(\lambda t)^j}{j!} . \]</span></p>
<p>We can now check this holds. For <span class="math inline">\(j = 0\)</span>, the Poisson probability is <span class="math display">\[ p_0(t) = \ee^{-\lambda t} \frac{(\lambda t)^0}{0!} = \ee^{-\lambda t} . \]</span> This indeed has <span class="math inline">\(p_0(0) = \ee^{0} = 1\)</span>, and we have <span class="math display">\[ p&#39;_0(t) = -\lambda \ee^{-\lambda t} = -\lambda p_0(t) , \]</span> as desired.</p>
For <span class="math inline">\(j \geq 0\)</span>, we have <span class="math inline">\(p_j(0) = 0^j = 0\)</span>, and the left-hand side is
<span class="math display">\[\begin{align*}
p&#39;_j(t) &amp;= \frac{\mathrm d}{\mathrm d t} \ee^{-\lambda t} \frac{(\lambda t)^j}{j!} \\
&amp; = \frac{1}{j!} \left(-\lambda \ee^{-\lambda t}(\lambda t)^j + \ee^{-\lambda t} \lambda j(\lambda t)^{j-1} \right) \\
&amp;= - \lambda \ee^{-\lambda t} \frac{(\lambda t)^j}{j!} + \lambda \ee^{-\lambda t} \frac{(\lambda t)^{j-1}}{(j-1)!} \\
&amp;= -\lambda p_j(t) + \lambda p_{j-1}(t)
\end{align*}\]</span>
<p>as desired. We are finally done. </p>
<p>%</p>
<p>%In fact, standard results about differential equations will verify that this is the only solution to the forward equations.</p>



<p>The Poisson process was made easier to understand because it was a . That is, because it was counting arrivals, we always knew that the next change would be to increase by <span class="math inline">\(1\)</span>; the only question is when that increase would happen.</p>
<p>Here look at two more complicated types of counting process; the only transitions will be from <span class="math inline">\(i\)</span> to <span class="math inline">\(i+1\)</span>, but the holding times will not just be IID exponential distributions any more.</p>

<p>Consider the following , which can model, for example, the division of cells. We suppose that each individual has an offspring (eg the cell divides) after an <span class="math inline">\(\Exp(\lambda)\)</span> period of time, and continues to have more offspring after another <span class="math inline">\(\Exp(\lambda)\)</span> period of time, and so on.</p>
<p>We start with <span class="math inline">\(X(0) = 1\)</span> individual. After an <span class="math inline">\(\Exp(\lambda)\)</span> time, the individual has an offspring, and we have <span class="math inline">\(X(t) = 2\)</span>.</p>
<p>Now each of these two individuals will have an offspring after an <span class="math inline">\(\Exp(\lambda)\)</span> time each. So how much longer will it be until <span class="math inline">\(X(t) = 3\)</span>? Well, the earliest offspring will appear at the minimum of these two <span class="math inline">\(\Exp(\lambda)\)</span> times, and we saw in Lecture 13 that this has an <span class="math inline">\(\Exp(2\lambda)\)</span> distribution. (Remember that the mean of an exponential distribution is the reciprocal of the rate parameter, so the holding time for the second offspring is half that of the first, on average.)</p>
<p>Now suppose we have <span class="math inline">\(n\)</span> individuals. By the memoryless property, they each still have an <span class="math inline">\(\Exp(\lambda)\)</span> time until producing an offspring, so the time until the next offspring is the minimum of these, which is <span class="math inline">\(\Exp(n\lambda)\)</span>.</p>
<p>In general, we have built a counting process defined entirely by its starting point <span class="math inline">\(X(0) = 1\)</span>, that the <span class="math inline">\(n\)</span>th holding time <span class="math inline">\(T_n\)</span> is exponential with rate <span class="math inline">\(n\lambda\)</span>, and that all transitions are increases by <span class="math inline">\(1\)</span>.</p>
<p>On Problem Sheet 8, you will show that the expected size of the population of the simple birth process at time <span class="math inline">\(t\)</span> is <span class="math inline">\(\mathbb EX(t) = \ee^{\lambda t}\)</span>, so the population grows exponentially quickly (on average).</p>
<p>The simple birth process is an example of the more general class of birth processes.</p>


<p>%Going back to the simple birth process, what is the expected size <span class="math inline">\(E(t) = \mathbb EX(t)\)</span> of the population at time <span class="math inline">\(t\)</span>?</p>
<p>%We start by conditioning on the time <span class="math inline">\(T_1\)</span> that the first birth occurs. We have %<span class="math display">\[ E(t) = \int_0^t f_{T_1}(s) \,\mathbb E\big(X(t) \mid T_1 = s\big) \, \mathrm ds = \int_0^\infty \lambda \ee^{-\lambda s} \,\mathbb E\big(X(t) \mid T_1 = s\big) \, \mathrm ds . \]</span></p>
<p>%What can we say about <span class="math inline">\(\mathbb E(X(t) \mid T_1 = s)\)</span>? Well, we have the first offspring at time <span class="math inline">\(s\)</span>, then have time <span class="math inline">\(t-s\)</span> left. We can then measure separately the descendents from the new individual and those not from the original individual (not from the new individual). Each of these populations has expected size <span class="math inline">\(E(t-s)\)</span>, because they can be considered as separate populations running for time <span class="math inline">\(t-s\)</span>. This gives us %[ E(t) = _0<sup></sup>{-s} E(t-s) ds</p>
<p>%In the Poisson process, the holding times <span class="math inline">\(T_j\)</span> between arrivals were independent exponentially distributed, and, further, were identically distributed. In birth processes, the holding times remain identically distributed, but can have different rate parameters.</p>
We can also give an equivalent definition using infinitesimal time periods, as we did for the Poisson process. Suppose we start from <span class="math inline">\(X(0) = 1\)</span>. We have
<span class="math display">\[\begin{align*}
    \mathbb P \big(X(t+\tau) = j\phantom{{}+1} \mid X(t) = j\big) &amp;= 1 - \lambda_j\tau + o(\tau) ,\\
    \mathbb P \big(X(t+\tau) = j+1 \mid X(t) = j\big) &amp;= \lambda_j\tau + o(\tau) ,\\
    \mathbb P \big(X(t+\tau) \geq j+2 \mid X(t) = j\big) &amp;= o(\tau) ,
\end{align*}\]</span>
<p>which is exactly the same as the Poisson process, except with <span class="math inline">\(\lambda\)</span> replaced by <span class="math inline">\(\lambda_j\)</span>.</p>
<p>We can continue as for the Poisson process. Write <span class="math inline">\(p_j(t) = \mathbb P(X(t) = j)\)</span>. Then, for <span class="math inline">\(j \geq 2\)</span>, <span class="math display">\[ p_j(t + \tau) = \big(1 - \lambda_{j}\tau + o(\tau)\big)p_j(t)  + \big(\lambda_{j-1}\tau + o(\tau)\big)p_{j-1}(t) + o(\tau) , \]</span> since the ways to get to <span class="math inline">\(k\)</span> are: either we are already at <span class="math inline">\(j\)</span> and the <span class="math inline">\(j\)</span>th arrival doesn’t occur, or we’re at <span class="math inline">\(j-1\)</span> and the <span class="math inline">\((j-1)\)</span>th arrival does occur; other possibilities have <span class="math inline">\(o(\tau)\)</span> probability. As before, take <span class="math inline">\(p_j(t)\)</span> over to the left-hand side, divide by <span class="math inline">\(\tau\)</span> to get <span class="math display">\[ \frac{p_j(t + \tau) - p_j(t)}{\tau} = -\lambda_j p_j(t) + \lambda_{j-1}p_{j-1}(t) + \frac{o(\tau)}{\tau} , \]</span> and then send <span class="math inline">\(\tau \to 0\)</span>. We end up with the forward equation <span class="math display">\[ p&#39;_j(t) = -\lambda_j p_j(t) + \lambda_{j-1}p_{j-1}(t) . \]</span>  The initial condition is <span class="math inline">\(p_j(0) = 0\)</span>, since we start at <span class="math inline">\(1\)</span>, not at any <span class="math inline">\(j \geq 2\)</span>.</p>
<p>Following the similar process for <span class="math inline">\(j =1\)</span>, we get <span class="math display">\[ p&#39;_1(t) = -\lambda_1 p_1(t) , \]</span> with initial condition <span class="math inline">\(p_1(0) = 1\)</span>, since we start from <span class="math inline">\(1\)</span>.</p>
<p>In some cases the forward equations can be explicitly solved to give an expression for <span class="math inline">\(p_j(t) = \mathbb P(X(t) = j)\)</span>. We saw the solution for the Poisson process in Lecture 14, and you will find a solution for the simple birth process on Problem Sheet 8.</p>
<p>%</p>

<p>The Poisson process and the birth processes were all ; that is, the transition probabilities <span class="math inline">\(\mathbb P(X(s+t) = j \mid X(s) = i)\)</span> depended on the state <span class="math inline">\(i,j\)</span> and on the length of time period <span class="math inline">\(t\)</span> but not on the current time <span class="math inline">\(s\)</span>. Most of the processes we consider in the rest of this course will be time homogeneous, but we consider some time inhomogeneous examples here. This means the transition probabilities will change over time.</p>
<p>A time inhomogeneous Poisson process is like the standard Poisson process, except, instead of a constant rate of arrival <span class="math inline">\(\lambda\)</span>, we will have a rate of arrival <span class="math inline">\(\lambda=\lambda(t)\)</span> that varies over time.</p>

In particular, if <span class="math inline">\(\lambda\)</span> is constant, then <span class="math inline">\(\int_t^{t+s} \lambda(u) \, \mathrm d u = \lambda s\)</span>, and we get back the definition of the Poisson process in terms of Poisson increments. 

The time inhomogeneous Poisson process can also be given a definition in terms of inifinitesimal increments. We have
<span class="math display">\[\begin{equation*} \mathbb P \big(X(t+\tau) - X(t) = j\big) = \begin{cases} 1 - \lambda(t) \tau + o(\tau) &amp; \text{if $j = 0$,} \\
\lambda(t)\tau + o(\tau) &amp; \text{if $j = 1$,} \\
o(\tau) &amp; \text{if $j \geq 2$.} \end{cases}  \end{equation*}\]</span>
<p>The only difference here is that we have replaced the rate <span class="math inline">\(\lambda\)</span> with the `current rate’ <span class="math inline">\(\lambda(t)\)</span>.</p>



<p>In this lecture, we will start looking at general Markov processes in continuous time and discrete space. All our examples will be time homogeneous, in that the transition probabilities and transition rates will remain constant over time.</p>
<p>We call these , as we will wait in a state for a random holding time, and then we will jump to another state, chosen at random.</p>
<p>The idea will be treat two matters separately: first,  – this will be studied through the jump chain, a discrete time Markov chain that tells us what transitions are made; and second,  – we call these the holding times, and to preserve the Markov property, these holding times must have an exponential distribution, since this is the only random variable that has the memoryless property.</p>
<p>Let us consider a Markov jump process <span class="math inline">\((X(t))\)</span> on a state space <span class="math inline">\(\mathcal S\)</span>. Suppose we are at a state <span class="math inline">\(i \in \mathcal S\)</span>. The  at which we wish to jump to a state <span class="math inline">\(j \neq i\)</span> will be written <span class="math inline">\(q_{ij} \geq 0\)</span>. This means that, after a time with an exponential distribution <span class="math inline">\(\Exp(q_{ij})\)</span>, if the process has not jumped yet, it will jump to state <span class="math inline">\(j\)</span>.</p>
<p>(We use the convention that if <span class="math inline">\(q_{ij} = 0\)</span> for some <span class="math inline">\(j \neq i\)</span>, this means we will never jump from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>, If <span class="math inline">\(q_{ij} = 0\)</span> for  <span class="math inline">\(j \neq i\)</span>, then we stay in state <span class="math inline">\(i\)</span> forever.)</p>
<p>So from <span class="math inline">\(i\)</span>, there are many states <span class="math inline">\(j\)</span> we could jump to, each waiting for a time <span class="math inline">\(\Exp(q_{ij})\)</span>. Which of these times will be up first, leading the process to jump to that state? And how long will it be until that first time is up and we move? The answer is given by Theorem 13.2, which showed us the behaviour of the minimum of the <span class="math inline">\(\Exp(q_{ij})\)</span>s is itself exponential with rate <span class="math display">\[ q_i = \sum_{j \neq i} q_{ij} . \]</span> Further, for each <span class="math inline">\(j \neq i\)</span>, the probability that we move to a given state <span class="math inline">\(j\)</span> is <span class="math display">\[ r_{ij} = \frac{q_{ij}}{\sum_{j \neq i} q_{ij}} = \frac{q_{ij}}{q_i} . \]</span> (The last two displayed equations defined the quantities <span class="math inline">\(q_i\)</span> and <span class="math inline">\(r_{ij}\)</span>.)</p>
<p>So, from state <span class="math inline">\(i\)</span>, we wait for a holding time with distribution <span class="math inline">\(\Exp(q_i)\)</span>, then move to another state, choosing state <span class="math inline">\(j\)</span> with probability <span class="math inline">\(r_{ij}\)</span>.</p>
<p>It will be convenient to write all the transition rates <span class="math inline">\(q_{ij}\)</span> down in a  <span class="math inline">\(\mathsf Q\)</span> defined as follows: the off-diagonal entries are <span class="math inline">\(q_{ij}\)</span>, for <span class="math inline">\(i \neq j\)</span>, and the diagonal entries are <span class="math display">\[ q_{ii} = - q_i = -\sum_{j \neq i} q_{ij} . \]</span> In particular, the off-diagonal entries are positive (or <span class="math inline">\(0\)</span>), the diagonal entries are negative (or <span class="math inline">\(0\)</span>) and each row adds up to <span class="math inline">\(0\)</span>.</p>
<p>One convenient way to consider a continuous time Markov jump process will be to look at the states it jumps to, leaving the holding times for later consideration. That is, consider the discrete time  <span class="math inline">\((Y_n)\)</span> associates to <span class="math inline">\((X(t))\)</span> given by <span class="math inline">\(Y_0 = X(0)\)</span> and <span class="math display">\[ Y_n = \text{state of $X(t)$ just after the $n$th jump.} \]</span> This is a discrete time Markov chain that starts from the same place as <span class="math inline">\((X(t))\)</span> does, and has transitions given by <span class="math inline">\(r_{ij} = q_{ij}/q_i\)</span>. (The jump chain cannot move from a state to itself.)</p>
<p>Once we know where the jump process will move, we can then be precise about the holding times. Specifically, from state <span class="math inline">\(i\)</span>, the holding time is <span class="math inline">\(\Exp(q_i)\)</span>.</p>
<p>The preceding discussion brings us finally to our main definition for the remainder of the course.</p>

<p>%</p>





<p>There is one point we have to be a little careful about with when dealing with continuous time processes with an infinite state space, which is the potential of `explosion’. Explosion is when the process goes off to infinity in finite time – that is when <span class="math inline">\(\mathbb P(X(t) &gt; j) &gt; 0\)</span> for all <span class="math inline">\(j\)</span>. Then some concepts can be ill defined – what state  the chain in, if it’s greater than <span class="math inline">\(j\)</span> for all <span class="math inline">\(j\)</span>?</p>
<p>Although there are often technical fixes that can get around this problem, we will not concern ourselves with them here. We shall normally deal with finite state spaces, where explosion is not a concern. When we do use infinite state space models (such as with the birth process, for example), we shall assume that explosion happens with probability <span class="math inline">\(0\)</span>.</p>



<p>Last time we defined the continuous time Markov jump process with generator matrix <span class="math inline">\(\mathsf Q\)</span> in terms of the holding times and the jump chain: for state <span class="math inline">\(i\)</span>, we wait for a holding time exponentially distributed with rate <span class="math inline">\(q_i = -q_{ii}\)</span>, then jump to state <span class="math inline">\(j\)</span> with probability <span class="math inline">\(q_{ij}/q_i\)</span>.</p>
<p>So what happens starting from state <span class="math inline">\(i\)</span> in a very small amount of time <span class="math inline">\(\tau\)</span>? The probability we don’t move is <span class="math display">\[ \mathbb P(T_1 &gt; \tau) = \ee^{-q_i \tau} = 1 - q_i\tau + o(\tau) , \]</span> where we have used the standard asymptotic <span class="math display">\[ \ee^x = 1 + x + o(x) \qquad \text{as $x \to 0$.} \]</span> So the probability we move to state <span class="math inline">\(j\)</span> is <span class="math display">\[ \mathbb P(T_1 \leq \tau)\frac{q_{ij}}{q_i} = \big(q_i\tau + o(\tau)\big) \frac{q_{ij}}{q_i}  = q_{ij}\tau + o(\tau) . \]</span> The probability we make two or more jumps is a lower order term <span class="math inline">\(o(\tau)\)</span>. Thus we have <span class="math display">\[ \mathbb P\big(X(t+\tau) = j \mid X(t) = i \big) = \begin{cases}
   1 - q_i\tau + o(\tau) &amp; \text{for } i = j \\
   q_{ij}\tau + o(\tau) &amp; \text{for } i \neq j. \end{cases} \]</span></p>
<p>This is an equivalent definition of the Markov jump process.</p>

<p>We write <span class="math inline">\(p_{ij}(t) = \mathbb P(X(t) = j \mid X(0) = 1)\)</span> for the transition probability over time <span class="math inline">\(t\)</span>. This fulfills the same role as the <span class="math inline">\(n\)</span>-step transition probability <span class="math inline">\(p_{ij}^{(n)}\)</span> did in discrete time, but now <span class="math inline">\(t\)</span> can be any positive real value. It can be convenient to use the matrix form <span class="math inline">\(\mathsf P(t) = (p_{ij}(t) : i,j \in \mathcal S)\)</span>. The idea is that we are often given the generator <span class="math inline">\(\mathsf Q\)</span>, but it’s the transition probabilities <span class="math inline">\(\mathsf P(t)\)</span> that we really want to know.</p>
<p>First, we can consider <span class="math inline">\(p_{ij}(s+t)\)</span>. To get from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> in time <span class="math inline">\(s + t\)</span>, we could first go from <span class="math inline">\(i\)</span> to some <span class="math inline">\(k\)</span> in time <span class="math inline">\(s\)</span>, then from that <span class="math inline">\(k\)</span> to <span class="math inline">\(j\)</span> in time <span class="math inline">\(t\)</span>. So we have <span class="math display">\[ p_{ij}(s+t) = \sum_{k\in \mathcal S} p_{ik}(s)p_{kj}(t) . \]</span> You should recognise this as the . In matrix form, we can write this as <span class="math display">\[ \mathsf P(s+t) = \mathsf P(s) \mathsf P(t) . \]</span> Pure mathematicians sometimes call this the `semigroup property’, so we sometimes call the matrices <span class="math inline">\((\mathsf P(t) : t \geq 0)\)</span> the .</p>
<p>Second, as in previous examples, we seek a differential equation for <span class="math inline">\(p_{ij}(t)\)</span> by looking at an infinitesimal increment <span class="math inline">\(p_{ij}(t+\tau)\)</span>.</p>
We can start with the Chapman–Kolmogorov equations. We have
<span class="math display">\[\begin{align*}
p_{ij}(t+\tau) &amp;= \sum_k p_{ik}(t)p_{kj}(\tau) \\
&amp;=p_{ij}(t)(1 - q_j\tau) + \sum_{k \neq j} p_{ik}(t)q_{kj}\tau +  o(\tau) \\
&amp;= p_{ij}(t) + \sum_k p_{ik}(t)q_{kj}\tau + o(\tau) ,
\end{align*}\]</span>
<p>where we have treated the <span class="math inline">\(k = j\)</span> term of the sum separately, and taken advantage of the fact that <span class="math inline">\(q_{jj} = - q_j\)</span>.</p>
<p>As we have done many times before, we can take a <span class="math inline">\(p_{ij}(t)\)</span> to the left hand side and divide through by <span class="math inline">\(\tau\)</span> to get <span class="math display">\[ \frac{p_{ij}(t + \tau) - p_{ij}(t)}{\tau} = \sum_k p_{ik}(t)q_{kj} + \frac{o(\tau)}{\tau} . \]</span> Sending <span class="math inline">\(\tau\)</span> to <span class="math inline">\(0\)</span> gives us the  <span class="math display">\[ p&#39;_{ij} (t) = \sum_k p_{ik}(t)q_{kj} . \]</span> The initial condition is, of course, <span class="math inline">\(p_{ii}(0) = 1\)</span> and <span class="math inline">\(p_{ij}(0) = 0\)</span> otherwise.</p>
<p>Writing <span class="math inline">\(\mathsf P(t) = (p_{ij}(t))\)</span>, and recognising the right hand side as a matrix multiplication, we get the convenient matrix form <span class="math display">\[ \mathsf P&#39;(t) = \mathsf{P}(t) \mathsf{Q}  \qquad \mathsf P(0) = \mathsf I .\]</span></p>
<p>Alternatively, we could have started with the Chapman–Kolmogorov equations as <span class="math display">\[ p_{ij}(t+\tau) = \sum_k p_{ik}(\tau)p_{kj}(t) , \]</span> with the <span class="math inline">\(\tau\)</span> in the first term rather than the second. Following through the same argument would have given the  <span class="math display">\[ \mathsf P&#39;(t) = \mathsf{Q} \mathsf{P}(t)   \qquad \mathsf P(0) = \mathsf I ,\]</span> where the <span class="math inline">\(\mathsf Q\)</span> and <span class="math inline">\(\mathsf P\)</span> are the other way round.</p>
<p>The forward and backward equations both define the transition semigroup <span class="math inline">\((\mathsf P(t))\)</span> in terms of the generator matrix <span class="math inline">\(\mathsf Q\)</span>.</p>

<p>When the state space <span class="math inline">\(\mathcal S\)</span> is finite, a crucial role is played for continuous time jump processes by the matrix exponential <span class="math inline">\(\ee^{t\mathsf Q}\)</span>. In the discrete time setting, the matrix power <span class="math inline">\(\mathsf P^n\)</span> (for integer times <span class="math inline">\(n\)</span>) was very important; the matrix exponential allows us to take matrices to the power of arbitrary real numbers, which is important for continuous time processes.</p>
<p>You may remember that the exponential function is defined by <span class="math display">\[ \ee^x = \exp(x) = \sum_{n=0}^\infty \frac{x^n}{n!} = 1 + x + \frac{x^2}{2} + \frac{x^3}{6} + \cdots. \]</span> Similarly, we can define the matrix exponential for any square matrix by <span class="math display">\[ \ee^{\mathsf A} = \exp({\mathsf A}) = \sum_{n=0}^\infty \frac{{\mathsf A}^n}{n!} = {\mathsf I} + {\mathsf A} + \frac{{\mathsf A}^2}{2} + \frac{{\mathsf A}^3}{6} + \cdots, \]</span> where we interpret <span class="math inline">\(\mathsf A^0 = \mathsf I\)</span> to be the identity matrix. In particular, since a matrix commutes with itself, we have <span class="math inline">\(\mathsf {A}^n \mathsf A = \mathsf{AA}^n\)</span>, so a matrix commutes with its own exponential, meaning that <span class="math inline">\(\mathsf A \ee^{\mathsf A} = \ee^{\mathsf A} \mathsf A\)</span>.</p>

<p>Some properties of the standard exponential include <span class="math display">\[ (\ee^x\big)^n = \ee^{nx} \qquad \frac{\mathrm d}{\mathrm d x} \ee^{ax} = a \ee^{ax} . \]</span> Similarly, we have for the matrix exponential <span class="math display">\[ (\ee^{\mathsf A}\big)^n = \ee^{n{\mathsf A}} \qquad \frac{\mathrm d}{\mathrm d t} \ee^{t{\mathsf A}} = {\mathsf A} \ee^{t{\mathsf A}} = \ee^{t{\mathsf A}} {\mathsf A} . \]</span></p>
<p>From this last expression, we have that <span class="math display">\[ \frac{\mathrm d}{\mathrm d t} \ee^{t{\mathsf Q}} = \ee^{t{\mathsf Q}} {\mathsf Q} = {\mathsf Q} \ee^{t{\mathsf Q}}  . \]</span> Comparing this to the Kolmogorov forward and backward equations <span class="math inline">\(\mathsf P&#39;(t) = \mathsf{P}(t) \mathsf Q\)</span>, we see that we have a solution to the forward equation of <span class="math inline">\(\mathsf P(t) = \ee^{t\mathsf Q}\)</span>, which also satisfies the initial condition <span class="math inline">\(\mathsf P(0) = \ee^{0\mathsf Q} = \mathsf I\)</span>.</p>
<p>Further, since <span class="math inline">\(\mathsf Q\)</span> and <span class="math inline">\(\ee^{t \mathsf Q}\)</span> commute, we also have that <span class="math inline">\(\mathsf P(t) = \ee^{t\mathsf Q}\)</span> satisfies the backward equation</p>
<p>We see that we have the semigroup property <span class="math display">\[ \mathsf P(s+t) = \ee^{(s+t)\mathsf Q} = \ee^{s \mathsf Q}\ee^{t \mathsf Q} = \mathsf P(s) \mathsf P(t) . \]</span></p>
<p>As a formal summary, we have the following.</p>




<p>When we studied discrete time Markov chains, we considered matters including communicating classes, periodicity, hitting probabilities and expected hitting times, recurrence and transience, stationary distributions, convergence to equilibrium, and the ergodic theorem. Over this lecture and the next, we develop this theory for continuous time Markov jump processes. Luckily, this will be simpler, as we will often be able to use exactly the same techniques as the discrete time case, often by looking at the discrete time jump chain.</p>

<p>In discrete time, we said that <span class="math inline">\(j\)</span> is accessible from <span class="math inline">\(i\)</span>, and wrote <span class="math inline">\(i \to j\)</span> if <span class="math inline">\(p_{ij}^{(n)} &gt; 0\)</span> for some <span class="math inline">\(n\)</span>. This allowed us to split the state space into communicating classes. We can do exactly the same in discrete time.</p>

<p>Recall that each state <span class="math inline">\(i\)</span> is in exactly one communicating class, and that communicating class is the set of all <span class="math inline">\(j\)</span> such that <span class="math inline">\(i \leftrightarrow j\)</span>.</p>
<p>Note that, if <span class="math inline">\(p_{ij}(t) &gt; 0\)</span>, then this means there is some sequence of jumps that can take us from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>. This means that, letting <span class="math inline">\(\mathsf R = (r_{ij})\)</span> be the transition matrix for the jump chain <span class="math inline">\((Y_n)\)</span>, we have that <span class="math inline">\(p_{ij}(t) &gt; 0\)</span> for some <span class="math inline">\(t\)</span> if and only if <span class="math inline">\(r_{ij}^{(n)} &gt; 0\)</span> for some <span class="math inline">\(n\)</span>. So the communicating classes for a Markov jump process are exactly the same as the communicating classes in its discrete time jump chain.</p>
<p>Further, since <span class="math inline">\(r_{ij} &gt; 0\)</span> if and only if <span class="math inline">\(q_{ij} &gt; 0\)</span>, we can tell what the communicating classes are directly from the transition rate diagram.</p>


<p>In discrete time we had to worry about periodic behaviour, especially when considering limiting behaviour, or the existence of an equilibrium distribution. However, for a continuous time jump process, we can stay in a state for any positive real-number amount of time. Thus we never see periodic behaviour, and we don’t have to worry about this.</p>
<p>This is one way in which continuous time processes are actually more pleasant to deal with than discrete time processes.</p>

<p>Recall that the hitting probability <span class="math inline">\(h_{iA}\)</span> is the probability of reach some state <span class="math inline">\(j \in A\)</span> at any point in the future starting from <span class="math inline">\(i\)</span>, and the expected hitting time <span class="math inline">\(\eta_{iA}\)</span> is the amount of time to get there. We found these in discrete time by forming simultaneous equations by conditioning on the first step.</p>
<p>For hitting probabilities, we just care about where we jump to, and not how long we wait. Thus we are free to consider only the jump chain <span class="math inline">\((Y_n)\)</span> and its transition matrix <span class="math inline">\(\mathsf R\)</span>. Everything works exactly as in discrete time.</p>

<p>For expected hitting times, we have to more careful, as we will spend a random amount of time in the current state before moving on. In particular, the time we spend in state <span class="math inline">\(i\)</span> is exponential with rate <span class="math inline">\(q_i = -q_{ii}\)</span>, so has expected value <span class="math inline">\(1/q_i\)</span>.</p>
<p>We illustrate the approach with an example.</p>

<p>We have to be a little bit careful with return times and return probabilities. The return time is the first time we come back to a state after having left it. In particular, if we begin in state <span class="math inline">\(i\)</span>, we first leave at time <span class="math inline">\(T_1 \sim \Exp(q_i)\)</span>, and we are looking for the first return after that.</p>

So, specifically, the definitions of , , and  are
<span class="math display">\[\begin{gather*}
    M_i = \min \big\{t &gt; T_1 : X(t) = i \big\} , \\
    m_i = \mathbb P\big(X(t) = i \text{ for some $t &gt; T_1$} \mid X(0) = i \big) = \mathbb P \big( M_i &lt; \infty \mid X(0) = i \big) ,\\
    \mu_i = \mathbb E\big( M_i \mid X(0) = i \big).
\end{gather*}\]</span>
<p>In particular, if <span class="math inline">\(q_i = 0\)</span>, so <span class="math inline">\(T_1 = \infty\)</span> and we never leave state <span class="math inline">\(i\)</span>, the return time, return probability and expected return time are not defined.</p>
<p>By conditioning on the first step, it’s clear we have <span class="math display">\[ m_i = \sum_j r_{ij} h_{ji} = \sum_{j \neq i} \frac{q_{ij}}{q_i} \, h_{ji}, \qquad \mu_i = \frac{1}{q_i} + \sum_j r_{ij} \eta_{ji} = \frac{1}{q_i} \bigg(1 + \sum_{j \neq i} q_{ij} \eta_{ji} \bigg). \]</span></p>
<p>Other than the <span class="math inline">\(q_i = 0\)</span> case, note that the return probability is the same in the jump chain as it is in the original Markov jump process, as was the case for the hitting probability.</p>

<p>In discrete time, we said a state was recurrent if the return probability <span class="math inline">\(1\)</span> and transient if the return probability was strictly less than <span class="math inline">\(1\)</span>. We take almost the same definition in continuous time – the difference is that if we never leave a state, because <span class="math inline">\(q_i = 0\)</span>, we should call state <span class="math inline">\(i\)</span> recurrent.</p>

<p>Note that the return probability is totally determined the jump chain, so whether a state is recurrent or transient can be decided exactly as in the discrete case. In particular, in a communicating class either all states are transient or all states are recurrent. As before, finite closed classes are recurrent, and non-closed classes are transient.</p>





<p>Our goal here is to develop the theorey of the long-term behaviour of continuous time Markov jump processes in the same way as we did for discrete time Markov chains.</p>
<p>In discrete time, we defined stationary distributions as solving <span class="math inline">\(\boldsymbol\pi\mathsf P = \boldsymbol\pi\)</span>. We then had the limit theorem, which told us about the limit of <span class="math inline">\(\mathbb P(X_n = i)\)</span>, and the ergodic theorem, which told us about the long term proportion of time spent in each state. We develop the same results here.</p>

<p>We start be defining a stationary distribution as before.</p>

<p>This initially looks rather awkward: we’re usually given a generator matrix <span class="math inline">\(\mathsf Q\)</span>, and then we might have to calculate all the <span class="math inline">\(\mathsf P(t)\)</span>s and simultaneously solve infinitely many equations. Luckily it’s much easier than that: we just have to solve <span class="math inline">\(\boldsymbol\pi \mathsf Q = \mathbf 0\)</span> (where <span class="math inline">\(\mathbf 0\)</span> is the row vector of all zeros).</p>


<p>(Strictly speaking, taking the <span class="math inline">\(\boldsymbol\pi\)</span> outside the derivative in the first step is only formally justified when the state space is finite, but the result is true for general processes.)</p>

<p>As before, we have a result on the existence and uniqueness of the stationary distribution.</p>


<p>We assume throughout that infinite-state processes are non-explosive (see Lecture 16).</p>
<p>The limit theorem tells us about the limit of <span class="math inline">\(\mathbb P(X(t) = j)\)</span> as <span class="math inline">\(t \to \infty\)</span>.</p>
<p>Recall from the discrete case that sometimes we have a distribution <span class="math inline">\(\mathbf p^*\)</span> such that <span class="math inline">\(\mathbb P(X(t) = j) \to p^*_j\)</span> for any initial distribution <span class="math inline">\(\boldsymbol\lambda\)</span>, and that such a distribution is called an . There can be at most one equilibrium distribution.</p>

<p>Note here that the conditions for convergence to an equilibrium distribution are irreducible and positive recurrent – we do not have a periodicity condition as we had in the discrete time case.</p>
<p>For the positive recurrent case, if we take the initial distribution to be `starting in state <span class="math inline">\(i\)</span> with certainty’, we see that <span class="math display">\[ p_{ij}(t) = \mathbb P\big(X(t) = j \mid X(0) = i\big) \to \pi_j , \]</span> and hence <span class="math inline">\(\mathsf P(t)\)</span> has the limiting value <span class="math display">\[ \lim_{t \to \infty} \mathsf P(t) = \begin{pmatrix} \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_n \\
\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_n \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_n \end{pmatrix} , \]</span> with each row the same. </p>

<p>The ergodic theorem refers to the long run proportion of time spent in each state.</p>
<p>We write <span class="math display">\[ V_j(t) = \int_0^t \mathbb{I}\big[X(s) = j\big]\, \mathrm ds \]</span> for the total amount of time spent in state <span class="math inline">\(j\)</span> up to time <span class="math inline">\(t\)</span>. Here, the indicator function <span class="math inline">\(\mathbb{I}\big[X(s) = j\big]\)</span> is <span class="math inline">\(1\)</span> when <span class="math inline">\(X(s) = j\)</span> and <span class="math inline">\(0\)</span> otherwise, so the integral is measuring the time we spend at <span class="math inline">\(j\)</span>. We interpret <span class="math inline">\(V_j(t)/t\)</span> to be the proportion of time up to time <span class="math inline">\(t\)</span> spent in state <span class="math inline">\(j\)</span>, and its limiting value (if it exists) to be the long-run proportion of time spent in state <span class="math inline">\(j\)</span>.</p>





<p>We start with a simple model of a process where individuals arrive, are `serviced’, and then leave.</p>
<p>We model the process as follows. Individuals arrive like a Poisson process of rate <span class="math inline">\(\lambda\)</span>. Then they receive a service which takes time <span class="math inline">\(\Exp(\mu)\)</span>, independent of everything else, before leaving.</p>
<p>This can be a useful model for many applications: for example, visitors watching a livestream on a website could arrive like a Poisson process, and watch the livestream for a <span class="math inline">\(\Exp(\mu)\)</span> amount of time. Other models could include the number of visitors in the University library, the number of insurance contracts held by an insurance broker, or</p>
<p>The number of individuals <span class="math inline">\((X(t))\)</span> in the process (that is, being serviced) at time <span class="math inline">\(t\)</span> is a Markov jump process with transition rates <span class="math inline">\(q_{i,i+1} = \lambda\)</span>, denoting arrivals, and <span class="math inline">\(q_{i,i-1} = i\mu\)</span>, for departures after service. Departures when <span class="math inline">\(i\)</span> inidividuals are being serviced have this rate because, by the memoryless property of the exponential distribution, the first service time to finish is the minimum of the <span class="math inline">\(i\)</span> exponential service times, and this is exponential with rate <span class="math inline">\(i\mu\)</span>. Thus <span class="math inline">\(q_i = -q_{ii} = \lambda + i\mu\)</span>.</p>

This process is called an M/M/<span class="math inline">\(\infty\)</span> queue.

<p>Suppose the process has been running for a long time, and we want to know what state the process is likely to be in. To find this out, we need to calculate the stationary distribution. The stationary distribution solves <span class="math inline">\(\boldsymbol\pi\mathsf Q = \mathsf 0\)</span>, or in coordinate form, <span class="math inline">\(\sum_i \pi_i q_{ij} = \pi_j\)</span>. Thus we have <span class="math display">\[
    (i+1)\mu \pi_{i+1} - (\lambda + i\mu)\pi_i + \lambda \pi_{i-1} = 0.
\]</span></p>
<p>If we check the first few values of the solution of this, we would find <span class="math display">\[ \pi_1 = \rho \pi_0 \qquad \pi_2 = \frac{\rho^2}{2} \pi_0 \qquad \frac{\rho^3}{6} \pi_0 , \]</span> where <span class="math inline">\(\rho = \lambda/\mu\)</span>. Thus it seems likely that the stationary distribution is the Poisson distribution with rate <span class="math inline">\(\rho\)</span>; that is, <span class="math display">\[ \pi_i = \ee^{-\rho} \,\frac{\rho^i}{i!} . \]</span></p>
To prove this claim, we substitute the supposed into the equation, to get
<span class="math display">\[\begin{align*}
  (i+1)\mu \,\ee^{-\rho} \frac{\rho^{i+1}}{(i+1)!} -(\lambda + i&amp;\mu) \,\ee^{-\rho} \,\frac{\rho^i}{i!} + \lambda\, \ee^{-\rho} \frac{\rho^{i-1}}{(i-1)!} \\
  &amp;= \ee^{-\rho} \mu \rho \,\frac{\rho^i}{i!} -(\lambda + i\mu) \,\ee^{-\rho} \,\frac{\rho^i}{i!} + \ee^{-\rho} \lambda \frac{i}{\rho} \frac{\rho^i}{i!} \\
  &amp;= \ee^{-\rho} \,\frac{\rho^i}{i!} \big( \lambda - (\lambda + i \mu) + i \mu   \big)  \\
  &amp;= 0 ,
\end{align*}\]</span>
<p>as desired.</p>
<p>Not also that this jump process is irreducible and positive recurrent. So, by the limit theorem, we see that <span class="math inline">\(\boldsymbol\pi\)</span> is an equilibrium distribution. Therefore, the average number of individuals being serviced is <span class="math inline">\(\rho = \lambda/\mu\)</span>, and the probability the process is empty is <span class="math inline">\(\pi_0 = \ee^{-\rho} = \ee^{-\lambda/\mu}\)</span>.</p>


<p>We now consider a slightly different process with only one server. This is called an M/M/1 queue, where the `1’ is the number of servers.</p>
<p>As before, individuals arrive as a Poisson process of rate <span class="math inline">\(\lambda\)</span>. If there is no one currently being serviced, the individual goes straight to the server, and begins an <span class="math inline">\(\Exp(\mu)\)</span> service time. Otherwise, they have to join a queue waiting for the server to become free. When the server becomes free, if the queue is nonempty, the server immediately begins to service the individual who has been waiting in the queue for the longest time.</p>
<p>This is again a very useful model for many applications. For example, it could model a lecturer’s office hours: students arrive at my office at rate <span class="math inline">\(\lambda\)</span>; if I’m free, I can immediately answer there question, which will take time <span class="math inline">\(1/\mu\)</span> on average; if I’m already dealing with a student, they must join a queue and waiting until I am free. This could also model purchases at a shop with a single till, phone calls to a receptionist, or the work of a self-employed person on different tasks.</p>
<p>Let <span class="math inline">\((X(t))\)</span> be the number of individuals in the process at time <span class="math inline">\(t\)</span>, that could be <span class="math inline">\(1\)</span> individual currently being serviced plus any number of individuals waiting in the queue. Individuals are entering the queue at rate <span class="math inline">\(q_{i,i+1} = \lambda\)</span>, and, when <span class="math inline">\(i \geq 1\)</span>, meaning someone is being serviced, we have departures at rate <span class="math inline">\(q_{i,i-1} = \mu\)</span>. Thus <span class="math inline">\(q_i = -q_{ii} = \lambda + \mu\)</span> for <span class="math inline">\(i \geq 1\)</span>, and <span class="math inline">\(q_0 = -q_{00} = \lambda\)</span>.</p>

Note that the discrete time jump chain <span class="math inline">\((Y_n)\)</span> is a simple random walk with up probability <span class="math inline">\(p = \lambda/(\lambda + \mu)\)</span>, down probability <span class="math inline">\(q = \mu/(\lambda + \mu)\)</span>, and a reflecting barrier at <span class="math inline">\(0\)</span>. Recall that this Markov chain is transient for <span class="math inline">\(p &gt; q\)</span>, null recurrent for <span class="math inline">\(p = q\)</span>, and positive recurrent for <span class="math inline">\(p &lt; q\)</span>. Thus we see that:

<p>In what follows, we consider only the case <span class="math inline">\(\lambda &lt; \mu\)</span>.</p>

<p>Again, we look for a stationary distribution, which will tell us about the long term behaviour of the queue. The equation <span class="math inline">\(\boldsymbol\pi\mathsf Q = \mathbf 0\)</span> becomes <span class="math display">\[ \mu \pi_{i+1} - (\lambda + \mu) \pi_i + \lambda \pi_{i-1} = 0 \qquad \text{for $i \geq 1$} , \]</span> together with the conditions <span class="math inline">\(\mu \pi_1 - \lambda \pi_0 = 0\)</span> and <span class="math inline">\(\sum_i \pi_i = 1\)</span>.</p>
<p>We recognise this as a linear difference equation. The characteristic equation (using <span class="math inline">\(\alpha\)</span> where we used <span class="math inline">\(\lambda\)</span> previously, since <span class="math inline">\(\lambda\)</span> is already in use) is <span class="math display">\[ \mu \alpha^2 - (\lambda + \mu)\alpha + \lambda = (\alpha - 1)(\mu \alpha - \lambda ) = 0 . \]</span> This has solutions <span class="math inline">\(\alpha = 1, \rho\)</span>, where <span class="math inline">\(\rho = \lambda/\mu &lt; 1\)</span>, so the general solution to the equation is <span class="math inline">\(\pi_i = A + B\rho^i\)</span>.</p>
<p>The initial condition on <span class="math inline">\(\pi_0\)</span> and <span class="math inline">\(\pi_1\)</span> is <span class="math display">\[  \mu \pi_1 - \lambda \pi_0 = \mu(A + B\rho) - \lambda(A + B) = A\mu + B\lambda - A \lambda - B\lambda = A(\mu - \lambda) = 0 . \]</span> Since <span class="math inline">\(\lambda &lt; \mu\)</span>, we must have <span class="math inline">\(A = 0\)</span>. The normalising condition requires <span class="math display">\[ \sum_{i=0}^\infty \pi_i = \sum_{i=0}^\infty B\rho^i = \frac{B}{1- \rho} = 1 , \]</span> so <span class="math inline">\(B = 1 - \rho\)</span>.</p>
<p>Thus we have the stationary distribution <span class="math display">\[ \pi_i = (1 - \rho)\rho^i , \]</span> which is a geometric distribution. By the ergodic theorem, the server will be free a proportion <span class="math inline">\(\pi_0 = 1 - \rho = 1 - \lambda/\mu\)</span> of the time, in the long run. The long run average number of individuals in the process will be <span class="math display">\[ \sum_{i=0}^\infty i \pi_i = \frac{\rho}{1-\rho} = \frac{\lambda}{\mu - \lambda} . \]</span> Note that if <span class="math inline">\(\lambda\)</span> is only slightly smaller than <span class="math inline">\(\mu\)</span> then this number can get very large.</p>






<p>%</p>


<p> </p>





<p>This question was about the simple random walk. We had a simple random walk <span class="math inline">\((X_n)\)</span> with up probability <span class="math inline">\(p\)</span> and down probabiltiy <span class="math inline">\(q\)</span>, and the question asked for <span class="math inline">\(\mathbb P(X_4 = 2 \mid X_8 = 6)\)</span>. This seems confusing, because it looks like we’re conditioning on the future behaviour of the random walk.</p>
<p>One way to solve the question is using a standard trick: Bayes’ formula tells us that <span class="math display">\[ \mathbb P(A \mid B) = \frac{\mathbb P(A \cap B)}{\mathbb P(B)} = \frac{\mathbb P(B \mid A) \, \mathbb P(A)}{\mathbb P(B)} . \]</span> This is a very convenient way to swap the order of a conditional probability from an awkward way round to a convenient way round. In this case, we get <span class="math display">\[ \mathbb P(X_4 = 2 \mid X_8 = 6) = \frac{\mathbb P(X_8 = 6 \mid X_4 = 2) \, \mathbb P(X_8 = 6)}{\mathbb P(X_4 = 2)} . \]</span> It should be easy to caculate each of these terms individually: <span class="math inline">\(\mathbb P(X_4 = 2) = 4p^3q\)</span>, as we must take <span class="math inline">\(3\)</span> steps up and <span class="math inline">\(1\)</span> step down; <span class="math inline">\(\mathbb P(X_8 = 6) = 8p^7q\)</span>, as we must take <span class="math inline">\(7\)</span> steps up and <span class="math inline">\(1\)</span> step down; and <span class="math inline">\(\mathbb P(X_8 = 6 \mid X_4 = 2) = p^4\)</span>, as, starting from <span class="math inline">\(X_4 = 2\)</span>, the next <span class="math inline">\(4\)</span> steps must all be up. Hence <span class="math display">\[ \mathbb P(X_4 = 2 \mid X_8 = 6) = \frac{p^4 \times p^3q}{8p^7q} = \frac{4p^7q}{8p^7q} = \frac48 = \frac12 . \]</span></p>
<p>Another way to do it would be to just think carefully about what’s going on. A walk of <span class="math inline">\(8\)</span> steps going to <span class="math inline">\(X_8 = 6\)</span> will consist of <span class="math inline">\(7\)</span> steps up and <span class="math inline">\(1\)</span> step down. If the step up happens at time 1, 2, 3 or 4, then <span class="math inline">\(X_4 = 2\)</span>, while is the step up happens at time 5, 6, 7 or 8, then <span class="math inline">\(X_4 = 4\)</span>. Hence <span class="math inline">\(\mathbb P(X_4 = 2 \mid X_8 = 6) = \frac48 = \frac12\)</span>.</p>

<p>%\begin{question} %What’s the definition of a communicating class? %\end{question}</p>
<p>%</p>

<p>We have the limit and ergodic theorems to tell us about irreducible Markov chains, where all the states are in the same communicating class.</p>
For finite non-irreducible chains the idea is:

<p>For example, consider the Markov chain with the following transition diagram:</p>
<pre><code>\begin{center}
\begin{tikzpicture}[shorten &gt;=1pt,scale=1]
\node[state] (1) at (0,0) {$1$};
\node[state] (2) at (2,0) {$2$};
\node[state] (3) at (4,0) {$3$};
\node[state] (4) at (6,0) {$4$};
\node[state] (5) at (8,0) {$5$};

\path[-&gt;] (1) edge [bend left]  node [above] {$\frac23$} (2);
\path[-&gt;] (2) edge [bend left]  node [below] {$\frac13$} (1);
\path[-&gt;] (3) edge              node [above] {$\frac35$} (2);
\path[-&gt;] (3) edge              node [above] {$\frac25$} (4);
\path[-&gt;] (4) edge [bend left]  node [above] {$\frac34$} (5);
\path[-&gt;] (5) edge [bend left]  node [below] {$\frac12$} (4);

\path[-&gt;] (1) edge [loop left]  node [left]  {$\frac13$} ();
\path[-&gt;] (2) edge [loop above] node [above] {$\frac23$} ();
\path[-&gt;] (4) edge [loop above] node [above] {$\frac14$} ();
\path[-&gt;] (5) edge [loop right] node [right] {$\frac12$} ();
\end{tikzpicture}
\end{center}</code></pre>
<p></p>
<p>First, we see the we have two positive recurrent communicating classes <span class="math inline">\(\{1,2\}\)</span> and <span class="math inline">\(\{4,5\}\)</span>, and one transient class <span class="math inline">\(\{3\}\)</span>. So, starting from <span class="math inline">\(3\)</span>, we will either end up in class <span class="math inline">\(\{1,2\}\)</span> and stay there forever, or end up in class <span class="math inline">\(\{4,5\}\)</span> and stay there forever. We will eventually leave <span class="math inline">\(3\)</span>, so <span class="math display">\[ \mathbb P(X_n = 3 \mid X_0 = 3) \to 0 \]</span></p>
<p>In this case, we can see immediately see that we end up in <span class="math inline">\(\{1,2\}\)</span> with probability <span class="math inline">\(h_{31} = h_{32} = \frac35\)</span> and in <span class="math inline">\(\{4,5\}\)</span> with probability <span class="math inline">\(h_{34} = h_{35} = \frac25\)</span>, since we in fact leave <span class="math inline">\(3\)</span> immediately.</p>
Within <span class="math inline">\(\{1,2\}\)</span> we seek a stationary distribution, and it’s not difficult to solve <span class="math inline">\(\boldsymbol\pi \mathsf P = \boldsymbol\pi\)</span> find <span class="math inline">\(\boldsymbol\pi = (\frac13,\frac23,0,0,0)\)</span>. Hence
<span class="math display">\[\begin{align*}
\mathbb P(X_n = 1 \mid X_0 = 3) &amp;\to h_{31}\pi_1 = \tfrac35 \times \tfrac13 = \tfrac15 \\
\mathbb P(X_n = 2 \mid X_0 = 3) &amp;\to h_{32}\pi_2 = \tfrac35 \times \tfrac23 = \tfrac25 .
\end{align*}\]</span>
Similarly, on <span class="math inline">\(\{4,5\}\)</span> we find a stationary distribution <span class="math inline">\((0,0,0,\frac25,\frac35)\)</span>. So
<span class="math display">\[\begin{align*}
\mathbb P(X_n = 4 \mid X_0 = 3) &amp;\to  \tfrac25 \times \tfrac25 = \tfrac{4}{25} \\
\mathbb P(X_n = 5 \mid X_0 = 3) &amp;\to  \tfrac25 \times \tfrac35 = \tfrac{6}{25} .
\end{align*}\]</span>

<p>For continuous time Markov jump processes, we are normally given the generator matrix <span class="math inline">\(\mathsf Q\)</span>, which tells us about the probability of changes in infinitesimal time periods. But we might might be more interested in the transition probabilities <span class="math inline">\(\mathsf P(t) = (p_{ij}(t))\)</span>, where <span class="math display">\[ p_{ij}(t) = \mathbb P\big(X(t) = j \mid X(0) = i \big) . \]</span></p>
The forward and backward equations tell us how these relate to each other. In matrix form, we have
<span class="math display">\[\begin{align*}
&amp;\text{Forward equation:}  &amp; \mathsf P&#39;(t) &amp;= \mathsf P(t) \mathsf Q \\
&amp;\text{Backward equation:}  &amp; \mathsf P&#39;(t) &amp;= \mathsf Q\mathsf P(t)  \\
&amp;\text{Initial condition:}  &amp; \mathsf P(t)&amp; = \mathsf I .
\end{align*}\]</span>
Alternatively, in coordinate form, we have
<span class="math display">\[\begin{align*}
&amp;\text{Forward equation:}  &amp; p&#39;_{ij}(t) &amp;=  \sum_k p_{ik}(t)q_{kj} \\
&amp;\text{Backward equation:}  &amp; p&#39;_{ij}(t) &amp;= \sum_k q_{ik} p_{kj}(t) \\
&amp;\text{Initial conditions:}  &amp; p_{ii} = 1 \quad  &amp;\qquad p_{ij} = 0 \quad \text{for $i \neq j$}. 
\end{align*}\]</span>
You should:

<p>You are not expected to be able to explicitly solve the forward and backward differential equations (except in perhaps the most trivial cases).</p>


For continuous time processes, the  is crucial. If <span class="math inline">\(T \sim \Exp(\lambda)\)</span>, then:

For the Poisson process, you will need to know about the . If <span class="math inline">\(X \sim \text{Po}(\lambda)\)</span>, then

For knowing about the exact distribution of the simple random walk, you will need to know about the . If <span class="math inline">\(X \sim \text{Bin}(n,p)\)</span>, then, letting <span class="math inline">\(q = 1-p\)</span>:

<p>You should also know how to calculate approximate probabilities by using the  as an approximation in suitable circumstances.</p>
<p>You will  be expected to remember facts about the geometric distribution.</p>


Some suggestions:

<p> </p>







The rough breakdown of material in the exam is:

<p>Material that was only covered in the computational practicals (eg. how to simulate Markov chains in R) will not be examined.</p>
<p>The `Summary of Part I’ and ‘Summary of Part II’ handouts are strongly encouraged for use in directing what material you revise.</p>

The following instructions words will appear on the exam:


<p>First, the questions in the exam are (in my opinion) somewhat easier than those in the assignments, and also easier than the hardest questions on the problem sheets (see below).</p>
<p>Second, the exam has been carefully written in order to give you the best possible chance to demonstrate what you’re learned in this course. There are no trick questions.</p>
<p>Good luck!</p>


The 10 problem sheets and 3 assignments are strongly recommended for revision work, with a few exceptions:


<p>The 2017 and 2018 exam papers are available online, with `checksheets’ of partial solutions.</p>
One important point: the last quarter of the course on general Markov jump processes is a bit different to previous years. In particular you are not expected to be able to answer the following questions:

Good questions to use instead to practice the new material include:


<p>(This past paper question has been lightly edited to make the notation match with that used this year.)</p>

<p>Part (i) is a `definition’ question, so simply requires a statement of the definition. There are a few equivalent ways to define the simple random walk, but this is one:</p>



<p>Part (ii) is a typical sort of question – it’s basically wanting to make sure you’ve learned what the Markov property is.</p>




<p>Since this is a `Prove that’ question, we will explain our reasoning carefully, using lots of words to accompany any equations.</p>




<p>First we will need to set up a linear difference equation (a proof problem, where we will use plenty of text to explain our arguments) and second we will solve the equation (a process problem, where we will make sure to show our working).</p>





<p>This is a typical final part of a question, that will require some original thinking on an unseen problem. Here, we can start by finding the expected number of occasions the process reaches <span class="math inline">\(0\)</span>, then we can later decide if it seems reasonable. Since the questions ask `whether or not you would say the result seems reasonable’, a heuristic justification will be fine – the question is not asking for a detailed statistical hypothesis test.</p>



<!--chapter:end:index.Rmd-->
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
