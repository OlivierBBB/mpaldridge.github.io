---
title: "MATH2750 Introduction to Markov Processes"
author: "Matthew Aldridge"
date: "Semester 2, 2019--20"
site: bookdown::bookdown_site
output: bookdown::gitbook

---

\newcommand{\Var}{\mathrm{Var}}

# (PART\*) Introduction {-}


# About the module {-}

This module is `MATH2750` **Introduction to Markov Processes**. The module organiser and lecturer is Dr Matthew Aldridge, and my email address is `m.aldridge@leeds.ac.uk`.

## Organisation  of the module {-}

This module lasts for 11 weeks. The first nine weeks run from 28 January to 29 March, then we break for Easter, then the final two weeks run from 29 April to 10 May.

### Lectures {-}

There are two lectures each week, for a total of 22 lectures. Lectures are on Tuesdays at 1400 and Thursdays at 1000, both in Roger Stevens LT 20 (7M.20). Attendance at lectures is mandatory.

Outline lecture notes summarising the main definitions and theorems from the course will be made available on Minerva. I'm very keen to hear about errors, mathematical, typographical or otherwise, in the lecture notes -- please email me or talk to me after lectures. The notes are not a substitute for attending the lectures. The lectures will be videoed on the lecture capture system.

### Problem sheets {-}

There will be 10 problem sheets; Problem Sheet $n$ covers the material from two lectures in week $n$, and will be discussed in your workshop in week $n+1$.

The best way of learning the material in this course is to spend plenty of time working on the problem sheets in advance of your workshop and writing up your answers. Collaboration is encouraged when working through the problems, but I recommend writing up your work on your own.

### Workshops {-}

There will be 10 workshops, starting in the second week. The main goal of the workshops will be to go over your answers to the problems sheets in smaller classes. You will have been assigned to one of five workshop groups, meeting on Mondays or Tuesdays, led by Dr Andrew Baczkowski, Dario Domingo (two groups), Dr Graham Murphy or me. Check your timetable for details.

### Assessments {-}

There will be three pieces of assessed coursework, each making up 5\% of your mark for the module, for a total of 15\%. These will involve writing up answer to a few problems, in a similar style to the problem sheets. While you may want to discuss the assessment with others in advance of completing it by yourself, copying is not allowed and will be dealt with in accordance with University rules.

The assessments will be due on Thursdays 21 February, 14 March and 2 May at 1400.

### Computer practicals {-}

There will be two computer practicals, to be completed in the fourth and seventh weeks of the module. There will be practical classes run during those weeks where you can work on the problems, get help with any difficulties, and  have your work marked. (This does not form part of your official mark for the module.)

### Exam {-}

There will be a two-hour exam after the end of the module, making up the other 85\% of your mark. The exam will consist of four questions, and you are expected to answer all of them. We will talk more about the exam in the revision sessions (Lectures 21 and 22).

### Office hours {-}

I will run office hours on Mondays and Wednesdays at 1500 in my office: 9.320, Physics Research Deck. This is your opportunity to discuss with me anything that you want from the course, including material you are confused about and problems you don't understand. If you can't make these times, try emailing me to arrange a meeting, or knock on my office door and see if I'm free.

(The Physics Research Deck is easiest to reach from the 8th floor of EC Stoner at staircase 4. If you haven't been there before, I recommend asking a friend who has for directions. Once in the maths area on the 9th floor of the PRD, my office is back on yourself, towards the Observatory.)

## Content of the module {-}

The course has two major parts: the first part will cover processes in discrete time and the second part processes in continuous time.

An outline plan of the lectures is the following:

* Introduction to stochastic processes [1 lecture]
* Discrete time Markov chains [10 lectures]
    * Important examples: Random walk, gambler's ruin, linear difference equations [3 lectures]
    * General theory: transition probabilities, communicating classes, hitting times, recurrence and transience, stationary distribution, long-term behaviour [7 lectures]
*	Continuous time Markov jump processes [9 lectures]
    * Important examples: Poisson process, birth processes [4 lectures]
    * General theory: holding times and jump chains, forward and backward equations, hitting times, stationary distribution, long-term behaviour, queues [5 lectures]
* Revision [2 lectures]

### Books {-}

You can do well on this module by attending the lectures and workshops, plus working on the problem sheets, assignments and practicals, without any further reading. However, for students who would like some book recommendations for optional extra background reading or an alternative view on the material, I recommend the following.

The two books I found most useful in planning the course were:

* J.R. Norris, *Markov Chains*, Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press, 1997. Chapters 1-3.
* G.R. Grimmett and D.R. Stirzaker, *Probability and Random Processes*, 3rd edition, Oxford University Press, 2001. Chapter 6.

Norris discusses only Markov processes, and has some more detailed material that goes beyond this module. Chapter 1 on discrete time Markov chains is [available online](http://www.statslab.cam.ac.uk/~james/Markov/). Grimmet and Stirzaker is an excellent handbook that covers most of undergraduate probability. Chapter 9 on Markov processes is available on Minerva.


The approach of Grimmet and Stirzaker is very closely mirrored in

* G. Grimmet and D. Walsh, *Probability: an introduction*, 2nd edition, Oxford University Press, 2014. Chapter 12.

which might be useful if you have a copy to hand, or if the library runs out of Grimmet and Stirzaker.

A gentler introduction with plenty of examples is provided by

* P.W. Jones and P. Smith, *Stochastic Processes: an introduction*, 3nd edition, Texts in Statistical Science, CRC Press, 2018. Chapters 2-7.

although it doesn't cover everything in this module. It's available online via the University library.

Finally, I've heard good things about

* D.R. Stirzaker, *Elementary Probability*, 2nd edition, Cambridge University Press, 2003. Chapter 9.

although I haven't used it myself. It's also available online.

(I also benefited from reading lecture notes from former lecturers of this course, particularly Dr Graham Murphy, whose help was invaluable.)



# Stochastic processes and the Markov property


```{block, type='summ'}
* Stochastic processes with discrete/continuous state space and discrete/continuous time
*	The Markov 'memoryless' property
```


## Deterministic and random models

A *model* is an imitation of a real-world system. For example, you might want to have a model to imitate the world's population, the level of water in a reservoir, future cashflows of a pension scheme, or future stock prices.
Models allow us to try to predict what might happen in the real world in a low risk, cost effective and fast way.

To design a model requires a set of assumptions about how it will work and suitable parameters need to be determined, perhaps based on past collected data.

An important distinction is between *deterministic* models and *random*} (or *stochastic*) models. Deterministic models do not contain any random components, so the output is completely determined by the inputs and any parameters. Random models have variable outcomes, so can be run many times to give a sense of the range of possible outcomes.

Consider models for:

1. the future position of the Moon as it orbits the Earth,
2. the future price of shares in Apple.

In (a), the random components -- for example, the effect of meteorites striking the Moon's surface -- are not very significant and a deterministic model based on physical laws is good enough for most purposes. In (b), the share price from day to day is highly uncertain, so a random model can take into account the variability and unpredictability in a useful way.

In this module we will see many examples of stochastic models.  Lots of the applications we will consider come from financial mathematics and actuarial science where the use of models that take into account uncertainty is very important, but the principles apply in many areas.


## Stochastic processes

If we want to model, for example, the total number of claims to an insurance company in the whole of 2019, we can use a random variable $X$ to model this -- perhaps a Poisson distribution with an appropriate mean, for example. However, if we want to track how the number of claims changes over the course of the year 2019, we will need to use a *stochastic process*. (The word 'stochastic' means the same thing as 'random'.)

A stochastic process, which we will write as $(X_n)$, is an indexed sequence of random variables that are (usually) dependent on each other. Each random variable $X_n$ takes a value in a *state space* $\mathcal S$ which is the set of possible values for the process. As with usual random variables, the state space $\mathcal S$ can be *discrete* or *continuous*. A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, $\mathcal S = \{\text{Heads},\text{Tails}\}$ is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers $\mathcal S = \mathbb Z_+ = \{0,1,2,\dots\}$. A continuous state spaces denotes an uncountably infinite continuum of gradually varying outcomes. For example, the nonnegative real line $\mathcal S = \mathbb R_+ = \{x \in \mathbb R : x \geq 0\}$ is the state space for the amount of rainfall on a given day, while a bounded subset of $\mathbb R^3$ is the state space for the position of a gas particle in a box.

Further, the process has an *index set* that puts the random variables that make up the process in order. The index set is usually interpreted as a time variable, telling us when the process will be measured. 
The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at different points, often denoted by $n = 0,1,2,\dots$, while continuous time denotes a process monitored constantly over time, often denoted by $t \in \mathbb R_+ = \{x \in \mathbb R : x \geq 0\}$. In the insurance example, we might count up the number of claims each day -- then the discrete index set will be the days of the year, which we could denote $\{1,2,\dots,365\}$. Alternatively, we might want to keep a constant tally that we update after every claim, requiring a continuous time index representing time across the whole year. In discrete time, we can write down the first few steps of the process as $(X_0, X_1, X_2, \dots)$.

This gives us four possibilities in total:

* Discrete time, discrete space
    * Example: Number of students turning up to each lecture of \texttt{MATH2750}.
    * *Markov chains* -- discrete time, discrete space stochastic processes with a certain 'Markov property' -- are the main topic of the first half of this module.
* Discrete time, continuous space
    * Example: Daily maximum temperature in Leeds.
    * We will briefly mention continuous space Markov chains in the first half of the course.
* Continuous time, discrete space
    * Example: Number of visitors to a webpage over time.
    * *Markov jump processes* -- continuous time, discrete space stochastic processes with the 'Markov property' -- are the main topic of the second half of this module.
* Continuous time, continuous space
    * Example: Level of the FTSE 100 share index over time.
    * Such processes, especially the famous Brownian motion -- another process with the Markov property -- are very important, but outside the scope of this course.  See `MATH3733` Stochastic Financial Modelling next year, for example.


## Markov property

Because stochastic processes consist of a large number -- even infinitely many -- random variables that could all be dependent on each other, they can get extremely complicated. The Markov property is a crucial property that restricts the type of dependences in a process, to make the process easier to study, yet still leaves most of the useful and interesting examples intact.

Think of a simple board game where we roll a die and move that many squares forward on the board. Suppose we are currently on the square $X_n$. Then which square $X_{n+1}$ we move to on our next turn:

* is random, since it depends on the roll of the die;
* depends on where we are now $X_n$, since the score of die will be added onto the number our current square;
* given the square we are now $X_n$, it doesn't depend any further on which sequence of squares $X_0, X_1, \dots, X_{n-1}$ we used to get here.

It is this third point that is the crucial property of the stochastic processes we will study in this course, and it is called the *Markov property* or *memoryless property*. We say 'memoryless', because it's as if the process forgot how it got here -- the process before this moment has no bearing on the future given where we are now. A mathematical way to say this is that 'the past and the future are conditionally independent given the present.'

To write this down formally, we need to recall *conditional probability*: the conditional probability of an event $A$ given another event $B$ is written $\mathbb P(A \mid B)$, and is the probability that $A$ occurs *given* that $B$ definitely occurs. You may remember the definition
  \[ \mathbb P(A \mid B) = \frac{\mathbb P(A \cap B)}{\mathbb P(B)} , \]
although is often more useful to reason directly about conditional probabilities than use this formula.

```{definition}
	Let $(X_n) = (X_0, X_1, X_2, \dots)$ be a stochastic process in \mbox{discrete} time $n = 0,1,2,\dots$. Then we say that $(X_n)$ has the *Markov property* or is *Markovian* if, for all times $n$ and all states $x_0, x_1, \dots,x_n, x_{n+1} \in \mathcal S$ we have
	\[ 	\mathbb P(X_{n+1}=x_{n+1} \mid X_{n}=x_{n}, \dots,X_1 = x_1, X_0=x_0)=\mathbb P(X_{n+1}=x_{n+1} \mid X_{n}=x_{n}) . \]
```

Here, the left hand side is the probability we go to state $x_{n+1}$ next conditioned on the entire history of the process, while the right hand side is the probability we go to state $x_{n+1}$ next conditioned only on where we are now. So this property tells us that it only matters where we are now and not how we got here.

(There's also a similar definition for continuous time processes, which we'll come to later in the course.)

Stochastic processes that have the Markov property are much easier to study than general processes, as we only have to keep track of where we are now and we don't have to keep track of the entire history that came before.






# (PART\*) Part I: Discrete time Markov chains {-}


# Random walk

```{block, type='summ'}
* Definition  of the simple random walk and the exact binomial distribution
* Expectation and variance of general random walks
```

## Simple random walk

Consider the following *simple random walk* on the integers $\mathbb Z$: We start at $0$, then at each time-step, we go up by one with probability $p$ and down by one with probability $q = 1-p$. When $p = q= 1/2$, we're equally as likely to go up as down, and call this the *simple symmetric random walk*.

The simple random walk can be used as simplified model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In most modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the 'drunkard's walk', suggesting it could model a drunk person trying to stagger home.

```{r}
steps <- 20
p <- 2/3
Z <- rbinom(steps, 1, p)
Z <- 2*Z - 1
X <- c(0, cumsum(Z))
plot(0:steps, X, xlab="n", ylab="X_n", main="Simple random walk, p = 2/3", type="b")
p <- 1/3
Z <- rbinom(steps, 1, p)
Z <- 2*Z - 1
X <- c(0, cumsum(Z))
plot(0:steps, X, xlab="n", ylab="X_n", main="Simple random walk, p = 1/3", type="b")
```

We can write this as a stochastic process $(X_n)$ with discrete time $n = \{0,1,2,\dots\} = \mathbb Z_+$ and discrete state space $\mathcal S = \mathbb Z$, where $X_0 = 0$ and, for $n \geq 0$, we have
  \[ X_{n+1} = \begin{cases} X_n + 1 & \text{with probability $p$,} \\
                             X_n - 1 & \text{with probability $q$.} \end{cases} \]

It's clear from this definition that $X_{n+1}$ (the future) depends on $X_n$ (the present), but, given $X_n$, does not depend on $X_{n-1}, X_{n-1}, \dots, X_0$ (the past). Thus the Markov property holds, and the simple random walk is a *Markov process* or *Markov chain*.

```{example}
*What's the probability that after two steps a simple random walk has reached $X_2 = 2$?*

To achieve this, the walk must go upwards in both time steps, so $\mathbb P(X_2 = 2) = pp = p^2$.
```

&nbsp;

```{example}
*What's the probability that after three steps a simple random walk has reached $X_3 = -1$?*

There are three ways to reach $-1$ after three steps: up--down--down, down--up--down, or down--down--up. So 
\[ \mathbb P(X_3 = -1) = pqq+qpq+qqp = 3pq^2 . \]
```


## General random walks

Note that alternative way to write the simple random walk is to put
  \[ X_n = X_0 + \sum_{i=1}^n Z_i , \qquad (*) \]
where the starting point is $X_0 = 0$ and the increments $Z_1, Z_2, \dots$ are independent and identically distributed (IID) random variables with distribution given by $\mathbb P(Z_i = 1) = p$ and $\mathbb P(Z_i = -1) = q$. You can check that this means $X_{n+1} = X_n + Z_{n+1}$, and that this property defines the simple random walk.

In fact, any stochastic process with the form $(*)$ for some $X_0$ and some distribution for the $Z_i$s is called a *random walk*.

Random walks often have state space $\mathcal S = \mathbb Z$, like the simple random walk, but they could be defined on other state spaces. We could look at higher dimensional simple random walks: in $\mathbb Z^2$, for example, we could step up, down, left or right with given probabilities. We could even have a continuous state space like $\mathbb R$, if, for example, the $Z_i$s had a normal distribution. 

We can use this structure to calculate the expectation or variance of any random walk (including the simple random walk).

Let's start with the expectation. For a random walk $(X_n)$ we have
  \[ \mathbb E X_n = \mathbb E \left(X_0 + \sum_{i=1}^n Z_i\right) = \mathbb E X_0 + \sum_{i=1}^n \mathbb E Z_i = \mathbb EX_0 + n \mathbb E Z_1 , \]
where we've used the linearity of expectation, and that the $Z_i$s are identically distributed.

In the case of the simple random walk, we have $\mathbb E X_0 = 0$, since we start from $0$ with certainty, and
  \[ \mathbb E Z_1 = \sum_{z \in \mathbb Z} z \mathbb P(Z_1 = z) = 1\times p + (-1)\times q = p-q ,\]
so $\mathbb EX_n = n(p-q)$.

If $p > 1/2$, then $p > q$, so $\mathbb E X_n$ grows ever bigger over time, while if $p < 1/2$, then $\mathbb E X_n$ grows ever smaller (that is, negative with larger absolute value) over time. If $p = 1/2 = q$, which is the case of the simple symmetric random walk, then then the expectation $\mathbb E X_n = 0$  is zero for all time.

Now the variance of a random walk. We have
  \[ \Var(X_n) = \Var \left(X_0 + \sum_{i=1}^n Z_i\right) = \Var X_0 + \sum_{i=1}^n \Var Z_i = \Var X_0 + n \Var Z_1 , \]
where it was crucial that $X_0$ and all the $Z_i$s were independent (so we had no covariance terms).

Again, for a simple random walk $\Var (X_0) = 0$, since we always start from $0$. To calculate the variance, we write
  $$\begin{align}
  \Var (Z_1) &= \mathbb E Z_1^2 - (\mathbb EZ_1)^2 \\
            &= 1^2 \times p  + (-1)^2 \times q - (p-q)^2 \\
            &= p + q - (p-q)^2 \\
            &= 1 - (2p - 1)^2 \\
            &= 4p - 4p^2 \\
            &= 4pq ,
  \end{align}$$
where we've used that $q = 1-p$. Hence the variance of the simple random walk is $4pqn$. Note that (unless $p$ is $0$ or $1$) the variance grows over time, so it becomes harder and harder to predict where the random walk will be.

The variance of the simple symmetric random walk is $4 \frac12 \frac12 n = n$.

For large $n$, we can use a normal approximation for a random walk. Suppose the increments process $(Z_n)$ has mean $\mu$ and variance $\sigma^2$, and that the walk starts from $X_0 = 0$. Then we have $\mathbb E X_n = \mu n$ and $\Var(X_n) = \sigma^2 n$, so for large $n$ we can use the normal approximation $X_n \approx \mathrm{N}(\mu n, \sigma^2 n)$. (Note, of course, that the $X_n$ are not independent.) To be more formal, the central limit theorem tells us that, as $n \to \infty$, we have
  \[ \frac{X_n - n\mu}{\sigma \sqrt{n}} \to \mathrm{N}(0,1) . \]


## Exact distribution of the simple random walk

In the case of the simple random walk, we can in fact give the exact distribution by writing down a formula for $\mathbb P(X_n = i)$ for any time $n$ and state $i$.

Recall that, at each of the first $n$ times, we take an upward step with probability $p$, and otherwise take a downward step. So if we let $Y_n$ be the number of upward steps over the first $n$ times, we see that $Y_n$ has a binomial distribution $Y \sim \text{Bin}(n,p)$.

Recall that the binomial distribution has probability
  \[  \mathbb P(Y_n = k)  = \binom nk p^k (1-p)^{n-k} = \binom nk p^k q^{n-k} , \]
for $k = 0,1,\dots, n$, where $\binom{n}{k}$ is a binomial coefficient '$n$ choose $k$'.

If $Y_n = k$, that means we've taken $k$ upward steps and $n-k$ downward steps, leaving us at position $k - (n-k) = 2k - n$. Thus we have that
  \[ \mathbb P(X_n = 2k - n) = \mathbb P(Y_n = k) = \binom nk p^k q^{n-k} . \qquad (**) \]

Note that after an odd number of time steps $n$ we're always at an odd-numbered state, since $2k - \text{odd} = \text{odd}$, while after an even number of time steps $n$ we're always at an even-numbered state, since $2k - \text{even} = \text{even}$.

Writing $i = 2k - n$, so $k = (n+i)/2$ and $n-k = (n-i)/2$, we can rearrange $(**)$ to see that the distribution for the simple random walk is
  \[ \mathbb P(X_n = i) = \binom{n}{(n+i)/2} p^{(n+i)/2} q^{n - (n+i)/2} = \binom{n}{(n+i)/2} p^{(n+i)/2} q^{(n-i)/2} , \]
when $n$ and $i$ have the same parity with $-n \leq i \leq n$, and is $0$ otherwise.

In the special case of the simple symmetric random walk, we have
  \[ \mathbb P(X_n = i) = \binom{n}{(n+i)/2} \left(\frac12\right)^{(n+i)/2} \left(\frac12\right)^{(n-i)/2} = \binom{n}{(n+i)/2} 2^{-n} . \]
  
  
  
  
# Gambler's ruin

```{block, type='summ'}
* The gambler's ruin Markov chain
* Equations for probability of ruin and expected duration of the game by conditioning on the first step
```

## Gambler's ruin Markov chain

Consider the following gambling problem. Alice is gambling against Bob. Alice starts with £$a$ and Bob starts with £$b$. It will be convenient to write $m = a + b$ for the total amount of money, so Bob starts with £$(m-a)$. At each step of the game, both players bet £$1$; Alice wins £$1$ off Bob with probability $p$, or Bob wins £$1$ off Alice with probability $q$. The game continues until one player is out of money.

Let $X_n$ denote how much money Alice has after $n$ steps of the game. We can write this as a stochastic process with discrete time $n = \{0,1,2,\dots\} = \mathbb Z_+$ and discrete state space $\mathcal S = \{0,1,\dots,m\}$, where $X_0 = a$ and, for $n \geq 0$, we have
\[ X_{n+1} = \begin{cases} X_n + 1 & \text{with prob. $p$ if $1\leq X_n \leq m-1$,} \\
                           X_n - 1 & \text{with prob. $q$ if $1\leq X_n \leq m-1$,} \\
                           0       & \text{if $X_n = 0$,} \\
                           m       & \text{if $X_n = m$.} \end{cases} \]
Note also that the gambler's ruin process $(X_n)$ clearly satisfies the Markov property: the next step $X_{n+1}$ depends on where we are now $X_n$, but, given that, does not depend on how we got here.

The gambler's ruin process is exactly like a simple random walk started from $X_0 = a$ except that we have *absorbing barriers* and $0$ and $m$, where the game stops because one of the players has 'ruined' -- that is, lost all their money. (One can also consider random walks with *reflecting barriers*, that bounce the random walk back into the state space, or *mixed barriers* that are absorbing or reflecting at random.) 


## Probability of ruin

The gambling game continues until either Alice is ruined ($X_n = 0$) or Bob is ruined ($X_n = m$). A natural question to ask is: What is the probability that the game ends in Alice's ruin?

Let us write $r_i$ for the probability Alice end up ruined if she has £$i$. Then the probability of ruin for the whole game is $r_a$, since Alice initially starts with £$a$.
The probability Bob is ruined is $1 - r_a$, since eventually one of the players must lose.

What can we say about $r_i$? Clearly we have $r_0 = 1$ and $r_m = 0$, since this means Alice ($i=0$) or Bob ($i=m$) is out of money and is ruined. What about for $1 \leq i \leq m-1$?

The key is to *condition on the first step*. That is, we can write
\begin{align}
\mathbb P(\text{ruin}) &= \mathbb P(\text{win first round}) \, \mathbb P(\text{ruin} \mid \text{win first round}) \\
&\qquad{}+ \mathbb P(\text{lose first round}) \, \mathbb P(\text{ruin} \mid \text{lose first round}) \\
&= p\,\mathbb P(\text{ruin} \mid \text{win first round}) \\
&\qquad{}+ q \,\mathbb P(\text{ruin} \mid \text{lose first round}) .
\end{align}
Here we have conditioned on whether Alice wins or loses the first round. More formally, we have used the *law of total probability*, which says that if the disjoint events $B_1, \dots, B_k$ cover the whole sample space, then
\[ \mathbb P(A) = \sum_{i=1}^k \mathbb P(A \cap B_i) = \sum_{i=1}^k \mathbb P(B_i) \, \mathbb P(A \mid B_i) . \]
This idea of conditioning on the first step will be a crucial tool throughout this module.

Note that if Alice wins the first round from having £$i$, she now has £$(i+1)$. By the Markov property, we now see that her probability of ruin is $r_{i+1}$, because it's as if the game were starting again with Alice having £$(i+1)$ to start with. The Markov property tells us that it doesn't matter *how* Alice got to having £$(i+1)$, it only matters how much she has now. Similarly, if Alice loses the first round, she now has £$(i-1)$, and the ruin probability is $r_{i-1}$. Hence we have
  \[ r_i = pr_{i+1} + qr_{i-1}. \]
  
Rearranging, and including the *boundary conditions*, we see that the equation we want to solve is
  \[ pr_{i+1} - r_i + qr_{i-1} = 0 \qquad \text{subject to} \qquad r_0 = 1,\ r_m = 0. \]
This is a *linear difference equation* -- and, because the left-hand side is $0$, we call it a *homogeneous* linear difference equation.
We will see how to solve this equation in the next lecture. We will see that, if we set  $\rho = q/p$, then the ruin probability is given by
  \[ r_a = \begin{cases} \displaystyle\frac{\rho^a - \rho^m}{1 - \rho^m} & \text{if $\rho \neq 1$,} \\[0.35cm]
           1 - \displaystyle\frac{a}{m} & \text{if $\rho = 1$.} \end{cases} \]
Note that $\rho = 1$ is the same as the condition $p = q = 1/2$.

Imagine Alice is not playing against her opponent Bob, but rather is up against a large casino. In this case, the casino's capital £$(m-a)$ is typically much bigger than Alice's £$a$. We can model this by keeping $a$ fixed taking a limit $m \to \infty$. Typically, the casino has 'an edge', meaning that $q > p$, so $\rho > 1$. In this case, we see that the ruin probability is
  \[ \lim_{m \to \infty} r_a = \lim_{m \to \infty} \frac{\rho^a - \rho^m}{1 - \rho^m} = \lim_{m \to \infty} \frac{\rho^a/\rho^m - 1}{1/\rho^m - 1} = \frac{0-1}{0-1} = 1, \]
so Alice will be ruined with certainty.

Even with a generous casino that offers an exactly fair game with $p = q$, so $\rho = 1$, we have
  \[ \lim_{m \to \infty} r_a = \lim_{m \to \infty}\left( 1 - \frac{a}{m} \right) = 1-0 = 1 , \]
so, even with this fair game, Alice will still certainly be ruined.



## Expected duration of the game

We could also ask for how long we expect the game to last.

We approach this like before. Let $d_i$ be the expected duration of the game when Alice has £$i$. Our boundary conditions are $d_0 = d_m = 0$, because $X_n = 0$ or $m$ means that the game is over. Again, we proceed by conditioning on the first step, so
\begin{align}
\mathbb E(\text{duration}) &= \mathbb P(\text{win first round}) \, \mathbb E(\text{duration} \mid \text{win first round}) \\
&\qquad{}+ \mathbb P(\text{lose first round}) \, \mathbb E(\text{duration} \mid \text{lose first round}) \\
&= p\,\mathbb E(\text{duration} \mid \text{win first round}) \\
&\qquad{}+ q \,\mathbb E(\text{duration} \mid \text{lose first round}) .
\end{align}
More formally, we've used another version of the law of total probability,
\[ \mathbb E(X) = \sum_{i=1}^k \mathbb P(B_i) \, \mathbb E(X \mid B_i) , \]
or, alternatively, the *tower law* for expectations
\[ \mathbb E(X) = \mathbb E_Y \mathbb E (X \mid Y) = \sum_{y} \mathbb P(Y= y)\, E(X \mid Y = y)\]

Now, the expected duration given we win the first round is $1 + d_{i+1}$. This is because the round itself takes $1$ time step, and then by the Markov property, it's as if we are starting again from $i+1$. Similarly, the expected duration given we lose the first round is $1 + d_{i-1}$. Thus we have 
\[ d_i = p(1 + d_{i+1}) + q (1 + d_{i-1}) = 1 + pd_{i+1} + qd_{i-1} . \]

Rearranging, and including the boundary conditions, we have another linear difference equation:
  \[ pd_{i+1} - d_i + qd_{i-1} = -1 \qquad \text{subject to} \qquad d_0 = 0,\ d_m = 0. \]
Because the right-hand side, $-1$, is nonzero, we call this an *inhomogeneous* linear difference equation.
Again, we'll see how to solve this in the next lecture, and will find that the solution is given by
  \[ d_a = \begin{cases} {\displaystyle \frac{1}{q-p} \left(a - m\frac{1-\rho^a}{1- \rho^m} \right)} & \text{if $\rho \neq 1$,} \\[0.35cm]
\displaystyle a(m-a) & \text{if $\rho = 1$.} \end{cases} \]

Thinking again of playing against the casino, with $q > p$, $\rho > 1$, and $m \to \infty$, we see that the expected duration is
  \[ \lim_{m\to\infty} d_a = \lim_{m\to\infty} \frac{1}{q-p} \left(a - m\frac{1-\rho^a}{1 - \rho^m} \right)  = \frac{1}{q-p} \left(a - 0 \right) = \frac{a}{q-p} , \]
since $\rho^m$ grows much quicker than $m$. So Alice ruins with certainty, and will take time $a/(q-p)$, on average.

In the case of the generous casino, though, with $q = p$, so $\rho = 1$, we have
  \[ \lim_{m\to\infty} d_a =  \lim_{m\to\infty} a(m-a) = \infty .  \]
So here, Alice will ruin with certainty, but it may take a very long time until the ruin occurs, since the average duration is infinite.


# Linear difference equations

```{block, type='summ'}
How to solve homogeneous and inhomogeneous linear difference equations
Solving for probability of ruin and expected duration for the gambler's ruin
```

*Linear difference equations* are equations that look like
  \[ a_k x_{n+k} + a_{k-1} x_{n+k-1} + \cdots + a_1 x_{n+1} + a_0 x_n = f(n)  \qquad (*) \]
for $n = 0,1,\dots$, where the $a_i$ are given constants, $f(n)$ is a given function, and we want to solve for the sequence $(x_n)$. The equation normally comes with some extra conditions, such as the value of the first few $x_n$s.

When the right-hand side of $(*)$ is zero, so $f(n) = 0$, we say the equation is *homogeneous*; if the right-hand side is nonzero, it is *inhomogeneous*. The number $k$, where there are $k+1$ terms on the left-hand side, is called the *degree*}* of the equation; we are mostly interested in second-degree linear difference equations.

A handout, **How to solve linear difference equations**, summarises the techniques we will learn in this lecture, and includes some extra examples.

## Homogeneous linear difference equations

We start with the homogeneous case, which is simpler.

Consider a homogeneous linear difference equation. We shall use the second-degree example
  \[ x_{n+2} - 5x_{n+1} + 6x_{n} = 0 \qquad \text{subject to } x_0 = 4, x_1 = 9 .  \]
Here, the conditions on $x_0$ and $x_1$ are *initial conditions*, because they tell us how the sequence $(x_n)$ starts.

For the moment, we shall put the initial conditions to the side and just worry about the equation
  \[ x_{n+2} - 5x_{n+1} + 6x_{n} = 0 . \]
We start by guessing there might be a solution of the form $x_n = \lambda^n$ for some constant $\lambda$. We can find out if there is such a solution by substituting in $x_n = \lambda^n$, and seeing if there exists a solution for $\lambda$. For our example, we get
  \[ \lambda^{n+2} - 5 \lambda^{n+1} + 6\lambda^n = 0 . \]
After cancelling off a common factor of $\lambda^n$, we get
  \[ \lambda^2 - 5 \lambda + 6 = 0 . \]
This is called the *characteristic equation*. For a general homogeneous linear difference equation \eqref{lde}, the characteristic equation is 
  \begin{equation} \label {cheq} a_k \lambda^{k} + a_{k-1} \lambda^{k-1} + \cdots + a_1 \lambda + a_0 = 0 . \end{equation}
  
We can now solve the characteristic equation for $\lambda$. In our example, we can factor the left-hand side to $(\lambda - 3)(\lambda - 2) = 0$, to find the solutions $\lambda = 2$ and $\lambda = 3$. Thus $x_n = 2^n$ and $x_n = 3^n$ both solve our equation. In fact, since the right-hand side of the equation is $0$, any linear combination of these two solutions is a solution also, thus we get the *general solution*
  \[ x_n = A 2^n + B 3^n , \]
which is a solution for any values of the constants $A$ and $B$.
  
For a general characteristic equation with distinct roots $\lambda_1, \lambda_2, \dots, \lambda_k$, the general solution is
  \[ x_n = C_1 \lambda_1^n + C_2 \lambda_2^n + \cdots + C_n \lambda_k^n . \]
If we have a repeated root -- say, $\lambda_1 = \lambda_2 = \cdots = \lambda_r$ is repeated $r$ times -- then you can check that a solution is given by
  \[ x_n = (D_0 + D_1 n + \cdots + D_{r-1} n^{r-1}) \lambda_1^n , \]
which should take its place in the general solution.

Once we have the general solution, we can use the extra conditions to find the values of the constants. In our example, we can use the initial conditions to find out the values of $A$ and $B$. We see that
\begin{gather*}
x_0 = A2^0 + B3^0 = A + B = 4 , \\
x_1 = A2^1 + B3^1 = 2A + 3B = 9 .
\end{gather*}
We can now solve this pair of simultaneous equations to solve for $A$ and $B$. By subtracting twice the first equation from the second we get $B = 1$, and substituting this into the first equation we get $A = 3$. Thus the solution is
  \[ x_n = 3\cdot 2^n + 3^n . \]

In conclusion, the process here was:

1. Find the general solution by writing down and solving the characteristic equation.
2. Use the extra conditions to find the values of the constants in the general solution.


## Probability of ruin for the gambler's ruin

In the last lecture we saw that probability of ruin for the gambler's ruin process is the solution to
  \[ pr_{i+1} - r_i + qr_{i-1} = 0 \qquad \text{subject to} \qquad r_0 = 1,\ r_m = 0 , \]
where the extra conditions here are *boundary conditions*.

The characteristic equation is
  \[ p\lambda^2 - \lambda + q = 0 .\]
We can solve the characteristic equation by factorising it as $(p \lambda - q)(\lambda - 1) = 0$. (It might take a moment to check this -- we've used that $p+q=1$.) So the characteristic equation has roots $\lambda = q/p$, which we called $\rho$ last time, and $\lambda = 1$. Note that if $\rho = 1$ (so $p = q = 1/2$) we have a repeated root, while if $\rho \neq 1$ we have distinct roots, so we'll need to deal with the two cases separately.

First,  the case $\rho \neq 0$. Since the two roots are distinct, we have the general solution
  \[ r_i = A\rho^i + B1^i = A\rho^i + B . \]

We can now use the boundary conditions to find $A$ and $B$. We have
  \begin{gather*} r_0 = A \rho^0 + B = A+B = 1, \\
                r_m = A \rho^m + B = 0 . \end{gather*}
From the first we get $B = 1-A$, which we substitute into the second to get
  \[ A\rho^m + 1 - A = 0 \quad \Rightarrow \quad A = \frac{1}{1-\rho^m} , \]
and hence
  \[ B = 1 - A = 1 - \frac{1}{1-\rho^m} = - \frac{\rho^m}{1 - \rho^m} . \]
Thus the solution is
  \[ r_i = \frac{1}{1-\rho^m} \rho^i -  \frac{\rho^m}{1 - \rho^m} = \frac{\rho^i - \rho^m}{1 - \rho^m}  , \]
as we claimed last time.

Second, the case $\rho = 1$. Now we have a repeated root $\lambda = 1$, so the general solution is
  \[ r_i = (A + Bi) 1^i = A+Bi . \]
  
Again, we use the boundary conditions, to get
  \begin{gather*} r_0 = A + B\cdot 0 = A = 1, \\
r_m = A + Bm = 0 , \end{gather*}
and we immediately see that $A = 1$ and $B = -1/m$. Thus the solution is
  \[ r_i = 1 - \frac{1}{m}i = 1 - \frac{i}{m} , \]
as claimed.





## Inhomogeneous linear difference equations#

Solving inhomogeneous linear difference equations requires three steps:

1. Find the general solution to the *homogeneous* equation by writing down and solving the characteristic equation.
2. Find any solution (a *particular solution*) to the inhomogeneous equation. The general solution solution to the inhomogeneous equation is a particular solution plus the general solution to the homogeneous equation.
3. Use the extra conditions to find the values of the constants in the general solution.


This idea works because adding a solution to the homogeneous equation to the left-hand side adds zero to the right-hand side.

Let's work through the example
  \[ x_{n+2} - 5x_{n+1} + 6x_{n} = 2 \qquad \text{subject to } x_0 = 4, x_1 = 9 . \]

We already know from earlier that the general solution to the homogeneous equation $x_{n+2} - 5x_{n+1} + 6x_{n} - 0$ (with a zero on the right-hand side) is
  \[ x_n = A2^n + B3^n . \]
 
We now need to find a *particular solution* -- that is, any solution -- to our new inhomogeneous equation. A general hint here is to guess a solution with the same `shape' as the right-hand side. For example, if the right-hand side is a polynomial of degree $d$, try a polynomial of degree $d$. Here our right-hand side is a constant ($2$), so we should try a constant. Substituting in $x_n = C$ for all $n$ gives us $C - 5C + 6C = 2$, thus $2C = 2$ and $C = 1$, giving a particular solution $x_n = 1$. The general solution to the inhomogeneous equation is therefore
  \[ x_n = 1 + A2^n + B3^n . \]
  
Again, we use the initial conditions to get the constants $A$ and $B$. We have
\begin{gather*}
x_0 = 1 + A2^0 + B3^0 = 1+ A + B = 4 , \\
x_1 = 1 + A2^1 + B3^1 = 1+ 2A + 3B = 9 ,
\end{gather*}
and we can easily check this gives $A = 1, B = 2$. Thus the solution is
  \[ x_n = 1 + 1\cdot 2^n + 2 \cdot 3^n = 1 + 2^n + 2 \cdot 3^n . \]



## Expected duration for the gambler's ruin

From last time, the expected duration of the gambler's ruin game solves
  \[ pd_{i+1} - d_i + qd_{i-1} = -1 \qquad \text{subject to} \qquad d_0 = 0,\ d_m = 0. \]
As before, we divide cases based on whether or not $\rho = 1$.

First, the case $\rho \neq 1$. We already know that the general solution to the homogeneous equation is
  \[ d_i =  A \rho^i + B . \]

Now we need a particular solution. It's tempting to guess a constant $C$ for a particular solution, but we know that constants solve the homogeneous equation, so will have right-hand side $0$, not$-1$. The next best guess is one degree up: let's try $x_i = Ci$. This gives
  \begin{align*}
  -1 &= pC(i+1) - Ci + qC(i-1)\\
     &= C(pi + p - i + qi - q) \\
     &= C\big((p+q-1)i + (p-q)\big) \\
     &= C(p-q) ,
  \end{align*}
giving $C = -1/(p-q) = 1/(q-p)$. The general solution to the inhomogeneous equation is
  \[ d_i = \frac{i}{q-p} + A \rho^i + B .  \]
  
Then to find the constants, we have
  \begin{gather*} r_0 = \frac{0}{q-p} + A \rho^0 + B = A+B = 0, \\
                  r_m = \frac{m}{q-p} + A \rho^m + B = 0 , \end{gather*}
which gives
  \[ A = -B = \frac{1}{q-p} \cdot \frac{m}{1 - \rho^m} . \]
The solution is
  \[ d_i = \frac{i}{q-p} + \frac{1}{q-p} \cdot \frac{m}{1 - \rho^m} \rho^i - \frac{1}{q-p} \cdot \frac{m}{1 - \rho^m} =  \frac{1}{q-p} \left(i - m\frac{1-\rho^i}{1- \rho^m} \right) . \]
  
Second, the case $\rho = 1$, so $p = q = 1/2$. We already know that the general solution to the homogeneous equation is
\[ d_i =  A + Bi . \]

We need a particular solution. Since both constants and linear terms solve the homogeneous equation, we'll have to go up another degree and try $x_i = Ci^2$. This gives
\begin{align*}
	-1 &= \frac12 C(i+1)^2 - Ci^2 + \frac 12 C(i-1)^2 \\
	   &= \frac12 C(i^2 + 2i + 1 - 2i^2 + i^2 - 2i + 1) \\
	   &= \frac12 C\big((1-2+1)i^2 + (2-2)i + (1+1)\big) \\
	   &=C ,
\end{align*}
so the general solution to the inhomogeneous equation is
\[ d_i = -i^2 + A + Bi .  \]

Then to find the constants, we have
  \begin{gather*} d_0 = -0i^2 + A + B\cdot0 = A = 0, \\
                  d_m = -m^2 + A + Bm = 0 , \end{gather*}
giving $A = 0, B = m$. The solution is
  \[ d_i = -i^2 + 0 + mi = i(m-i) .\]



# How to solve linear difference equations {-}

This handout accompanies **Lecture 4: Linear difference equations**. We summarise the techniques from the lecture, and give some extra examples.

## How to solve homogeneous linear difference equations {-}

*Homogeneous linear difference equations* are of the form
 \[  a_k x_{n+k} + a_{k-1} x_{n+k-1} + \cdots + a_1 x_{n+1} + a_0 x_n = 0 \]
with $0$ on the right hand side. Here, the $(a_i)$ are constants, and we want to solve for $(x_n)$. We are given extra conditions, such as initial conditions (the values of $x_0, x_1, \dots$).

**Step 1.** *Solve the characteristic equation to find the general solution.*

The characteristic equation is 
\[  a_k \lambda^{k} + a_{k-1} \lambda^{k-1} + \cdots + a_1 \lambda + a_0 = 0 .\]

If the solutions $\lambda_1, \lambda_2, \dots, \lambda_k$ are distinct, then the general solution is
\[ x_n = C_1 \lambda_1^n + C_2 \lambda_2^n + \cdots + C_k\lambda_k^n , \]
for constants $C_1, C_2, \dots, C_k$.

If we have a repeated root -- say, $\lambda_1 = \lambda_2 = \cdots = \lambda_r$ is repeated $r$ times -- than a solution is given by
\[ x_n = (D_0 + D_1 n + \cdots + D_{r-1} n^{r-1}) \lambda_1^n , \]
which should take its place in the general solution.


**Step 2.** *Use the extra conditions to solve for the constants in the general solution.*

The extra conditions can be substituted into the general solution. This gives a number of simultaneous equations which can be solved to find the values of the constants $C_1, C_2, \dots, C_k$.



**Example.** *Solve the homogeneous linear difference equation*
\[ x_{n+2} - x_{n+1} - 6x_n = 0 \qquad \text{\emph{subject to} \quad $x_0 = 3$,\quad $x_1 = 4$.} \]


*Step 1.* The characteristic equation is
  \[ \lambda^2 - \lambda - 6 = 0 . \]
We can solve this by factorising it as $(\lambda - 3) (\lambda + 2) = 0$,
to find the solutions $\lambda_1 = -2$ and $\lambda_2 = 3$. Thus the general solution is
  \[ x_n = A(-2)^n + B3^n . \]
(We've used the more convenient $A$ and $B$ rather than $C_1$ and $C_2$.)

*Step 2.* Substituting the initial conditions into the general solution, we have
\begin{align*}
x_0 &= A(-2)^0 + B3^0 = A + B = 3 \\
x_1 &= A(-2)^1 + B3^1 = -2A + 3B = 4 .
\end{align*}
We can add twice the first equation from the second to get $5B = 10$, so $B=2$. We can substitute this into the first equation to get $A = 1$.

The solution is therefore
\[ x_n = 1\cdot(-2)^n + 2 \cdot 3^n = \cdot(-2)^n + 2 \cdot 3^n . \]

**Example.** *Solve the homogeneous linear difference equation*
\[ x_{n+2} + 4x_{n+1} +4x_n = 0 \qquad \text{\emph{subject to} \quad $x_0 = 2$,\quad $x_1 = -6$.} \]

*Step 1.* The characteristic equation is
\[ \lambda^2 + 4\lambda + 4 = 0 . \]
We can solve this by factorising it as $(\lambda + 2)^2 = 0$, to find a repeated root $\lambda_1 = \lambda_2 = -2$. Thus the general solution is
\[ x_n = (A + Bn) (-2)^n . \]

*Step 2.* Substituting the initial conditions into the general solution, we have
\begin{align*}
x_0 &= (A + B0)(-2)^0 = A = 2 \\
x_1 &= (A + B1)(-2)^1 = -2A - 2B = -6 .
\end{align*}
The first immediately gives $A = 2$, and substituting this into the second equation gives $B = 1$.

The solution is therefore
\[ x_n = (2 + n)(-2)^n . \]


## How to solve inhomogeneous linear difference equations {-}

*Inhomogeneous linear difference equations* are of the form
\[  a_k x_{n+k} + a_{k-1} x_{n+k-1} + \cdots + a_1 x_{n+1} + a_0 x_n = f(n) \]
with a nonzero right hand side $f(n)$. As before, we are given extra conditions, such as initial conditions (the values of $x_0, x_1, \dots$).

**Step 1.** *Solve the characteristic equation to find the general solution to the homogeneous equation.*

As before, the characteristic equation is 
\[  a_k \lambda^{k} + a_{k-1} \lambda^{k-1} + \cdots + a_1 \lambda + a_0 = 0 .\]
We form the general solution to the homogeneous equation in the usual way:
if the solutions $\lambda_1, \lambda_2, \dots, \lambda_k$ are distinct, then the general solution is
\[ x_n = C_1 \lambda_1^n + C_2 \lambda_2^n + \cdots + C_k\lambda_k^n , \]
for constants $C_1, C_2, \dots, C_k$, while we require adjustments for repeated roots.


**Step 2.** *Find a particular solution to the inhomogeneous equation, usually by guessing the `shape' of the solution and solving for any constants. The general solution of the inhomogeneous equation is the particular solution plus the general solution of the homogeneous equation from step 1.*


A 'particular solution' is any solution of the inhomogeneous equation. We can usually find such a solution by guessing the `shape' of the solution to be the same as the right hand side $f(n)$. A few suggestions are

* If $f(n)$ is a constant, try a constant $x_n = C$. (If $1$ was a root of the characteristic equation, this will not work: try $x_n = Cn$ instead. If the root $1$ was repeated $r$ times, try $x_n = Cn^r$.) This is the main case we will use here.
* If $f(n)$ is a degree-$d$ polynomial, try a polynomial of degree $d$.
* If $f(n)$ is an exponential $b^n$, try a multiple of the same exponential $Cb^n$.

After guessing the shape, we can substitute the guess into the inhomogeneous linear difference equation to find the value of any constants in our guess.

**Step 3.** *Use the extra conditions to solve for the constants in the general solution to the inhomogeneous equation.*


As before, the extra conditions can be substituted into the general solution to give a number of simultaneous equations which can be solved to find the values of the constants $C_1, C_2, \dots, C_k$.


**Example.** *Solve the inhomogeneous linear difference equation*
\[ 10 x_{n+2} - 7x_{n+1} + x_n = 8 \qquad \text{\emph{subject to} \quad $x_0 = 0$,\quad $x_1 = \tfrac{13}{10} $.} \]

*Step 1.* The characteristic equation is
\[ 10\lambda^2 - 7\lambda + 1 = 0 . \]
We can solve this by factorising it as
\[ (2\lambda - 1) (5\lambda - 1) = 0 , \]
to find the solutions $\lambda_1 = \frac12$ and $\lambda_2 = \frac15$. Thus the general solution of the homogeneous equation is
\[ x_n = A\left(\frac12\right)^n + B\left(\frac15\right)^n . \]

*Step 2.* Since the right hand side of the inhomogeneous equation is a constant, we guess a constant particular solution with shape $x_n = C$. Substituting in this guess, we get
\[ 10C - 7C + C = 4C = 8 \]
with solution $C=2$. Thus a particular solution is $x_n = 2$, and the general solution to the inhomogeneous equation is
\[ x_n = 2 + A\left(\frac12\right)^n + B\left(\frac15\right)^n . \]

*Step 3.* Substituting the initial conditions into the general solution, we have
\begin{align*}
x_0 = 2 + A\left(\frac12\right)^0 + B\left(\frac15\right)^0 = 2 + A + B = 0 \quad &\Rightarrow \quad A + B = -2 \\
x_1 = 2 + A\left(\frac12\right)^1 + B\left(\frac15\right)^1 = 2 + A\frac12 + B\frac15 = \frac{13}{10} \quad &\Rightarrow \quad 5A + 2B = -7.
\end{align*}
We can subtract twice the first equation from the second to get $3A = -3$, so $A = -1$. We can substitute this into the second equation to get $B = -1$.

The solution is therefore
\[ x_n = 2 - \left(\frac12\right)^n - \left(\frac15\right)^n . \]




# Discrete time Markov chains

# Examples from actuarial science

# Class structure

# Hitting times

# Recurrence and transience

# Stationary distributions

# How to find stationary distributions {-}

# Long-term behaviour of Markov chains

# Summary of Part I {-}


# (PART\*) Part II: Continuous time Markov jump processes {-}

# Poisson process and the Poisson distribution

# Poisson process and the exponential distribution

# Poisson process in infinitesimal time periods

# Counting processes

# Continuous time Markov jump processes: jump chain and holding times

# Forward and backward equations

# Class structure and hitting times

# Long-term behaviour of Markov jump processes

# Queues

# Summary of Part II {-}



# (PART\*) Revision {-}
  
# Frequently asked questions

# Exam preparation



