[
["index.html", "Introduction to Markov Processes About the module Organisation of the module Content of the module", " Introduction to Markov Processes Dr Matthew Aldridge m.aldridge@leeds.ac.uk Semester 2, 2019–20 About the module This module is MATH2750 Introduction to Markov Processes. The module organiser and lecturer is Dr Matthew Aldridge, and my email address is m.aldridge @leeds.ac.uk. Organisation of the module This module lasts for 11 weeks. The first nine weeks run from 28 January to 29 March, then we break for Easter, then the final two weeks run from 29 April to 10 May. Lectures There are two lectures each week, for a total of 22 lectures. Lectures are on Tuesdays at 1400 and Thursdays at 1000, both in Roger Stevens LT 20 (7M.20). Attendance at lectures is mandatory. Outline lecture notes summarising the main definitions and theorems from the course will be made available on Minerva. I’m very keen to hear about errors, mathematical, typographical or otherwise, in the lecture notes – please email me or talk to me after lectures. The notes are not a substitute for attending the lectures. The lectures will be videoed on the lecture capture system. Problem sheets There will be 10 problem sheets; Problem Sheet \\(n\\) covers the material from two lectures in week \\(n\\), and will be discussed in your workshop in week \\(n+1\\). The best way of learning the material in this course is to spend plenty of time working on the problem sheets in advance of your workshop and writing up your answers. Collaboration is encouraged when working through the problems, but I recommend writing up your work on your own. Workshops There will be 10 workshops, starting in the second week. The main goal of the workshops will be to go over your answers to the problems sheets in smaller classes. You will have been assigned to one of five workshop groups, meeting on Mondays or Tuesdays, led by Dr Andrew Baczkowski, Dario Domingo (two groups), Dr Graham Murphy or me. Check your timetable for details. Assessments There will be three pieces of assessed coursework, each making up 5% of your mark for the module, for a total of 15%. These will involve writing up answer to a few problems, in a similar style to the problem sheets. While you may want to discuss the assessment with others in advance of completing it by yourself, copying is not allowed and will be dealt with in accordance with University rules. The assessments will be due on Thursdays 21 February, 14 March and 2 May at 1400. Computer practicals There will be two computer practicals, to be completed in the fourth and seventh weeks of the module. There will be practical classes run during those weeks where you can work on the problems, get help with any difficulties, and have your work marked. (This does not form part of your official mark for the module.) Exam There will be a two-hour exam after the end of the module, making up the other 85% of your mark. The exam will consist of four questions, and you are expected to answer all of them. We will talk more about the exam in the revision sessions (Lectures 21 and 22). Office hours I will run office hours on Mondays and Wednesdays at 1500 in my office: 9.320, Physics Research Deck. This is your opportunity to discuss with me anything that you want from the course, including material you are confused about and problems you don’t understand. If you can’t make these times, try emailing me to arrange a meeting, or knock on my office door and see if I’m free. (The Physics Research Deck is easiest to reach from the 8th floor of EC Stoner at staircase 4. If you haven’t been there before, I recommend asking a friend who has for directions. Once in the maths area on the 9th floor of the PRD, my office is back on yourself, towards the Observatory.) Content of the module The course has two major parts: the first part will cover processes in discrete time and the second part processes in continuous time. An outline plan of the lectures is the following: Introduction to stochastic processes [1 lecture] Discrete time Markov chains [10 lectures] Important examples: Random walk, gambler’s ruin, linear difference equations [3 lectures] General theory: transition probabilities, communicating classes, hitting times, recurrence and transience, stationary distribution, long-term behaviour [7 lectures] Continuous time Markov jump processes [9 lectures] Important examples: Poisson process, birth processes [4 lectures] General theory: holding times and jump chains, forward and backward equations, hitting times, stationary distribution, long-term behaviour, queues [5 lectures] Revision [2 lectures] Books You can do well on this module by attending the lectures and workshops, plus working on the problem sheets, assignments and practicals, without any further reading. However, for students who would like some book recommendations for optional extra background reading or an alternative view on the material, I recommend the following. The two books I found most useful in planning the course were: J.R. Norris, Markov Chains, Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press, 1997. Chapters 1-3. G.R. Grimmett and D.R. Stirzaker, Probability and Random Processes, 3rd edition, Oxford University Press, 2001. Chapter 6. Norris discusses only Markov processes, and has some more detailed material that goes beyond this module. Grimmet and Stirzaker is an excellent handbook that covers most of undergraduate probability. The approach of Grimmet and Stirzaker is very closely mirrored in G. Grimmet and D. Walsh, Probability: an introduction, 2nd edition, Oxford University Press, 2014. Chapter 12. which might be useful if you have a copy to hand, or if the library runs out of Grimmet and Stirzaker. A gentler introduction with plenty of examples is provided by P.W. Jones and P. Smith, Stochastic Processes: an introduction, 3nd edition, Texts in Statistical Science, CRC Press, 2018. Chapters 2-7. although it doesn’t cover everything in this module. It’s available online via the University library. Finally, I’ve heard good things about D.R. Stirzaker, Elementary Probability, 2nd edition, Cambridge University Press, 2003. Chapter 9. although I haven’t used it myself. It’s also available online. (I also benefited from reading lecture notes from former lecturers of this course, particularly Dr Graham Murphy, whose help was invaluable.) "],
["stochastic-processes-and-the-markov-property.html", "1 Stochastic processes and the Markov property 1.1 Deterministic and random models 1.2 Stochastic processes 1.3 Markov property", " 1 Stochastic processes and the Markov property Stochastic processes with discrete/continuous state space and discrete/continuous time The Markov `memoryless’ property 1.1 Deterministic and random models A model is an imitation of a real-world system. For example, you might want to have a model to imitate the world’s population, the level of water in a reservoir, future cashflows of a pension scheme, or future stock prices. Models allow us to try to predict what might happen in the real world in a low risk, cost effective and fast way. To design a model requires a set of assumptions about how it will work and suitable parameters need to be determined, perhaps based on past collected data. An important distinction is between deterministic models and random} (or stochastic) models. Deterministic models do not contain any random components, so the output is completely determined by the inputs and any parameters. Random models have variable outcomes, so can be run many times to give a sense of the range of possible outcomes. Consider models for: the future position of the Moon as it orbits the Earth, the future price of shares in Apple. In (a), the random components – for example, the effect of meteorites striking the Moon’s surface – are not very significant and a deterministic model based on physical laws is good enough for most purposes. In (b), the share price from day to day is highly uncertain, so a random model can take into account the variability and unpredictability in a useful way. In this module we will see many examples of stochastic models. Lots of the applications we will consider come from financial mathematics and actuarial science where the use of models that take into account uncertainty is very important, but the principles apply in many areas. 1.2 Stochastic processes If we want to model, for example, the total number of claims to an insurance company in the whole of 2019, we can use a random variable \\(X\\) to model this – perhaps a Poisson distribution with an appropriate mean, for example. However, if we want to track how the number of claims changes over the course of the year 2019, we will need to use a stochastic process. (The word “stochastic”&quot; means the same thing as “random”.) A stochastic process, which we will write as \\((X_n)\\), is an indexed sequence of random variables that are (usually) dependent on each other. Each random variable \\(X_n\\) takes a value in a state space \\(\\mathcal S\\) which is the set of possible values for the process. As with usual random variables, the state space \\(\\mathcal S\\) can be discrete or continuous. A discrete state space denotes a set of distinct possible outcomes, which can be finite or countably infinite. For example, \\(\\mathcal S = \\{\\text{Heads},\\text{Tails}\\}\\) is the state space for a single coin flip, while in the case of counting insurance claims, the state space would be the nonnegative integers \\(\\mathcal S = \\mathbb Z_+ = \\{0,1,2,\\dots\\}\\). A continuous state spaces denotes an uncountably infinite continuum of gradually varying outcomes. For example, the nonnegative real line \\(\\mathcal S = \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}\\) is the state space for the amount of rainfall on a given day, while a bounded subset of \\(\\mathbb R^3\\) is the state space for the position of a gas particle in a box. Further, the process has an index set that puts the random variables that make up the process in order. The index set is usually interpreted as a time variable, telling us when the process will be measured. The index set for time can also be discrete or continuous. Discrete time denotes a process sampled at different points, often denoted by \\(n = 0,1,2,\\dots\\), while continuous time denotes a process monitored constantly over time, often denoted by \\(t \\in \\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}\\). In the insurance example, we might count up the number of claims each day – then the discrete index set will be the days of the year, which we could denote \\(\\{1,2,\\dots,365\\}\\). Alternatively, we might want to keep a constant tally that we update after every claim, requiring a continuous time index representing time across the whole year. In discrete time, we can write down the first few steps of the process as \\((X_0, X_1, X_2, \\dots)\\). This gives us four possibilities in total: Discrete time, discrete space Example: Number of students turning up to each lecture of . Markov chains – discrete time, discrete space stochastic processes with a certain “Markov property”&quot; – are the main topic of the first half of this module. Discrete time, continuous space Example: Daily maximum temperature in Leeds. We will briefly mention continuous space Markov chains in the first half of the course. Continuous time, discrete space Example: Number of visitors to a webpage over time. Markov jump processes – continuous time, discrete space stochastic processes with the `Markov property’ – are the main topic of the second half of this module. Continuous time, continuous space Example: Level of the FTSE 100 share index over time. Such processes, especially the famous Brownian motion – another process with the Markov property – are very important, but outside the scope of this course. See MATH3733 Stochastic Financial Modelling next year, for example. 1.3 Markov property Because stochastic processes consist of a large number – even infinitely many – random variables that could all be dependent on each other, they can get extremely complicated. The Markov property is a crucial property that restricts the type of dependences in a process, to make the process easier to study, yet still leaves most of the useful and interesting examples intact. Think of a simple board game where we roll a die and move that many squares forward on the board. Suppose we are currently on the square \\(X_n\\). Then which square \\(X_{n+1}\\) we move to on our next turn: is random, since it depends on the roll of the die; depends on where we are now \\(X_n\\), since the score of die will be added onto the number our current square; given the square we are now \\(X_n\\), it doesn’t depend any further on which sequence of squares \\(X_0, X_1, \\dots, X_{n-1}\\) we used to get here. It is this third point that is the crucial property of the stochastic processes we will study in this course, and it is called the Markov property or memoryless property. We say “memoryless”“, because it’s as if the process forgot how it got here – the process before this moment has no bearing on the future given where we are now. A mathematical way to say this is that `the past and the future are conditionally independent given the present.’ To write this down formally, we need to recall conditional probability: the conditional probability of an event \\(A\\) given another event \\(B\\) is written \\(\\mathbb P(A \\mid B)\\), and is the probability that \\(A\\) occurs given that \\(B\\) definitely occurs. You may remember the definition \\[ \\mathbb P(A \\mid B) = \\frac{\\mathbb P(A \\cap B)}{\\mathbb P(B)} , \\] although is often more useful to reason directly about conditional probabilities than use this formula. Definition 1.1 Let \\((X_n) = (X_0, X_1, X_2, \\dots)\\) be a stochastic process in time \\(n = 0,1,2,\\dots\\). Then we say that \\((X_n)\\) has the or is if, for all times \\(n\\) and all states \\(x_0, x_1, \\dots,x_n, x_{n+1} \\in \\mathcal S\\) we have \\[ \\mathbb P(X_{n+1}=x_{n+1} \\mid X_{n}=x_{n}, \\dots,X_1 = x_1, X_0=x_0)=\\mathbb P(X_{n+1}=x_{n+1} \\mid X_{n}=x_{n}) . \\] Here, the left hand side is the probability we go to state \\(x_{n+1}\\) next conditioned on the entire history of the process, while the right hand side is the probability we go to state \\(x_{n+1}\\) next conditioned only on where we are now. So this property tells us that it only matters where we are now and not how we got here. (There’s also a similar definition for continuous time processes, which we’ll come to later in the course.) Stochastic processes that have the Markov property are much easier to study than general processes, as we only have to keep track of where we are now and we don’t have to keep track of the entire history that came before. "],
["random-walk.html", "2 Random walk 2.1 Simple random walk 2.2 General random walks 2.3 Exact distribution of the simple random walk", " 2 Random walk Definition of the simple random walk and the exact binomial distribution Expectation and variance of general random walks 2.1 Simple random walk Consider the following simple random walk on the integers \\(\\mathbb Z\\): We start at \\(0\\), then at each time-step, we go up by one with probability \\(p\\) and down by one with probability \\(q = 1-p\\). When \\(p = q= 1/2\\), we’re equally as likely to go up as down, and call this the simple symmetric random walk. The simple random walk can be used as simplified model for lots of processes, like stock prices, sizes of populations, or positions of gas particles. (In most modern models, however, these have been replaced by more complicated continuous time and space models.) The simple random walk is sometimes called the `drunkard’s walk’, suggesting it could model a drunk person trying to stagger home. Z &lt;- rbinom(20,1,2/3) Z &lt;- 2*Z - 1 X &lt;- c(0,cumsum(Z)) plot(0:20,X,xlab=&quot;n&quot;,ylab=&quot;X_n&quot;,main=&quot;Simple random walk, p = 2/3&quot;,type=&quot;b&quot;) Z &lt;- rbinom(20,1,1/3) Z &lt;- 2*Z - 1 X &lt;- c(0,cumsum(Z)) plot(0:20,X,xlab=&quot;n&quot;,ylab=&quot;X_n&quot;,main=&quot;Simple random walk, p = 1/3&quot;,type=&quot;b&quot;) We can write this as a stochastic process \\((X_n)\\) with discrete time \\(n = \\{0,1,2,\\dots\\} = \\mathbb Z_+\\) and discrete state space \\(\\mathcal S = \\mathbb Z\\), where \\(X_0 = 0\\) and, for \\(n \\geq 0\\), we have \\[ X_{n+1} = \\begin{cases} X_n + 1 &amp; \\text{with probability $p$,} \\\\ X_n - 1 &amp; \\text{with probability $q$.} \\end{cases} \\] It’s clear from this definition that \\(X_{n+1}\\) (the future) depends on \\(X_n\\) (the present), but, given \\(X_n\\), does not depend on \\(X_{n-1}, X_{n-1}, \\dots, X_0\\) (the past). Thus the Markov property holds, and the simple random walk is a Markov process or Markov chain. Example 2.1 What’s the probability that after two steps a simple random walk has reached \\(X_2 = 2\\)? To achieve this, the walk must go upwards in both time steps, so \\(\\mathbb P(X_2 = 2) = pp = p^2\\). Example 2.2 What’s the probability that after three steps a simple random walk has reached \\(X_3 = -1\\)? There are three ways to reach \\(-1\\) after three steps: up–down–down, down–up–down, or down–down–up. So \\[ \\mathbb P(X_3 = -1) = pqq+qpq+qqp = 3pq^2 . \\] 2.2 General random walks Note that alternative way to write the simple random walk is to put \\[ X_n = X_0 + \\sum_{i=1}^n Z_i , \\qquad (*) \\] where the starting point is \\(X_0 = 0\\) and the increments \\(Z_1, Z_2, \\dots\\) are independent and identically distributed (IID) random variables with distribution given by \\(\\mathbb P(Z_i = 1) = p\\) and \\(\\mathbb P(Z_i = -1) = q\\). You can check that this means \\(X_{n+1} = X_n + Z_{n+1}\\), and that this property defines the simple random walk. In fact, any stochastic process with the form \\((*)\\) for some \\(X_0\\) and some distribution for the \\(Z_i\\)s is called a random walk. Random walks often have state space \\(\\mathcal S = \\mathbb Z\\), like the simple random walk, but they could be defined on other state spaces. We could look at higher dimensional simple random walks: in \\(\\mathbb Z^2\\), for example, we could step up, down, left or right with given probabilities. We could even have a continuous state space like \\(\\mathbb R\\), if, for example, the \\(Z_i\\)s had a normal distribution. We can use this structure to calculate the expectation or variance of any random walk (including the simple random walk). Let’s start with the expectation. For a random walk \\((X_n)\\) we have \\[ \\mathbb E X_n = \\mathbb E \\left(X_0 + \\sum_{i=1}^n Z_i\\right) = \\mathbb E X_0 + \\sum_{i=1}^n \\mathbb E Z_i = \\mathbb EX_0 + n \\mathbb E Z_1 , \\] where we’ve used the linearity of expectation, and that the \\(Z_i\\)s are identically distributed. In the case of the simple random walk, we have \\(\\mathbb E X_0 = 0\\), since we start from \\(0\\) with certainty, and \\[ \\mathbb E Z_1 = \\sum_{z \\in \\mathbb Z} z \\mathbb P(Z_1 = z) = 1\\times p + (-1)\\times q = p-q ,\\] so \\(\\mathbb EX_n = n(p-q)\\). If \\(p &gt; 1/2\\), then \\(p &gt; q\\), so \\(\\mathbb E X_n\\) grows ever bigger over time, while if \\(p &lt; 1/2\\), then \\(\\mathbb E X_n\\) grows ever smaller (that is, negative with larger absolute value) over time. If \\(p = 1/2 = q\\), which is the case of the simple symmetric random walk, then then the expectation \\(\\mathbb E X_n = 0\\) is zero for all time. Now the variance of a random walk. We have \\[ {\\mathrm{Var}}(X_n) = {\\mathrm{Var}}\\left(X_0 + \\sum_{i=1}^n Z_i\\right) = {\\mathrm{Var}}X_0 + \\sum_{i=1}^n {\\mathrm{Var}}Z_i = {\\mathrm{Var}}X_0 + n {\\mathrm{Var}}Z_1 , \\] where it was crucial that \\(X_0\\) and all the \\(Z_i\\)s were independent (so we had no covariance terms). Again, for a simple random walk \\({\\mathrm{Var}}X_0 = 0\\), since we always start from \\(0\\). To calculate the variance, we write \\[\\begin{align} \\Var(Z_1) &amp;= \\mathbb E Z_1^2 - (\\mathbb EZ_1)^2 \\\\ &amp;= 1^2 \\times p + (-1)^2 \\times q - (p-q)^2 \\\\ &amp;= p + q - (p-q)^2 \\\\ &amp;= 1 - (2p - 1)^2 \\\\ &amp;= 4p - 4p^2 \\\\ &amp;= 4pq , \\end{align}\\] where we’ve used that \\(q = 1-p\\). Hence the variance of the simple random walk is \\(4pqn\\). Note that (unless \\(p\\) is \\(0\\) or \\(1\\)) the variance grows over time, so it becomes harder and harder to predict where the random walk will be. The variance of the simple symmetric random walk is \\(4 \\frac12 \\frac12 n = n\\). For large \\(n\\), we can use a normal approximation for a random walk. Suppose the increments process \\((Z_n)\\) has mean \\(\\mu\\) and variance \\(\\sigma^2\\), and that the walk starts from \\(X_0 = 0\\). Then we have \\(\\mathbb E X_n = \\mu n\\) and \\({\\mathrm{Var}}(X_n) = \\sigma^2 n\\), so for large \\(n\\) we can use the normal approximation \\(X_n \\approx \\mathrm{N}(\\mu n, \\sigma^2 n)\\). (Note, of course, that the \\(X_n\\) are not independent.) To be more formal, the central limit theorem tells us that, as \\(n \\to \\infty\\), we have \\[ \\frac{X_n - n\\mu}{\\sigma \\sqrt{n}} \\to \\mathrm{N}(0,1) . \\] 2.3 Exact distribution of the simple random walk In the case of the simple random walk, we can in fact give the exact distribution by writing down a formula for \\(\\mathbb P(X_n = i)\\) for any time \\(n\\) and state \\(i\\). Recall that, at each of the first \\(n\\) times, we take an upward step with probability \\(p\\), and otherwise take a downward step. So if we let \\(Y_n\\) be the number of upward steps over the first \\(n\\) times, we see that \\(Y_n\\) has a binomial distribution \\(Y \\sim \\text{Bin}(n,p)\\). Recall that the binomial distribution has probability \\[ \\mathbb P(Y_n = k) = \\binom nk p^k (1-p)^{n-k} = \\binom nk p^k q^{n-k} , \\] for \\(k = 0,1,\\dots, n\\), where \\(\\binom{n}{k}\\) is a binomial coefficient `\\(n\\) choose \\(k\\)’. If \\(Y_n = k\\), that means we’ve taken \\(k\\) upward steps and \\(n-k\\) downward steps, leaving us at position \\(k - (n-k) = 2k - n\\). Thus we have that \\[ \\mathbb P(X_n = 2k - n) = \\mathbb P(Y_n = k) = \\binom nk p^k q^{n-k} . \\qquad (**) \\] Note that after an odd number of time steps \\(n\\) we’re always at an odd-numbered state, since \\(2k - \\text{odd} = \\text{odd}\\), while after an even number of time steps \\(n\\) we’re always at an even-numbered state, since \\(2k - \\text{even} = \\text{even}\\). Writing \\(i = 2k - n\\), so \\(k = (n+i)/2\\) and \\(n-k = (n-i)/2\\), we can rearrange \\((**)\\) to see that the distribution for the simple random walk is \\[ \\mathbb P(X_n = i) = \\binom{n}{(n+i)/2} p^{(n+i)/2} q^{n - (n+i)/2} = \\binom{n}{(n+i)/2} p^{(n+i)/2} q^{(n-i)/2} , \\] when \\(n\\) and \\(i\\) have the same parity with \\(-n \\leq i \\leq n\\), and is \\(0\\) otherwise. In the special case of the simple symmetric random walk, we have \\[ \\mathbb P(X_n = i) = \\binom{n}{(n+i)/2} \\left(\\frac12\\right)^{(n+i)/2} \\left(\\frac12\\right)^{(n-i)/2} = \\binom{n}{(n+i)/2} 2^{-n} . \\] "],
["gamblers-ruin.html", "3 Gambler’s ruin 3.1 Gambler’s ruin Markov chain 3.2 Probability of ruin 3.3 Expected duration of the game", " 3 Gambler’s ruin The gambler’s ruin Markov chain Equations for probability of ruin and expected duration of the game by conditioning on the first step 3.1 Gambler’s ruin Markov chain Consider the following gambling problem. Alice is gambling against Bob. Alice starts with £\\(a\\) and Bob starts with £\\(b\\). It will be convenient to write \\(m = a + b\\) for the total amount of money, so Bob starts with £\\((m-a)\\). At each step of the game, both players bet £\\(1\\); Alice wins £\\(1\\) off Bob with probability \\(p\\), or Bob wins £\\(1\\) off Alice with probability \\(q\\). The game continues until one player is out of money. Let \\(X_n\\) denote how much money Alice has after \\(n\\) steps of the game. We can write this as a stochastic process with discrete time \\(n = \\{0,1,2,\\dots\\} = \\mathbb Z_+\\) and discrete state space \\(\\mathcal S = \\{0,1,\\dots,m\\}\\), where \\(X_0 = a\\) and, for \\(n \\geq 0\\), we have \\[ X_{n+1} = \\begin{cases} X_n + 1 &amp; \\text{with probability $p$ if $1\\leq X_n \\leq m-1$,} \\\\ X_n - 1 &amp; \\text{with probability $q$ if $1\\leq X_n \\leq m-1$,} \\\\ 0 &amp; \\text{if $X_n = 0$,} \\\\ m &amp; \\text{if $X_n = m$.} \\end{cases} \\] Note also that the gambler’s ruin process \\((X_n)\\) clearly satisfies the Markov property: the next step \\(X_{n+1}\\) depends on where we are now \\(X_n\\), but, given that, does not depend on how we got here. The gambler’s ruin process is exactly like a simple random walk started from \\(X_0 = a\\) except that we have absorbing barriers and \\(0\\) and \\(m\\), where the game stops because one of the players has `ruined’ – that is, lost all their money. (One can also consider random walks with reflecting barriers, that bounce the random walk back into the state space, or mixed barriers that are absorbing or reflecting at random.) 3.2 Probability of ruin The gambling game continues until either Alice is ruined (\\(X_n = 0\\)) or Bob is ruined (\\(X_n = m\\)). A natural question to ask is: What is the probability that the game ends in Alice’s ruin? Let us write \\(r_i\\) for the probability Alice end up ruined if she has £\\(i\\). Then the probability of ruin for the whole game is \\(r_a\\), since Alice initially starts with £\\(a\\). The probability Bob is ruined is \\(1 - r_a\\), since eventually one of the players must lose. What can we say about \\(r_i\\)? Clearly we have \\(r_0 = 1\\) and \\(r_m = 0\\), since this means Alice (\\(i=0\\)) or Bob (\\(i=m\\)) is out of money and is ruined. What about for \\(1 \\leq i \\leq m-1\\)? The key is to condition on the first step. That is, we can write \\[\\begin{align} \\mathbb P(\\text{ruin}) &amp;= \\mathbb P(\\text{win first round}) \\, \\mathbb P(\\text{ruin} \\mid \\text{win first round}) \\\\ &amp;\\qquad{}+ \\mathbb P(\\text{lose first round}) \\, \\mathbb P(\\text{ruin} \\mid \\text{lose first round}) \\\\ &amp;= p\\,\\mathbb P(\\text{ruin} \\mid \\text{win first round}) + q \\,\\mathbb P(\\text{ruin} \\mid \\text{lose first round}) . \\end{align}\\] Here we have conditioned on whether Alice wins or loses the first round. More formally, we have used the , which says that if the disjoint events \\(B_1, \\dots, B_k\\) cover the whole sample space, then \\[ \\mathbb P(A) = \\sum_{i=1}^k \\mathbb P(A \\cap B_i) = \\sum_{i=1}^k \\mathbb P(B_i) \\, \\mathbb P(A \\mid B_i) . \\] This idea of conditioning on the first step will be a crucial tool throughout this module. Note that if Alice wins the first round from having £\\(i\\), she now has £\\((i+1)\\). By the Markov property, we now see that her probability of ruin is \\(r_{i+1}\\), because it’s as if the game were starting again with Alice having £\\((i+1)\\) to start with. The Markov property tells us that it doesn’t matter Alice got to having £\\((i+1)\\), it only matters how much she has now. Similarly, if Alice loses the first round, she now has £\\((i-1)\\), and the ruin probability is \\(r_{i-1}\\). Hence we have \\[ r_i = pr_{i+1} + qr_{i-1}. \\] Rearranging, and including the boundary conditions, we see that the equation we want to solve is \\[ pr_{i+1} - r_i + qr_{i-1} = 0 \\qquad \\text{subject to} \\qquad r_0 = 1,\\ r_m = 0. \\] This is a linear difference equation – and, because the left-hand side is \\(0\\), we call it a homogeneous linear difference equation. We will see how to solve this equation in the next lecture. We will see that, if we set \\(\\rho = q/p\\), then the ruin probability is given by \\[ r_a = \\begin{cases} \\displaystyle\\frac{\\rho^a - \\rho^m}{1 - \\rho^m} &amp; \\text{if $\\rho \\neq 1$,} \\\\[0.35cm] 1 - \\displaystyle\\frac{a}{m} &amp; \\text{if $\\rho = 1$.} \\end{cases} \\] Note that \\(\\rho = 1\\) is the same as the condition \\(p = q = 1/2\\). Imagine Alice is not playing against her opponent Bob, but rather is up against a large casino. In this case, the casino’s capital £\\((m-a)\\) is typically much bigger than Alice’s £\\(a\\). We can model this by keeping \\(a\\) fixed taking a limit \\(m \\to \\infty\\). Typically, the casino has `an edge’, meaning that \\(q &gt; p\\), so \\(\\rho &gt; 1\\). In this case, we see that the ruin probability is \\[ \\lim_{m \\to \\infty} r_a = \\lim_{m \\to \\infty} \\frac{\\rho^a - \\rho^m}{1 - \\rho^m} = \\lim_{m \\to \\infty} \\frac{\\rho^a/\\rho^m - 1}{1/\\rho^m - 1} = \\frac{0-1}{0-1} = 1, \\] so Alice will be ruined with certainty. Even with a generous casino that offers an exactly fair game with \\(p = q\\), so \\(\\rho = 1\\), we have \\[ \\lim_{m \\to \\infty} r_a = \\lim_{m \\to \\infty}\\left( 1 - \\frac{a}{m} \\right) = 1-0 = 1 , \\] so, even with this fair game, Alice will still certainly be ruined. 3.3 Expected duration of the game We could also ask for how long we expect the game to last. We approach this like before. Let \\(d_i\\) be the expected duration of the game when Alice has £\\(i\\). Our boundary conditions are \\(d_0 = d_m = 0\\), because \\(X_n = 0\\) or \\(m\\) means that the game is over. Again, we proceed by conditioning on the first step, so \\[\\begin{align} \\mathbb E(\\text{duration}) &amp;= \\mathbb P(\\text{win first round}) \\, \\mathbb E(\\text{duration} \\mid \\text{win first round}) \\\\ &amp;\\qquad{}+ \\mathbb P(\\text{lose first round}) \\, \\mathbb E(\\text{duration} \\mid \\text{lose first round}) \\\\ &amp;= p\\,\\mathbb E(\\text{duration} \\mid \\text{win first round}) + q \\,\\mathbb E(\\text{duration} \\mid \\text{lose first round}) . \\end{align}\\] More formally, we’ve used another version of the law of total probability, \\[ \\mathbb E(X) = \\sum_{i=1}^k \\mathbb P(B_i) \\, \\mathbb E(X \\mid B_i) , \\] or, alternatively, the for expectations \\[ \\mathbb E(X) = \\mathbb E_Y \\mathbb E (X \\mid Y) = \\sum_{y} \\mathbb P(Y= y)\\, E(X \\mid Y = y)\\] Now, the expected duration given we win the first round is \\(1 + d_{i+1}\\). This is because the round itself takes \\(1\\) time step, and then by the Markov property, it’s as if we are starting again from \\(i+1\\). Similarly, the expected duration given we lose the first round is \\(1 + d_{i-1}\\). Thus we have \\[ d_i = p(1 + d_{i+1}) + q (1 + d_{i-1}) = 1 + pd_{i+1} + qd_{i-1} . \\] Rearranging, and including the boundary conditions, we have another linear difference equation: \\[ pd_{i+1} - d_i + qd_{i-1} = -1 \\qquad \\text{subject to} \\qquad d_0 = 0,\\ d_m = 0. \\] Because the right-hand side, \\(-1\\), is nonzero, we call this an inhomogeneous linear difference equation. Again, we’ll see how to solve this in the next lecture, and will find that the solution is given by \\[ d_a = \\begin{cases} {\\displaystyle \\frac{1}{q-p} \\left(a - m\\frac{1-\\rho^a}{1- \\rho^m} \\right)} &amp; \\text{if $\\rho \\neq 1$,} \\\\[0.35cm] \\displaystyle a(m-a) &amp; \\text{if $\\rho = 1$.} \\end{cases} \\] Thinking again of playing against the casino, with \\(q &gt; p\\), \\(\\rho &gt; 1\\), and \\(m \\to \\infty\\), we see that the expected duration is \\[ \\lim_{m\\to\\infty} d_a = \\lim_{m\\to\\infty} \\frac{1}{q-p} \\left(a - m\\frac{1-\\rho^a}{1 - \\rho^m} \\right) = \\frac{1}{q-p} \\left(a - 0 \\right) = \\frac{a}{q-p} , \\] since \\(\\rho^m\\) grows much quicker than \\(m\\). So Alice ruins with certainty, and will take time \\(a/(q-p)\\), on average. In the case of the generous casino, though, with \\(q = p\\), so \\(\\rho = 1\\), we have \\[ \\lim_{m\\to\\infty} d_a = \\lim_{m\\to\\infty} a(m-a) = \\infty . \\] So here, Alice will ruin with certainty, but it may take a very long time until the ruin occurs, since the average duration is infinite. "]
]
