<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Random Variables | MATH1710 Probability and Statistics 1</title>
  <meta name="description" content="Lecture notes for MATH1710 Probability and Statistics 1 at the University of Leeds." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Random Variables | MATH1710 Probability and Statistics 1" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes for MATH1710 Probability and Statistics 1 at the University of Leeds." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Random Variables | MATH1710 Probability and Statistics 1" />
  
  <meta name="twitter:description" content="Lecture notes for MATH1710 Probability and Statistics 1 at the University of Leeds." />
  

<meta name="author" content="Robert G Aykroyd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="conditional-probability-and-independence.html"/>
<link rel="next" href="models-for-count-data.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>MATH1710</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Overview</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contact-information"><i class="fa fa-check"></i>Contact Information</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#syllabus-details"><i class="fa fa-check"></i>Syllabus Details</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#methods-of-teaching"><i class="fa fa-check"></i>Methods of Teaching</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#booklist"><i class="fa fa-check"></i>Booklist</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>1</b> Exploratory Data Analysis in R</a><ul>
<li class="chapter" data-level="1.1" data-path="eda.html"><a href="eda.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="eda.html"><a href="eda.html#frequency-and-relative-frequency"><i class="fa fa-check"></i><b>1.2</b> Frequency and relative frequency</a></li>
<li class="chapter" data-level="1.3" data-path="eda.html"><a href="eda.html#histograms-time-series-plots-and-scatterplots"><i class="fa fa-check"></i><b>1.3</b> Histograms, time series plots and scatterplots</a></li>
<li class="chapter" data-level="1.4" data-path="eda.html"><a href="eda.html#numerical-summary-statistics"><i class="fa fa-check"></i><b>1.4</b> Numerical summary statistics</a></li>
<li class="chapter" data-level="1.5" data-path="eda.html"><a href="eda.html#the-5-figure-summary-and-boxplots"><i class="fa fa-check"></i><b>1.5</b> The 5-figure summary and boxplots</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basic-probability.html"><a href="basic-probability.html"><i class="fa fa-check"></i><b>2</b> Basic Probability</a><ul>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#background"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="2.1" data-path="basic-probability.html"><a href="basic-probability.html#sample-space-and-events"><i class="fa fa-check"></i><b>2.1</b> Sample space and events</a><ul>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#the-venn-diagram"><i class="fa fa-check"></i>The Venn diagram</a></li>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#operations-with-events"><i class="fa fa-check"></i>Operations with events</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basic-probability.html"><a href="basic-probability.html#the-axioms-and-basic-rules-of-probability"><i class="fa fa-check"></i><b>2.2</b> The axioms and basic rules of probability</a></li>
<li class="chapter" data-level="2.3" data-path="basic-probability.html"><a href="basic-probability.html#assignment-of-probability"><i class="fa fa-check"></i><b>2.3</b> Assignment of probability</a><ul>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#classical-probability-for-equally-likely-events"><i class="fa fa-check"></i>Classical probability for equally-likely events</a></li>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#probability-as-relative-frequency-and-the-law-of-large-numbers"><i class="fa fa-check"></i>Probability as relative frequency and the Law of Large Numbers</a></li>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#subjective-assignment-of-probability"><i class="fa fa-check"></i>Subjective assignment of probability</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#combinatorics"><i class="fa fa-check"></i>Combinatorics</a><ul>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#basic-definitions"><i class="fa fa-check"></i>Basic definitions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html"><i class="fa fa-check"></i><b>3</b> Conditional Probability and Independence</a><ul>
<li class="chapter" data-level="3.1" data-path="eda.html"><a href="eda.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#definitions"><i class="fa fa-check"></i><b>3.2</b> Definitions</a></li>
<li class="chapter" data-level="3.3" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#independent-events"><i class="fa fa-check"></i><b>3.3</b> Independent events</a></li>
<li class="chapter" data-level="3.4" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#theorem-of-total-probability-and-bayes-theorem"><i class="fa fa-check"></i><b>3.4</b> Theorem of total probability and Bayes’ theorem</a><ul>
<li class="chapter" data-level="" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#total-probability-formula"><i class="fa fa-check"></i>Total probability formula</a></li>
<li class="chapter" data-level="" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#bayes-rule"><i class="fa fa-check"></i>Bayes’ rule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>4</b> Random Variables</a><ul>
<li class="chapter" data-level="4.1" data-path="random-variables.html"><a href="random-variables.html#basic-rv-definitions"><i class="fa fa-check"></i><b>4.1</b> Basic definitions</a></li>
<li class="chapter" data-level="4.2" data-path="random-variables.html"><a href="random-variables.html#expected-value-and-variance"><i class="fa fa-check"></i><b>4.2</b> Expected value and variance</a><ul>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#properties-of-expectation"><i class="fa fa-check"></i>Properties of expectation</a></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#estimation-of-parameters-using-the-expectation"><i class="fa fa-check"></i>Estimation of parameters using the expectation</a></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#variance-of-a-random-variable"><i class="fa fa-check"></i>Variance of a random variable</a></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#properties-of-variance"><i class="fa fa-check"></i>Properties of variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="models-for-count-data.html"><a href="models-for-count-data.html"><i class="fa fa-check"></i><b>5</b> Models for Count Data</a></li>
<li class="chapter" data-level="6" data-path="models-for-measurement-data.html"><a href="models-for-measurement-data.html"><i class="fa fa-check"></i><b>6</b> Models for Measurement Data</a></li>
<li class="chapter" data-level="7" data-path="bayesian-methods.html"><a href="bayesian-methods.html"><i class="fa fa-check"></i><b>7</b> Bayesian Methods</a></li>
<li class="divider"></li>
<li><a href="https://www.leeds.ac.uk/">University of Leeds</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH1710 Probability and Statistics 1</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-variables" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Random Variables</h1>

<div id="basic-rv-definitions" class="section level2">
<h2><span class="header-section-number">4.1</span> Basic definitions</h2>
<p>Statistical models often describe the experimental outcome in terms of probability statements.
In particular, a random variable is defined along with a distribution which might depend on a parameter.
For example, let random variable <span class="math inline">\(X\)</span> record the number of <code>Heads</code> when a coin is tossed.
Then, <span class="math inline">\(X=1\)</span> corresponds to <code>Heads</code>, with probability <span class="math inline">\(p\)</span>, and <span class="math inline">\(X=0\)</span> to <code>Tails</code>, with probability <span class="math inline">\(1-p\)</span>.
In this example, <span class="math inline">\(p\)</span> is the parameter of the
model, which might be unknown.
Tossing the coin many times and observing the outcomes
would give us a better idea of the value of <span class="math inline">\(p\)</span>.
Hence, to learn about parameter values, we need to collect data.
Then, taken together, our model and estimated parameter value give us an approximation to reality which can then be used to describe the current situation and to make predictions about the future.</p>
<p>Formally, a random variable is
“a function which maps elements of the sample space onto the set of real numbers”.
Informally, it is a numerical value which summarizes a random experiment by measuring some property of interest.</p>
<p><img src="math1710_files/figure-html/rv-1.png" width="450" /></p>
<p>It is common to use the letters <span class="math inline">\(X, Y, Z\)</span> to denote random variables.
The set of all possible values which can be taken by the random variable is called the <strong>range space</strong>.</p>
<p>A discrete random variable <span class="math inline">\(X\)</span>, say, has a finite, or countably infinite, range space which is denoted
<span class="math inline">\(\Omega_X =\{x_1, x_2,\ldots \}\)</span>
and the corresponding probabilities are written as
<span class="math display">\[
Pr( \{X=x_i\}) = p_X(x_i) =p_i \qquad i=1,2,\ldots
\]</span></p>
<p>The possible values and corresponding probabilities is called the <strong>probability mass function</strong> (pmf) and is often shown in a table.
Note that the probability mass function must satisfy the Axioms of Probability, hence
<span class="math display">\[
0\le p_X(x_i)\le 1 \qquad \mbox{for all $x_i\in \Omega_X$}
\]</span><br />
and
<span class="math display">\[\displaystyle \sum_{x_i\in \Omega_X} p_X(x_i)=1\]</span>
where the sum is over all elements of the range space.</p>

<div class="example">
<span id="exm:unnamed-chunk-1" class="example"><strong>Example 4.1  </strong></span>The following is an example probability mass function, with
<span class="math inline">\(\Omega_X = \{-1, 0, 0.5, 1, 2\}\)</span>,
</div>

<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(-1\)</span></th>
<th align="center"><span class="math inline">\(0\)</span></th>
<th align="center"><span class="math inline">\(0.5\)</span></th>
<th align="center"><span class="math inline">\(1\)</span></th>
<th align="center"><span class="math inline">\(2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(p_X(x)\)</span></td>
<td align="center"><span class="math inline">\(0.1\)</span></td>
<td align="center"><span class="math inline">\(0.3\)</span></td>
<td align="center"><span class="math inline">\(0.3\)</span></td>
<td align="center"><span class="math inline">\(0.2\)</span></td>
<td align="center"><span class="math inline">\(0.1\)</span></td>
</tr>
</tbody>
</table>
<p><img src="math1710_files/figure-html/pmf-1.png" width="500" style="display: block; margin: auto;" /></p>
<p>The graph is produced in R using the command <code>plot</code> with an option <code>type = "h"</code>.</p>
<p>The <strong>cumulative distribution function</strong> (cdf) is defined as
<span class="math display">\[
F_X(x) = Pr( \{X\le x\}) = \sum _{x_i\le x} Pr(\{X=x_i\}).
\]</span>
Note that this function is defined for all real numbers, and not just at the possible values.
This means that for discrete random variable the cdf is a <em>step function</em>.</p>
<p><strong>Example 4.1 cont.</strong>
Consider the earlier probability mass function, which gives the following cumulative distribution function.</p>
<p><span class="math display">\[ F_X(x) = \begin{cases}
0.0 &amp;        x&lt; - 1 \\
0.1 &amp; -1\le  x &lt; 0 \\
0.4 &amp; 0 \le  x &lt; 0.5 \\
0.7 &amp; 0.5\le x &lt;1 \\
0.9 &amp; 1\le   x &lt;2 \\
1.0 &amp; 2\le   x \end{cases} \]</span></p>
<p><img src="math1710_files/figure-html/cdf-1.png" width="500" style="display: block; margin: auto;" /></p>
<p>The graph is produced in R using the commands <code>cumsum</code>, <code>stepfun</code>, and <code>plot</code>.</p>
</div>
<div id="expected-value-and-variance" class="section level2">
<h2><span class="header-section-number">4.2</span> Expected value and variance</h2>
<p><strong>Definition:</strong>
The <strong>expectation</strong>, or <strong>expected value</strong>, of random variable <span class="math inline">\(X\)</span> is defined as
<span class="math display">\[
E[X] = \sum_{i = 1}^N x_i \; p_X(x_i).
\]</span>
This is a weighted average of the possible values.
The quantity <span class="math inline">\(E[X]\)</span> is also called the “mean of <span class="math inline">\(X\)</span>” and is sometimes denoted using the symbol <span class="math inline">\(\mu\)</span> (say “mu”).</p>
<p><strong>Example 4.1 cont.</strong>
Consider again the above random variable, then the expectation can be evaluated as
<span class="math display">\[
E[X] = (-1)\times 0.1 + 0 \times 0.3 + 0.5\times 0.3 +1\times 0.2 + 2\times 0.1 = 0.45.
\]</span></p>
<p>Note that, although we call it the <em>expected value</em>, in fact for many random variables, such as the one above, the actual expected value can never occur.
Instead we should think of it as the long-term average.</p>
<div id="properties-of-expectation" class="section level3 unnumbered">
<h3>Properties of expectation</h3>
<ul>
<li><p>(E1) The expectation of a constant is the constant itself,
if <span class="math inline">\(c\)</span> is a constant then <span class="math inline">\(E[c]=c\)</span>.</p></li>
<li><p>(E2) Expectation is a <em>linear operator</em>, that is if both <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, then
<span class="math display">\[
E[aX+b] = aE[X]+b.
\]</span>
A simple case of this is when
$Y= 3-2X $, and then
<span class="math inline">\(E[Y] = E[3-2X] = 3-2 E[X]\)</span>.</p>
<p><strong>Proof:</strong> To see this let <span class="math inline">\(Y=aX+b\)</span>, and then note that
<span class="math inline">\(\{Y=y_i\}\)</span> where <span class="math inline">\(y_i=ax_i+b\)</span> and <span class="math inline">\(\{X=x_i\}\)</span> are equivalent events and hence
<span class="math inline">\(p_y(y_i)=p_X(x_i)\)</span>.
Then
<span class="math display">\[\begin{align*}
E[Y] &amp; = \sum y_i p_Y(y_i) = \sum (ax_i +b)p_X(x_i)\\
&amp;= a \sum x_i p_X(x_i) + b\sum p_X(x_i) = a E[X]+b
\end{align*}\]</span>
where the last step uses the definition of the expectation of <span class="math inline">\(X\)</span> and the second Axiom.</p></li>
<li><p>(E3)
Consider a collection of random variables <span class="math inline">\(X_1, X_2,\ldots, X_n\)</span> and constants <span class="math inline">\(c_1, c_2,\ldots, c_n\)</span>.
Let <span class="math inline">\(Y=\sum_{j=1}^n c_j X_j\)</span>, that is <span class="math inline">\(Y\)</span> is a linear combination of the <span class="math inline">\(X_j\)</span>, then
<span class="math display">\[
E[Y] = \sum_{j=1}^n c_j E[X_j].
\]</span>
A simple case of this is when
<span class="math inline">\(Y=X_1-2X_2\)</span>, and then
<span class="math inline">\(E[Y] = E[X_1-2X_2] = E[X_1]-2 E[X_2]\)</span>.
Again, we see the linear properties of expectation.</p></li>
</ul>

<div class="example">
<p><span id="exm:unnamed-chunk-2" class="example"><strong>Example 4.2  </strong></span>In October the average monthly rainfall in Leeds is 2.3 inches with average minimum daily temperature of 11<span class="math inline">\(^oC\)</span> and
average maximum daily temperature of 18<span class="math inline">\(^oC\)</span>.</p>
<p>Suppose we want these in cm (<span class="math inline">\(1\)</span> inch equal <span class="math inline">\(2.5\)</span>cm) and <span class="math inline">\(^oF\)</span>
(<span class="math inline">\({\tt T}_{F} = \frac95 \times {\tt T}_{C} + 32\)</span>).</p>
<ul>
<li>The expected rainfall is <span class="math inline">\(2.3\times 2.54=5.84\)</span>cm.</li>
<li>The expected minimum temperature is <span class="math inline">\(\frac95\times 11+32=51.8^oF\)</span>.</li>
<li>The expected maximum temperature is <span class="math inline">\(\frac95\times 18+32=64.4^oF\)</span>.
</div></li>
</ul>
</div>
<div id="estimation-of-parameters-using-the-expectation" class="section level3 unnumbered">
<h3>Estimation of parameters using the expectation</h3>
<p>Many models will contain unknown parameters and one aim of a statistical analysis is to use data to say something
about likely values of the parameter — this process is called <em>estimation</em> or <em>inference</em>.
One of the simplest cases is the unknown probability of <code>Heads</code> in a biased coin, but we will see many more examples later in the module.
The basic idea is to use the data, <span class="math inline">\(\underline{x} =(x_1, x_2,..., x_n)\)</span>, to make a “good guess” at the numerical value of the parameter.
Let the parameter be called <span class="math inline">\(\theta\)</span> (say “theta”), then
an <em>estimate</em>, <span class="math inline">\(\hat \theta= \hat \theta (\underline{x})\)</span> is a numeric value which is a function of the data — different datasets lead to different estimates.
The simplest approach to estimation is to choose the value of the parameter so that the theoretical mean is equal to the sample mean.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-3" class="example"><strong>Example 4.3  </strong></span>Let random variable <span class="math inline">\(X\)</span> represent the number of <code>Heads</code> when a coin is toss once with
<span class="math inline">\(Pr(\{X=1\})=p\)</span> and
<span class="math inline">\(Pr(\{X=0\})=1-p\)</span>, and hence <span class="math inline">\(p\)</span> is the unknown parameter.
Now, the expectation is given by
<span class="math inline">\(E[X]=1\times p+0\times(1-p) = p\)</span>.</p>
<p>Also, let <span class="math inline">\(\underline{x}=(x_1,\ldots, x_n)\)</span> be a corresponding dataset obtained by tossing the coin <span class="math inline">\(n\)</span> times, with <span class="math inline">\(\bar{x}\)</span> being the sample mean.
Hence, the estimate is simply given as
<span class="math inline">\(\hat p = \bar x\)</span>.</p>
<p>Suppose a sequence of <span class="math inline">\(10\)</span> tosses yields
<span class="math inline">\(\underline{x}=(1,0,0,1,1,1,0,1,0,1)\)</span>, then <span class="math inline">\(\bar{x}=0.6\)</span> hence <span class="math inline">\(\hat{p}=\bar{x}=0.6\)</span>.</p>
R commands:
<code>x = c(1, 0, 0, 1, 1, 1, 0, 1, 0, 1)</code> and then
<code>phat = mean(x)</code>
</div>

</div>
<div id="variance-of-a-random-variable" class="section level3 unnumbered">
<h3>Variance of a random variable</h3>
<p>The mean (or expectation) gives a “typical” or “representative” value,
<span class="math inline">\(E[X]=\mu\)</span> for random variable <span class="math inline">\(X\)</span>, but it is also of interest to know about variation around the mean.</p>
<p>We might imagine looking at the expected value of deviations of the random variable from the mean, but this is useless, as <span class="math inline">\(E[X-\mu]=E[X]-\mu=0\)</span>,
hence, instead, we consider the expected squared deviation.</p>
<p><strong>Definition:</strong> The variance of random variable <span class="math inline">\(X\)</span> is defined as
<span class="math display">\[
Var(X)= E[(X- E[X] )^2]
\]</span>
and the (positive) square-root is called the standard deviation,
<span class="math inline">\(SD(X)=\sqrt{Var(X)}\)</span>.</p>
<p>In practice we usually evaluate the variance using the equivalent expression
<span class="math display">\[
Var(X) = E[X^2] - \{E[X]\}^2.
\]</span></p>
<p><strong>Proof:</strong>
Starting with the definition and
then multiplying the square
<span class="math display">\[ Var(X) = E[(X-E[X])^2] = E[X^2-2XE[X]+E[X]^2] \]</span>
using the standard properties of expectation
<span class="math display">\[ Var(X) = E[X^2]-2E[X]E[X]+E[X]^2. \]</span>
Finally, collecting terms together, gives
<span class="math display">\[ Var(X) = E[X^2] - \{E[X]\}^2. \]</span></p>
<p><strong>Example 4.1 cont.</strong>
Consider again the earlier random variable with <span class="math inline">\(E[X]=0.45\)</span>, then we need
<span class="math display">\[
E[X^2] = (-1)^2\times 0.1 + 0^2 \times 0.3 + 0.5^2\times 0.3 +1^2\times 0.2 + 2^2\times 0.1 =  0.775
\]</span>
giving
<span class="math display">\[
Var(X) = E[X^2]-\{E[X]\}^2 
=  0.775- \left\{0.45\right\}^2
= 0.5725.
\]</span>
The first step can be easily calculated in R using
<code>sum(xvals^2 * probs)</code>, with <code>xvals</code> and <code>probs</code> as before.</p>
</div>
<div id="properties-of-variance" class="section level3 unnumbered">
<h3>Properties of variance</h3>
<ul>
<li><p>(V1) The variance of a constant is zero, <span class="math inline">\(Var[c]=0\)</span>.</p></li>
<li><p>(V2) For constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we have
<span class="math display">\[
Var[aX+b] = a^2 Var[X].
\]</span></p>
<p><strong>Proof</strong> Recall, with <span class="math inline">\(Y=aX+b\)</span>, that <span class="math inline">\(E[Y]=aE[X]+b\)</span>, then
<span class="math display">\[\begin{align*}
Var(Y) &amp;= E[(Y-E[Y])^2] \\
&amp;= E\left[\left(aX+b -aE[X]- b\}\right)^2\right] \\
&amp;= E[(aX-aE[X])^2] = a^2 E[(X-E[X])^2] = a^2 Var(X).
\end{align*}\]</span></p></li>
<li><p>(V3) For (independent) random variables <span class="math inline">\(X_1,\ldots , X_n\)</span> and constants <span class="math inline">\(c_1,\ldots , c_n\)</span>, and with <span class="math inline">\(Y=\sum_{j=1}^n c_i X_i\)</span>, then
<span class="math display">\[
Var[Y] = \sum_{i=j}^n c_j^2 Var[X_j].
\]</span></p></li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li>Variance is unaffected by an additive shift in the random variable, but a multiplicative scaling has a quadratic effect – compare to the linear properties of expectation.</li>
<li>In property (V3) above, we see that an additional condition of independence was included.
Two (or more) random variable are said to be independent if the value of one tells us nothing about the value of the other.
If the random variables relate to physically separate experiments then we can assume independence, but otherwise we cannot.</li>
</ul>
<p>\end{itemize}</p>
<!--

\addtocounter{exn}{-1}
\begin{example}(cont.)
For the eight-sided dice example earlier we saw that 
$E[X]=14/3$ and $Var[X]=47/9$.

\medskip

Consider $Z=6-3X$ then
\[ 
E[Z] =E[6-3X] = 6-3E[X] = 6-3\times \frac{14}{3} = -8
\]
and
\[ 
Var[Z] =Var[6-3X] = (-3)^2Var[X] = 9\times  \frac{47}{9}= 47.
\]

Now suppose that we also have a standard six-sided die which has
\( E[Y]=7/2\) and \(Var[Y]= 35/12\)
Then, the variance of the sum of the two dice is
\[Var[X+Y] = Var[X]+Var[Y]
= \frac{47}{9}+\frac{35}{12} = \frac{879}{108} = 8.14.
\]
Note that here, $X$ and $Y$ are physically independent and so using this result for independent random variables is valid.
\end{example}

%
%\begin{boxit}
%Now complete Worksheet 5 on basics of discrete random variables 
%to check your understanding.
%\end{boxit}
%
%\begin{boxit}
%Now complete Worksheet 6 on properties of expectation and variance 
%to check your understanding.
%\end{boxit}

\newpage

\index{Expectation}\index{Functions of random variables}
%\index{Essential directed reading}

%\rhead{Additional reading}


\renewcommand{\baselinestretch}{1.3}

\newpage
%\index{Essential directed reading}
\rhead{Essential directed reading}

\subsection*{Functions of random variables}

We have seen how to find the expectation and variance of  linear functions, e.g.\ $E[aX+b]=aE[X]+b$.
Now we shall see what to do for other general functions, $Y=g(X)$, such as $s^X$ or $e^{tX}$ (for constants \(s\) and \(t\)).

The function $y=g(x)$ maps points in the range space of $X$, $\Omega_X$, to points in the range space of $Y$, $\Omega_Y$.
Hence, if $\Omega_X = \{x_1, x_2,\ldots\}$, then we can determine the range space of $Y$ as
the distinct elements of $\{g(x_1), g(x_2),\ldots\}$.

Note that as more than one element of $\Omega_X$ might map to the same point in $\Omega_Y$, then $\Omega_Y$ cannot contain more elements than $\Omega_X$. 
Hence if  $\Omega_X$ is finite or countable infinite then so is $\Omega_Y$ which also means that if $X$ is discrete then so is $Y$.
%
If \(X\) is a continuous random variable, then usually \(Y\) will also be continuous but there is no guarantee, for example
if \(Y={\rm integer}(X)\). Hence, we should consider each case carefully.

\bigskip

\begin{center}
\begin{tikzpicture}[line width=0.25pt, scale=0.6]

\draw[thick, ->] (0,1) -- (6,1);
\draw[thick] (0.5,0.8) -- (0.5,1.2);
\draw (-0.5,2) node {$\Omega_X$};

\draw[thick, ->] (8,1) -- (12,1);
\draw[thick] (8.5,0.8) -- (8.5,1.2);
\draw (12,2) node {$\Omega_Y$};

\draw [thick, ->] (5,1) .. controls (5,3) and (9,3) .. (9,1);
\draw [thick, ->] (3,1) .. controls (3,4) and (9,4) .. (9,1);

\draw (3,0.5) node {$x_1$};
\draw (5,0.5) node {$x_2$};

\draw (9,0.5) node {$y$};

\end{tikzpicture}
\end{center}

\bigskip

We can then evaluate the probability mass function of $Y$ by transferring probabilities using the idea of {\it equivalent events}.

Consider each of the elements of the range space of $Y$ in turn;
for element $y$ from $\Omega_Y$:

\hspace*{10mm} $\bullet$ if  $\{Y=y\}$ implies that $\{X=x\}$, then 
\\[-4mm]
\[
p_Y(y)=p_X(x) \hspace*{58mm} \
\]

\hspace*{10mm} $\bullet$ if $\{Y=y\}$ implies that $\{X\in(x_1, x_2,\ldots)\}$, then \\[-4mm]
\[
p_Y(y)=p_X(x_1)+ p_X(x_2)+\cdots = \sum_{x:g(x)=y} p_X(x).
\]

%%
Let us now check that the axioms are still valid:

Since $Pr(Y=y)=p_Y(y)$ is the sum of at least one $p_X(x)$ then since 
$p_X(x)> 0 $ for all $x\in \Omega_X$, then 
$p_Y(y)>0$ for all $y\in \Omega_Y$. Hence axiom (K1) is valid.

Also, starting with 
$Pr(\Omega_X)=\sum _x p_X(x)=1$, then consider
$Pr(\Omega_Y)=\sum _y p_Y(y) 
= \sum _y \sum_{x:g(x)=y} p_X(x) = \sum _x p_X(x)=1$.
Hence axiom (K2) is valid.
%

This can be illustrated as follows.
Suppose that $n_1$ of the elements of $\Omega_X$ map to the first
element of $\Omega_Y$, 
$n_2$ map to the second, etc.\ until $n_m$ map to the final element of $\Omega_Y$.
Then after possible reordering and relabelling we have
%

\hspace*{5mm}
\begin{minipage}{\linewidth}
$x_{1}, \ldots, x_{n_1}$ map to $y_1$, and hence
$g(x_{1})=\cdots = g(x_{n_1})=y_1$ \\
%
$x_{n_1+1}, \ldots, x_{n_1+n_2}$ map to $y_2$, and hence
$g(x_{n_1+1})=\cdots = g(x_{n_1+n_2})=y_2$ \\
%
$\vdots$ \\
%
$x_{n_1+\cdots +n_{m-1}}, \ldots, x_{n_1+\cdots +n_m}$ map to $y_m$, and so
$g(x_{n_1+\cdots +n_{m-1}1})=\cdots = g(x_{n_1+\cdots +n_m})=y_m$ 
\end{minipage}

If we now want $p_Y(y)$ then this can be obtained by adding all the probabilities of the corresponding $x$ values, that is all
$x$ such that $g(x)=y$ giving $p_Y(y) = \sum _{x:g(x)=y)} p_X(x)$,
for example $p_Y(y_1) =  \sum _{x:g(x)=y_1)} p_X(x) 
= p_X(x_1)+\cdots+p_X(x_{n_1})$.
%

%\newpage

Further, \\[-8mm]
\begin{align*}
\sum_y p_Y(y) &= p_Y(y1)+p_Y(y_2)+\cdots + p_Y(y_m)\\[-4mm]
& =
p_X(x_{1})+ \cdots + p_X(x_{n_1})+
p_X(x_{n_1+1})+ \cdots + p_X(x_{n_1+n_2}) \\
& \hspace*{30mm} +
p_X(x_{n_1+\cdots +n_{m-1}})+ \cdots + p_X(x_{n_1+\cdots +n_m})
\end{align*}

\vspace{-8mm}

%
\begin{example}
If $X$ has probability mass function: \index{Probability mass function}
\begin{center}
\begin{tabular}{c|ccc}
$X$ & $-1$&$0$&$1$ \\
\hline
$p_X(x)$ & $0.2$&$0.7$&$0.1$
\end{tabular}
\end{center}

Suppose we are interested in $Y=X^2$, then clearly 
$\Omega_Y=\{0, 1\}$ and
as $\{Y=0\}=\{X=0\}$ then $p_Y(0)=p_X(0)=0.7$, but
as $\{Y=1\}=\{X=-1\}\cup\{X=1\}$ hence 
$p_Y(1)=p_X(-1)+ p_X(1)=0.3$.
%
Notice that 
$E[Y]= \sum_y y p_(y) = 0\times 0.7 + 1\times 0.3 = 0.3$.
\end{example}


\vspace{-8mm}


\subsection*{The law of the unconscious statistician}
\index{Expectation}\index{Functions of random variables}

A simple and automatic approach uses the following theorem:
if $Y=g(X)$, then 
\[
E[g(X)] = 
\left\{
\begin{array}{ll}
\sum g(x) p_X(x) & \mbox{if \(X\) is discrete},\\[5mm]
\int g(x) f_X(x) &\mbox{if \(X\) is continuous.}
\end{array}
\right.
\]
We have already seen examples of this with \(E[X^2]\) and
\(E[X(X-1)]\), but there are many others.
For example, \(E[s^X]\) is known as the probability generating function and is particularly useful for deriving many theoretical results regarding discrete random variables.
\index{Probability density function}\index{Expectation}


\vspace{-8mm}


\subsection*{Proof:}

\noindent
\begin{minipage}{\textwidth}
\renewcommand{\baselinestretch}{0.9}

Starting with the definition of expectation of $Y$,
\begin{align*}
E[g(X)] = E[Y] &= \sum _y y p_Y(y) =  \sum _y y \sum _{x:g(x)=y} p_x(x) 
= \sum _y \sum _{x:g(x)=y} y p_x(x) 
\intertext{using $p_Y(y) = \sum _{x:g(x)=y} p_x(x)$
and replacing $y$ by the equivalent numerical value $g(x)$}
&= \sum _y \sum _{x:g(x)=y} g(x)p_x(x) 
\intertext{and noting that the double sum can be replaced by a single sum}
&=\sum _x g(x) p_x(x).
\end{align*}

\end{minipage}



%\addtocounter{exn}{-1}
%\begin{example}(cont.)
%
%\begin{center}
%\begin{tabular}{c|ccc}
%$x$ & $-1$ &$0$& $1$\\
%\hline
%$p_X(x)$ & $0.2$ & $0.7$ & $0.1$
%\end{tabular}
%\end{center}
%
%\bigskip
%\bigskip
%
%\[
%E[X^2] = \sum x^2 p_X(x)
%= (-1)^2\times 0.2  + (0)^2\times 0.7 +1^2 \times 0.1 =0.3
%\]
%as before.
%
%
%\end{example}



\newpage
\rhead{Additional reading}

\subsection*{Probability generating functions}
%\index{Probability density function}\index{Expectation}

We have already seen several discrete random variables (including Bernoulli, binomial, geometric, Poisson)
and their corresponding probability mass functions.
%
To help with the derivation of theoretical results, the same information can also, and sometimes more conveniently, be summarized by the \ul{probability generating functions} (pgf).
%
The pgf also has many special properties which make it more useful than the probability mass function.


For a discrete random variable, $X$, (taking only integer values) the probability generating function is defined as
\begin{align*}
G_X(s) & = p_X(0) s^0 + p_X(1) s^1 +  p_X(2) s^2 + \cdots
  = \sum_{x=0}^\infty s^x p_X(x) 
  \shortintertext{that is}
G_X(s)  & = E\left[s^X\right]
\end{align*}
where $s$ is an arbitrary variable.
%
It is useful to note that each probability multiplies the corresponding power of $s$.

\begin{example}
Suppose that $X$ described the outcome of the roll of a fair die, so
$\Omega_X=\{1,2,3,4,5,6\}$ and $p_X(x) = 1/6$ for
$x=1,2,3,4,5,6$.

\medskip

So for the probability generating function we have,\\[-5mm]
\begin{align*}
G_X(s)  =  \frac16 s^1 + \frac16 s^2 +\cdots + \frac16 s^6 
 = \frac16 \left( s+s^2+\cdots + s^6\right).
\end{align*}

\end{example}


\medskip

If we want to find the expectation consider the following
\[
G'_X(s) = \frac{dG_X(s)}{ds} = \frac{d}{ds} \sum _x p_X(x) s^x = \sum _x x p_X(x) s^{x-1}
\]
and so,
\[
G'_X(1) = \left. \frac{dG_X(s)}{ds}\right|_{s=1}
= \sum_x x p_X(x) = E[X].
\]


\addtocounter{exn}{-1}
\begin{example}(cont.)
In this example we have
\[
\frac{dG_X(s)}{ds} = \frac16 \left( 1+2s+\cdots + 6s^5\right)
\]
and with $s=1$ we get
\[
G'_X(1) = \frac16 \left( 1+2+\cdots + 6\right)
=\frac72
\]
and so $E[X] =7/2$.
\end{example}

\medskip

Also,
\[
G''_X(s) = \frac{dG_X^2(s)}{ds^2} = 
\frac{d^2}{ds^2} \sum _x p_X(x) s^x 
= \sum _x x(x-1) p_X(x) s^{x-2}
\]
and so,
\[
G''_X(1) = \left. \frac{d^2G_X(s)}{ds^2}\right|_{s=1}
= \sum_x x(x-1) p_X(x) = E[X(X-1)].
\]

Hence we can find the variance as
\[
Var(X) = E[X^2] -\left\{ E[X] \right\}^2
= E[X(X-1)]+E[X] - \left\{ E[X] \right\}^2
= G''_X(1) +G'(1) -\left\{G'_X(1)\right\}^2
\]

\medskip

\addtocounter{exn}{-1}
\begin{example}(cont.)
In this example we have
\[
G''_X(1) = \frac16 \left( 2\times 1+3\times 2 s\cdots + 6\times s^4\right)
\]
and with $s=1$ we get
\[
G''_X(1) = \frac16 \left( 2 + 6 + 12 +20 +30 \right)
=\frac{70}{6}.
\]
Hence we have
\[
Var(X) = \frac{70}{6} +\frac72 -\left\{\frac72\right\}^2 = \frac{35}{12}.
\]
\end{example}


\begin{example}
Suppose $X$ follows a Bernoulli distribution, that is
with $Pr(X=1)=p$ and $Pr(X=0)=1-p$.
\index{Bernoulli}

\medskip

Then
\[
G_X(s) = (1-p)s^0 +p s^1 = 1-p+ps
\]
so
\[
G'_X(s) = p \quad \mbox{ hence } \quad G'_X(1) = p
\]
and
\[
G''_X(s) = 0 \quad \mbox{ hence } \quad G''_X(1) = 0.
\]
Hence we have
\[
E[X] = p
\]
and
\[
Var(X) = G''_X(1) +G'_X(1) -\left\{G'_X(1)\right\}^2
= 0 + p - p^2 = p(1-p).
\]
\end{example}

%
%\newpage
%\rhead{Additional material}
%\subsection*{Estimation of parameters}
%\index{Estimating parameters}
%
%
%Statistical inference is the process where we attempt to say something
%about an unknown probability model based on a set of data which were
%generated by the model.
%This inference does not have the status of absolute truth, since there will 
%be (infinitely) many probability models which are consistent
%with a given set of data.
%All we can do is to establish that some of these models are plausible, while
%others are implausible.
%
%A common approach is to use a probability model for the data which is completely
%specified except for the  numerical values of a finite number of quantities
%called parameters.
%In this chapter we will introduce methods for making inferences about
%parameters assuming that the given model is correct.
%The idea is to use the data, $\underline{x} =(x_1, x_2,..., x_n)$, to 
%make a ``good guess" at the numerical value of a parameter, $\theta$.
%
%An \textsc{estimate}, $\hat \theta= \hat \theta (\underline{x})$ is a numeric
%value which is a function of the data.
%An \textsc{estimator} is a random variables, $\hat \theta (\underline{X})$,
%which is a function of a random sample $\underline{X} =(X_1, X_2,..., X_n)$.
%
%
%Assume that the $X_i$ are mutually independent with common p.d.f.\
%\index{Independence}
%$f(x; \theta_1, \theta_2,..., \theta_p)$.
%Then the $r$th population moment (about zero) is
%$$ E[X^r] = \mu _r (\theta_1, \theta_2,..., \theta_p)$$
%and the $r$th sample moment
%\vspace{-5mm}
%$$m_r = \frac{1}{n} \sum _{i=1} ^n x_i^r .$$
%
%The method of moments estimates $\theta_1, \theta_2,..., \theta_p$ is the solution of the $p$ simultaneous (non-linear) equations
%
%\vspace{-10mm}
%
%$$ \mu _r (\theta_1, \theta_2,..., \theta_p) = m_r, \hspace{10mm} 
%r=1, 2,..., p.$$
%
%This method of estimation has no general optimality properties and 
%sometimes does very badly, but usually provides sensible initial guesses
%for numerical search procedures.
%
%\subsubsection*{Example}
%\index{Exponential}
%
%\vspace{-3mm}
%
%Let $X$ be an exponential random variable with unknown parameter $\lambda$.
%Now let $x_i: i=1,.., n$ be a set of independent observations of this variable.
%%
%The first sample moment is the sample mean $\bar x$, and
%the first population moment is the expectation of $X$, i.e. $1/\lambda$.
%Hence, we find the method of moments estimate of $\lambda$ by solving
%$1/\hat \lambda = \bar x$, that is $\hat \lambda = 1/\bar x$.
%\index{Moments}
%
%\vfill
%\noindent
%\makebox[\textwidth]{\hrulefill}
%{\textsc{Further reading}: 
%Sections 8.1 to 8.5 of Rice, and Chapter 10 of Clarke and Cooke.}

-->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="conditional-probability-and-independence.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="models-for-count-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
