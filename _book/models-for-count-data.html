<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Models for Count Data | MATH1710 Probability and Statistics 1</title>
  <meta name="description" content="Lecture notes for MATH1710 Probability and Statistics 1 at the University of Leeds." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Models for Count Data | MATH1710 Probability and Statistics 1" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes for MATH1710 Probability and Statistics 1 at the University of Leeds." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Models for Count Data | MATH1710 Probability and Statistics 1" />
  
  <meta name="twitter:description" content="Lecture notes for MATH1710 Probability and Statistics 1 at the University of Leeds." />
  

<meta name="author" content="Robert G Aykroyd" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="random-variables.html"/>
<link rel="next" href="models-for-measurement-data.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><b>MATH1710</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Module Overview</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contact-information"><i class="fa fa-check"></i>Contact Information</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#syllabus-details"><i class="fa fa-check"></i>Syllabus Details</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#methods-of-teaching"><i class="fa fa-check"></i>Methods of Teaching</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#booklist"><i class="fa fa-check"></i>Booklist</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>1</b> Exploratory Data Analysis in R</a><ul>
<li class="chapter" data-level="1.1" data-path="eda.html"><a href="eda.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="eda.html"><a href="eda.html#frequency-and-relative-frequency"><i class="fa fa-check"></i><b>1.2</b> Frequency and relative frequency</a></li>
<li class="chapter" data-level="1.3" data-path="eda.html"><a href="eda.html#histograms-time-series-plots-and-scatterplots"><i class="fa fa-check"></i><b>1.3</b> Histograms, time series plots and scatterplots</a></li>
<li class="chapter" data-level="1.4" data-path="eda.html"><a href="eda.html#numerical-summary-statistics"><i class="fa fa-check"></i><b>1.4</b> Numerical summary statistics</a></li>
<li class="chapter" data-level="1.5" data-path="eda.html"><a href="eda.html#the-5-figure-summary-and-boxplots"><i class="fa fa-check"></i><b>1.5</b> The 5-figure summary and boxplots</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basic-probability.html"><a href="basic-probability.html"><i class="fa fa-check"></i><b>2</b> Basic Probability</a><ul>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#background"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="2.1" data-path="basic-probability.html"><a href="basic-probability.html#sample-space-and-events"><i class="fa fa-check"></i><b>2.1</b> Sample space and events</a><ul>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#the-venn-diagram"><i class="fa fa-check"></i>The Venn diagram</a></li>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#operations-with-events"><i class="fa fa-check"></i>Operations with events</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basic-probability.html"><a href="basic-probability.html#the-axioms-and-basic-rules-of-probability"><i class="fa fa-check"></i><b>2.2</b> The axioms and basic rules of probability</a></li>
<li class="chapter" data-level="2.3" data-path="basic-probability.html"><a href="basic-probability.html#assignment-of-probability"><i class="fa fa-check"></i><b>2.3</b> Assignment of probability</a><ul>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#classical-probability-for-equally-likely-events"><i class="fa fa-check"></i>Classical probability for equally-likely events</a></li>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#probability-as-relative-frequency-and-the-law-of-large-numbers"><i class="fa fa-check"></i>Probability as relative frequency and the Law of Large Numbers</a></li>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#subjective-assignment-of-probability"><i class="fa fa-check"></i>Subjective assignment of probability</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#combinatorics"><i class="fa fa-check"></i>Combinatorics</a><ul>
<li class="chapter" data-level="" data-path="basic-probability.html"><a href="basic-probability.html#basic-definitions"><i class="fa fa-check"></i>Basic definitions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html"><i class="fa fa-check"></i><b>3</b> Conditional Probability and Independence</a><ul>
<li class="chapter" data-level="3.1" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#indep-intro"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#definitions"><i class="fa fa-check"></i><b>3.2</b> Definitions</a></li>
<li class="chapter" data-level="3.3" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#independent-events"><i class="fa fa-check"></i><b>3.3</b> Independent events</a></li>
<li class="chapter" data-level="3.4" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#theorem-of-total-probability-and-bayes-theorem"><i class="fa fa-check"></i><b>3.4</b> Theorem of total probability and Bayes’ theorem</a><ul>
<li class="chapter" data-level="" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#total-probability-formula"><i class="fa fa-check"></i>Total probability formula</a></li>
<li class="chapter" data-level="" data-path="conditional-probability-and-independence.html"><a href="conditional-probability-and-independence.html#bayes-rule"><i class="fa fa-check"></i>Bayes’ rule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>4</b> Random Variables</a><ul>
<li class="chapter" data-level="4.1" data-path="random-variables.html"><a href="random-variables.html#basic-rv-definitions"><i class="fa fa-check"></i><b>4.1</b> Basic definitions</a></li>
<li class="chapter" data-level="4.2" data-path="random-variables.html"><a href="random-variables.html#expected-value-and-variance"><i class="fa fa-check"></i><b>4.2</b> Expected value and variance</a><ul>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#properties-of-expectation"><i class="fa fa-check"></i>Properties of expectation</a></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#estimation-of-parameters-using-the-expectation"><i class="fa fa-check"></i>Estimation of parameters using the expectation</a></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#variance-of-a-random-variable"><i class="fa fa-check"></i>Variance of a random variable</a></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#properties-of-variance"><i class="fa fa-check"></i>Properties of variance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#functions-of-random-variables"><i class="fa fa-check"></i>Functions of random variables</a></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#the-law-of-the-unconscious-statistician"><i class="fa fa-check"></i>The law of the unconscious statistician</a></li>
<li class="chapter" data-level="" data-path="random-variables.html"><a href="random-variables.html#probability-generating-functions"><i class="fa fa-check"></i>Probability generating functions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="models-for-count-data.html"><a href="models-for-count-data.html"><i class="fa fa-check"></i><b>5</b> Models for Count Data</a></li>
<li class="chapter" data-level="6" data-path="models-for-measurement-data.html"><a href="models-for-measurement-data.html"><i class="fa fa-check"></i><b>6</b> Models for Measurement Data</a></li>
<li class="chapter" data-level="7" data-path="bayesian-methods.html"><a href="bayesian-methods.html"><i class="fa fa-check"></i><b>7</b> Bayesian Methods</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH1710 Probability and Statistics 1</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="models-for-count-data" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Models for Count Data</h1>
<p><em>To follow.</em></p>
<!--

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\EE}{\mathbb{E}}
\renewcommand{\comb}[2]{\binom{#1}{#2}}

## Introduction {-}


The world is now awash with data. Commercial and public-sector
organisations increasingly depend on
massive databases
to refine their business operations. You may have thought, before starting this module,
that statistics is all about calculating summary statistics such as means, standard deviations and
correlations, and perhaps plotting pie charts and histograms of variables in a dataset.
To be sure, these things are statistics, but they are not the subject of Statistics. 
Statistics is the art and science of learning about the
real world through data and probability models.

Models can take many forms, but a key property is that
they are approximations of reality.
The famous statistician George Box said: "Essentially, all models are wrong, but some are useful" --- though later this was re-expressed elsewhere as "all models are approximations".


## Bernoulli trials and related distributions


We have discussed how random variables can be used to summarize the outcome of random experiments.
We shall continue this by looking at distributions which arise from repeated experiments.

Suppose a single **trial** is performed which has just two possible outcomes, for example

* a tossed coin is `Heads` or `Tails`,
* a child is a boy or a girl,
* a learner passes their driving test or not.


Let random variable $X$ describe this situation, taking value $1$ if the event of interest occurs and value $0$ if it does not occur,  with
\[ Pr(X=1)=p \text{ and } Pr(X=0)=1-p . \]

Note that, alternatively, we can write these in a single expression
\[ Pr(X=x) = p_X(x) = p^x(1-p)^{1-x}, \qquad x=0,1. \]

Here, $X$ is called a **Bernoulli random variable** and such a random experiment is called a **Bernoulli trial**.
Now suppose that we repeat this trial many times, but we keep all conditions constant.
That is the trials are independent and the probability is fixed.

\BeginKnitrBlock{example}<div class="example"><span class="example" id="exm:unnamed-chunk-1"><strong>(\#exm:unnamed-chunk-1) </strong></span>Consider a Call Centre, and suppose we call five randomly chosen phone numbers and note whether the call was answered or not.
Let $A_i$ denote the event that the call was answered on the $i$th call, and let $Pr(A_i)=p$ for $i=1,\ldots, 5$.
It is assumed that calls are answered independently.

We might observe the sequence $A_1A^c_2A_3A_4A^c_5$ (say) from the
$2^5=32$ possible sequences.
The probability of this particular sequence is
\[
Pr(A_1A^c_2A_3A_4A^c_5) = p\times (1-p)\times p\times p\times (1-p) = p^3 (1-p)^2.
\]
If we are only interested in the number of calls answered, then the above is only one of ten possible ways of getting three $A$'s and
two $A^c$'s, and so
\[
Pr(3A\mbox{ and } 2 A^c) = 10 \times p^3(1-p)^2.
\]
We can generalise this idea into the following result.</div>\EndKnitrBlock{example}


### The binomial distribution {-}

Let $X$ be the number of "successes" in $n$ repeated independent Bernoulli trials.
Possible values for $X$ are $0, 1,\ldots ,n$ giving a 
range space $\Omega_X=\{0, 1,\ldots ,n\}$.
The probability mass function of $X$ is
\[
p_X(x) = {n \choose x} p^x (1-p)^{n-x} \qquad x=0, 1,\ldots ,n.
\]
Note that $\binom{n}{x} = n!/(x!(n-x)!)$ arises by considering the number of permutations of $n$ objects, of which $x$ are of one type and the remaining $(n-x)$ are of another type (see Directed Reading on Combinatorics).

We say that $X$ is a binomial random variable and can write 
$X\sim B(n,p)$.

The following conditions give rise to the binomial distribution.

* A sequence of $n$ independent trials ($n$ fixed).
* Two possible outcomes at each trial (say "success" and "failure").
* Fixed probability of "success" at each trail.
* The random variable counts the number of "successes".

\BeginKnitrBlock{example}<div class="example"><span class="example" id="exm:unnamed-chunk-2"><strong>(\#exm:unnamed-chunk-2) </strong></span>A fair die is rolled ten times, what is the probability that exactly four rolls give "6"?

Let $X$ be the number of times "6" appears out of the ten rolls, then
$X\sim B(n=10,p=1/6)$ and we require the probability
\[
Pr(X=4) = p_X(4) = 
{10 \choose 4} \left(\frac16\right)^4 \left(\frac56\right)^6 = 0.0543.
\]

This is easily done in $R$ using the `dbinom` command, as
`dbinom(4, 10, 1/6)`.</div>\EndKnitrBlock{example}


Unfortunately, it is not possible to produce a simple equation to give the cumulative distribution function.
Instead, we would need to evaluate the individual probabilities, add them together and present the results as a table.


**Example 5.2 (cont.)**
First the probability mass function:

|    $x$   | $0$ | $1$ | $2$ | $3$ | $4$ | $5$ | $6$ | $7$ | $8$ | $9$ | $10$ |
|:--------:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:----:|
| $p_X(x)$ | $0.16$ | $0.32$ | $0.29$ | $0.16$ | $0.05$ | $0.01$ | $0.00$ | $0.00$ | $0.00$ | $0.00$ | $0.00$ |

Then, the corresponding cumulative distribution function:


| $F_X(x)$       | $0$           | $0.16$  | $0.48$  | $0.77$  | $0.93$  | $0.98$  | $1.00$  | $\cdots$ |
|----------------|---------------|---------|---------|---------|---------|---------|---------|----------|
| for $x \in {}$ | $(-\infty,0)$ | $[0,1)$ | $[1,2)$ | $[2,3)$ | $[3,4)$ | $[4,5)$ | $[5,6)$ | $\cdots$ |

These calculations 
use the commands: 
`round(dbinom(0:10, 10, 1/6), 2)` for the pmf and
`round(cumsum(dbinom(0:10, 10, 1/6)), 2)` for the cdf.


**Example 5.2 (cont.)**
The probability mass function of 
$X\sim B(n=10, p=1/6)$ is shown below-left, and the corresponding cumulative distribution function on the right.


<img src="math1710_files/figure-html/binom-1.png" width="50%" /><img src="math1710_files/figure-html/binom-2.png" width="50%" />

These graphs are produced in R using a combination of the commands 
`dbinom`, `cumsum`, `plot`, and `stepfun` -- see *R for MATH1710* Lesson 6.





**Expectation and variance of the binomial:**
If $X\sim B(n,p)$ then
\[
E[X]=np, \quad \mbox{and} \quad Var[X]=np(1-p).
\]


**Example 5.2 (cont.)**
In the previous example, we had
\(X\sim B(10, 1/6)\) and so
\(E[X]=np=10 \times \frac16 = \frac53\)
and
\(Var[X] = np(1-p) = 10\times \frac16 \times \frac56 = \frac{50}{36}=\frac{25}{18}
\).


**Proof:**
Starting with the definition of expectation and using the probability mass function for a binomial distribution we have
\[
E[X] = \sum_{x\in \Omega_X} x \; p_X(x)
=\sum_{x=0}^n x \; {n \choose x} p^x (1-p)^{n-x}.
\]
Then, the first term in the sum (with $x=0$) is zero, also in the general term we have
\[
x {n \choose x} = x \; \frac{n!}{x!(n-x)!}
= x \; \frac{n(n-1)!}{x(x-1)!(n-x)!} 
= n \; \frac{(n-1)!}{(x-1)!(n-x)!}
= n \; {(n-1)\choose (x-1)}
\]
and so
\[
E[X] = np \; \sum_{x=1}^n  {(n-1) \choose (x-1)} p^{x-1} (1-p)^{n-x}.
\]
Now, relabelling with \(r=x-1\), say, we get
\[
E[X] = np \; \sum_{r=0}^{n-1} {(n-1) \choose r} p^r (1-p)^{(n-1)-r}, \]
which is the probability mass function of \(B(n-1, p)\) so sums to \(1\), hence we get the required result
\[ E[X]  =np .\]
\end{align*}


Next, let us consider the $E[X(X-1)]$ hence
\[
E[X(X-1)] = \sum_{x\in \Omega_X} x(x-1) \; p_X(x)
=\sum_{x=0}^n x(x-1) \; {n \choose x} p^x (1-p)^{n-x}.
\]
Then, the first and second terms in the sum (with $x=0$ and $x=1$) are zero, 
also in the general term we have
\begin{align*}
x(x-1) {n \choose x} 
& = x(x-1) \; \frac{n!}{x!(n-x)!}
= x(x-1) \; \frac{n(n-1)!}{x(x-1)(x-2)!(n-x)!} \\
&= n(n-1) \; \frac{(n-2)!}{(x-2)!(n-x)!}
= n(n-1) \; {(n-2)\choose (x-2)}
\end{align*}
and so
\[
E[X(X-1)] = n(n-1)p^2 \; \sum_{x=2}^n  {(n-2) \choose (x-2)} p^{x-2} (1-p)^{n-x}.
\]
Now, relabelling with \(r=x-2\), say, we get
\[
E[X(X-1)] = n(n-1)p^2 \; \sum_{r=0}^{n-1} {(n-1) \choose r} p^r (1-p)^{(n-1)-r}. \]
which is the probability mass function of \(B(n-2, p)\) so sums to \(1\), hence we get the  result
\[ E[X(X-1)] =n(n-1)p^2. \]
To obtain the required result note that \(E[X(X-1)] = E[X^2]-E[X]\) hence
\(E[X^2]=E[X(X-1)]+E[X]\).
Recalling that \(E[X]=np\), we then get 
\(E[X^2]=n(n-1)p^2+np\) and, finally, this leads to 
\(Var[X]=E[X^2]-\{E[X\}^2 
= n(n-1)p^2+np - \{np\}^2 
= n^2p^2 - np^2 +np -n^2p^2 =  np(1-p)\), as required.




## Geometric distribution {-}

Suppose that we repeat independent Bernoulli trials until we get a "success".
Let $X$ be the number of failures before the first success.
Possible values for $X$ are $0, 1,\ldots$ (with no upper limit) giving
$\Omega_X = \{0, 1, \ldots\}$ and its probability mass function
\[
p_X(x) = Pr\left( x \mbox{ failures, then $1$ success}\right)
= (1-p)^{x} p, \qquad x= 0, 1, \ldots
\]
This is called the geometric distributions and is denoted  $X\sim Ge(p)$.
The same conditions are valid as for the binomial, except that $n$ is no longer fixed and the definition of the random variable has changed.


\BeginKnitrBlock{example}<div class="example"><span class="example" id="exm:unnamed-chunk-3"><strong>(\#exm:unnamed-chunk-3) </strong></span>Now suppose that a fair die is rolled until a six is shown and let $X$ be the total number of tosses needed hence $X\sim Ge(p=1/6)$. 
The probability mass function is shown below-left, and
the corresponding cumulative distribution function on the right.</div>\EndKnitrBlock{example}

<img src="math1710_files/figure-html/geom-1.png" width="50%" /><img src="math1710_files/figure-html/geom-2.png" width="50%" />

These graphs are produced in R using a combination of the commands 
`dgeom`, `cumsum`, `plot`, and `stepfun` -- see *R for MATH1710* Lesson 6.



---

*Note that there are two versions of the geometric distribution.
Compare to above the situation where we count the total number of trials, $Y$, which includes the failures but also include the final success -- this is sometimes referred to as the shifted geometric.*
*Here, possible values for $Y$ are $1, 2,\ldots$ (with no upper limit) giving*
$\Omega_Y = \{1, 2, \ldots\}$ *and its probability mass function*
\[
p_Y(y) = Pr\left( (y-1) \mbox{ failures, then $1$ success}\right)
= (1-p)^{y-1} p, \qquad y= 1, 2, \ldots.
\]
*Note that $Y=X+1$ and that $Pr(X=a) = Pr(Y=a+1)$.*
*Although here is no contradiction, we must be careful to ensure which version of the geometric is being used.*

---



When evaluating cumulative probabilities, it is useful to recall the formula for the sum of a geometric series
\[
S_n = 1+r+\cdots + r^n 
= \sum_{k=0}^n r^k 
= \frac{1-r^{n+1}}{1-r}
\]
and when $n$ is infinite,
$S_\infty = 1/(1-r)$ (for $|r|<1$).
Further, when deriving the expectation and variance, we consider 
derivatives of this expression with respect to $r$
giving
\[
\sum_{k=1}^\infty k \; r^{k-1} = \frac{1}{(1-r)^2}, \quad \mbox{and} \quad
\sum_{k=2}^\infty k(k-1) \; r^{k-2} = \frac{2}{(1-r)^3}, \qquad \mbox{for $|r|<1$}.
\]
Note that in these expressions the lower limit of summation has been adjusted to include only non-zero terms -- but it would have been equally correct to leave $k=0$.





**Expectation and variance of the geometric:**
If $X\sim Ge(p)$ then
\[
E[X] = (1-p)/p, \quad \mbox{and} \quad Var[X]=(1-p)/p^2.
\]

**Proof:**
Starting with the definition of expectation and using the probability mass function for a geometric distribution we have
\[
E[X] = \sum_{x\in \Omega_X} x \; p_X(x)
 =\sum_{x=0}^\infty x \; (1-p)^{x}p  = p \sum_{x=1}^\infty x \; (1-p)^{x} \]
then, replacing $x$ by $k$ and $1-p$ by $r$ and using one of the above results, we get
& =p (1-p)\sum_{k=1}^\infty k \; r^{k-1} = p (1-p) \times \frac{1}{p^2} = \frac{1-p}{p}.
\end{align*}

Next, let us consider the $E[X(X-1)]$ hence
\begin{align*}
E[X(X-1)] = \sum_{x\in \Omega_X} x(x-1) \; p_X(x)
& =\sum_{x=0}^\infty x(x-1) \; (1-p)^{x}p  \\
& = p(1-p)^2 \sum_{x=2}^\infty x(x-1) \; (1-p)^{x-2} \end{align*}
then, replacing $x$ by $k$ and $1-p$ by $r$ and using one of the above results, we get
\[ E[X(X-1)] =p (1-p)^2\sum_{k=2}^\infty k (k-1)\; r^{k-2} = p(1-p)^2 \times \frac{2}{p^3} = \frac{2(1-p)^2}{p^2}.\]


To obtain the required result note that \(E[X(X-1)] = E[X^2]-E[X]\) hence
\(E[X^2]=E[X(X-1)]+E[X]\).
Recalling that 
\(E[X]=(1-p)/p\), we then get 
\(E[X^2]={2(1-p)^2}/{p^2} +(1-p)/p \) 
and, finally, this leads to \(Var[X]=(1-p)/p^2\), as required.




\BeginKnitrBlock{example}<div class="example"><span class="example" id="exm:unnamed-chunk-4"><strong>(\#exm:unnamed-chunk-4) </strong></span>A fair coin is tossed until the first head is obtained.
What is the probability that at least 3 tosses are needed?

Let $X$ be the number of failures before the success, with $\Omega_X=\{0,1,\ldots\}$, then we require 
$Pr(X\ge 2)$ where $X\sim Ge(1/2)$.

Rather than evaluating $Pr(X\ge 2) = Pr(X=2)+Pr(X=3)+\cdots$, instead consider the complementary event
\begin{align*}
Pr(X\ge 2) &= 1-Pr(X<2) = 1-Pr(X=0\mbox{ or }X=1)\\
& = 1-\{Pr(X=0)+Pr(X=1)\}
=1-\left\{ \left(\frac12\right)^0\left(\frac12\right)
+ \left(\frac12\right)^1\left(\frac12\right)\right\}\\
&= 1-\frac12 - \frac14 =  \frac14.
\end{align*}

Further, the expectation and variance are:
\[
E[X]=\frac{1-1/2}{1/2} = 1 \qquad \mbox{and} \qquad
Var[X] = \frac{(1-1/2)}{(1/2)^2}  = 2.
\]


If, instead, we defined $Y$ as the total number of tosses, with $\Omega_Y=\{1,2\ldots\}$, then we require $\Pr(Y\ge 3)$ which would give the same numerical value as above.
In this situation, $\EE[Y]=2$ but the variance remains unchanged as $\Var[Y]=2$.
These can be verified from first principles, similar to the proof above, or by noting that 
$\EE[Y]=\EE[X+1] = \EE[X]+1$ and $\Var[Y]=\Var[X+1] = \Var[X]$.</div>\EndKnitrBlock{example}



## Poisson distribution (the law of rare events)


Let $X$ denote the number of events occurring (in some time interval or region in space) with known average rate, $\lambda$ (say "lambda"), with probability mass function
\[
Pr(X=x) = p_X(x) = \frac{e^{-\lambda} \lambda^x}{x!}, \qquad x=0,1,\ldots,
\]
then \(X\) is a Poisson random variable and we write $X\sim Po(\lambda)$.
Note that here, the range space is (countably) infinite, \(\Omega_X=\{0,1,\ldots \}\).


As before, to evaluate the expectation and variance, knowing the relevant standard 
(Maclaurin) series result can be useful:
$\sum _{k=0} ^\infty x^k/k! = e^x$.

The distribution is named after French mathematician Siméon Poisson (1781-1840) and was made popular through application to the number of cavalrymen in the Prussian army killed by kicks from a horse -- see, for example, Scheaffer and Young, 2010, pp 158-159. 


\BeginKnitrBlock{example}<div class="example"><span class="example" id="exm:unnamed-chunk-5"><strong>(\#exm:unnamed-chunk-5) </strong></span>Suppose that the average number of students missing lectures due to flu virus in a week is $\lambda=5.2$.
What is the probability that in a randomly selected week there are fewer than $2$ missing due to flu?

We have $X\sim Po(5.2)$ and we require $Pr(X<2)$, so
\begin{align*}
Pr(X<2) &= Pr(X=0) +Pr(X=1) \\
& = \frac{ e^{5.2} (5.2)^0}{0!} +  \frac{ e^{5.2} (5.2)^1}{1!}
=   e^{5.2} \left( 1 + 5.2\right) 
=  0.034.
\end{align*}

In this example you might have said that the binomial was a suitable model, with $n$ the number of students and $p$ the probability that a student gets flu. In fact, you are correct -- see the next section. However, we may not know the values of \(n\) and \(p\), but we might easily observe the average number missing.</div>\EndKnitrBlock{example}

**Example 5.5 (cont.)**
The probability mass function of a Poisson, $X\sim Po(\lambda=5.2)$ is shown below-left, and
the corresponding cumulative distribution function on the right.


<img src="math1710_files/figure-html/pois-1.png" width="50%" /><img src="math1710_files/figure-html/pois-2.png" width="50%" />


These graphs are produced in } using a combination of the commands 
`dpois`, `cumsum`, `plot`, and `stepfun` -- see *R for MATH1710* Lesson 6.


**Expectation and variance of the Poisson:**
If $X\sim Po(\lambda)$ then
\[
E[X] = \lambda, \quad \mbox{and} \quad Var[X]=\lambda.
\]

**Proof:**
Starting with the definition of expectation and using the probability mass function for a geometric distribution we have
\[
E[X] = \sum_{x\in \Omega_X} x \; p_X(x)
 =\sum_{x=0}^\infty x \; \frac{e^{-\lambda}\lambda^x}{x!}  
= \lambda \sum_{x=1}^\infty \frac{e^{-\lambda}\lambda^{x-1}}{(x-1)!} 
\]
then, replacing $x-1$ by $k$, we get
\[
E[X] = \lambda \sum_{k=0}^\infty \frac{e^{-\lambda}\lambda^k}{k!}   = \lambda
\]
since the sum is of a $Po(\lambda)$ over all possible values and hence equals 1.

Next, let us consider the $E[X(X-1)]$ hence
\[
E[X(X-1)] = \sum_{x\in \Omega_X} x(x-1) \; p_X(x)
 =\sum_{x=0}^\infty x(x-1) \; \frac{e^{-\lambda}\lambda^x}{x!}  
= \lambda^2 \sum_{x=2}^\infty \frac{e^{-\lambda}\lambda^{x-2}}{(x-2)!} 
\]
then, replacing $x-2$ by $k$, we get
\[
E[X(X-1)] = \lambda^2 \sum_{k=0}^\infty \frac{e^{-\lambda}\lambda^k}{k!}   = \lambda^2
\]
since the sum is of a $Po(\lambda)$ over all possible values and hence equals 1.




To obtaindorf the required result note that \(E[X(X-1)] = E[X^2]-E[X]\) hence
\(E[X^2]=E[X(X-1)]+E[X]\).
Recalling that \(E[X]=\lambda\), we then get 
$E[X^2]=\lambda^2+\lambda$ and hence 
\(Var[X]=E[X^2]-\{E[X\}^2 = \lambda^2+\lambda - \{\lambda\}^2 =\lambda\), as required.


### Poisson approximation to the binomial {-}


Suppose that $X\sim Po(\lambda)$ and
$Y\sim B(n,p)$ with $\lambda=np$, then if $p$ is small
\[
Pr(Y=r)\rightarrow  Pr(X=r), \quad \mbox{ as } n\rightarrow \infty.
\]
More usefully, we can say that for large $n$, and $p$ small, that
$Pr(Y=r) \approx  Pr(X=r)$.
In particular, the approximation is good if $n\ge 20$ and $p\le 0.05$ and 
very good if $n\ge 100$ and $np\le 10$.

**Proof:** See theoretical part of Example 4.10 in Stirzaker pp139-140.

We have, using $p=\lambda/n$,
\[
Pr(Y=r)  = {n \choose r} p^r (1-p)^{n-r}
= \frac{n!}{r! (n-r)!} 
\left(\frac{\lambda}{n}\right)^r
\left(1-\frac{\lambda}{n}\right)^{n-r}\]
which can be re-arranged to give
\[ Pr(Y = r) = 
\frac{n}{n}
\times
\frac{(n-1)}{n}
\times \cdots \times
\frac{(n-r+1)}{n}
 \, \,
\frac{\lambda^r}{r!}
\left(1-\frac{\lambda}{n}\right)^{n-r}.
\]
Now, $n\rightarrow \infty$ all the terms like
$(n-r+1)/n\rightarrow 1$, and  
$\left(1-\frac{\lambda}{n}\right)^{-r}\rightarrow 1$.

Now, what about the term $\left(1-\frac{\lambda}{n}\right)^{n}$ as $n\rightarrow \infty$?
Recall the binomial theorem:
\[
(A+B)^n = \sum _{r=0}^n {n\choose r} A^r B^{n-r} . \]
Then with $B=-\lambda/n$ and $A=1$ we have
\[
\left(1-\frac{\lambda}{n} \right)^n 
 = 
\sum_{r=0} ^\infty 
{n\choose r}
\left(-\frac{\lambda}{n}\right)^r
= 
\sum_{r=0} ^\infty 
\frac{n}{n}
\times
\frac{(n-1)}{n}
\times \cdots \times
\frac{(n-r+1)}{n}
\frac{(-\lambda)^r}{r!} \]
then following the same approach as above
\[\left(1-\frac{\lambda}{n} \right)^n  =
\sum_{r=0} ^\infty \frac{(-\lambda)^r}{r!}
= e^{-\lambda}.
\]
Putting these various parts together gives
\[
Pr(Y=r) = \frac{\lambda^r e^{-\lambda}}{r!}
= Pr(X=r), \qquad r=0,1,\ldots
\]

### Poisson processes {-}

Before moving on, it is worth noting another, and very important derivation of the Poisson distribution.
Suppose that  events occur *at random* throughout some period of time, or within some region, then the number of events
occurring in the interval of time, or region, follows a Poisson distribution.
As an example, suppose that on average an insurance
company receives 120 claims per working day (12 hours), but that 
claims have equal chances of being made at any time through the day.
Then, the actual number in a particular day follows a Poisson distribution with expected value 120, and that the number in a 1-hour period (say) follows a Poisson distribution with expected value 10.
The process in time, or space, is called a Poisson process --- you can see more of this in a Second Year module  Markov Processes.

---

*Now complete Worksheet 7 on standard discrete distributions 
to check your understanding.*

---

## Sampling from a finite population {-}



Consider a bag of $N$ balls, with $M$ being black and the remaining $N-M$ being white.
The experiment is to select a random sample of $n$ balls from the bag of $N$.
Let random variable $X$ be the number of black balls in the sample.
Suppose we want the probability
of the event, $A$, that there are $m$ black balls in the sample, that is $A=\{X=m\}$.


<img src="multinom.png" width="500" style="display: block; margin: auto;" />


Before we can calculate the probability, we must know if the selected balls are returned to the population or not -- this leads to:
(I) sampling with replacement, and
(II) sampling without replacement.


In (I) there are always the same $N$ balls to choose from, and each is equally likely to be selected.
So $|\Omega| = N\cdot N\cdots N = N^n$
and to calculate $|A|$ first consider a particular sequence of first $m$ black and then $(n-m)$ white, once chosen these can be permuted to give
$|A|= {n \choose m} M^m (N-M)^{n-m}$, hence
\[
Pr(A) = \frac{|A|}{|\Omega|} = 
\frac{{n \choose m} M^m (N-M)^{n-m}}{N^n}
= {n \choose m} \left(\frac{M}{N}\right)^m 
\left(1-\frac{M}{N}\right)^{n-m}.
\]
We can write $p=M/N$, which is the (unchanging) proportion of black balls in the population, and then
\[
Pr(A) = {n \choose m} p^m 
\left(1-p\right)^{n-m}.
\]
This is the binomial probability formula seen earlier.



In (II), the selected ball is not replaced and so the number of balls changes at each stage.

Now, $|\Omega|= \comb{N}{n}$, as we are considering selecting $n$ objects from $N$, and
$|A|= \comb{M}{m} \comb{N-M}{n-m}$ as we require $m$ from the $M$ and $(n-m)$ from the $N-M$,
hence
\[
Pr(A) = 
\frac{\comb{M}{m} \comb{N-M}{n-m}}{\comb{N}{n}}
=
\frac{{
{M\choose m}
{{N-M}\choose{n-m}}
}}{{{N}\choose {n}}}.
\]
this is called the hyper-geometric probability formula.

If we let $X$ be the number of black balls chosen  then we can write $X\sim Hyp(n,M,N)$ and the probability mass function
\[
p_X(x) 
=
\frac{{
{M\choose x}
{{N-M}\choose{n-x}}
}}{{{N}\choose {n}}}, \qquad
\max(0,n+M-N),\ldots, \min(M,n)
\]
and
\[
E[X] = n \left(\frac{M}{N}\right) \qquad
\mbox{and} \qquad
Var(X) = n \left(\frac{M}{N}\right)\left(\frac{N-M}{N}\right) \left(\frac{N-n}{N-1}\right).
\]
Notice that, initially the proportion of black balls in the population is $p_0=M/N$ and so
\[
E[X] = n p_0 \qquad
\mbox{and} \qquad
Var(X) = n p_0 (1-p_0) \left(\frac{N-n}{N-1}\right).
\]
Comparing these to the mean and variance of the binomial we see that the expectation is equal, that is on average we obtain the same number of black balls, but that since 
$(N-n)/(N-1)<1$, the variance of the hyper-geometric is smaller.

The value $(N-n)/(N-1)$ is called the **finite population correction factor**.
Notice that for small sample sizes and large population sizes the factor is close to 1
(for example with $n=10$ and $N=1000$ then 
$(N-n)/(N-1)=0.99$).
In fact, when dealing with such situations we can use the binomial as a good approximation to the 
hyper-geometric -- making calculations much easier.

\BeginKnitrBlock{example}<div class="example"><span class="example" id="exm:unnamed-chunk-6"><strong>(\#exm:unnamed-chunk-6) </strong></span>Consider sampling $n=5$ students in a class of size $N=50$ of whom $M=28$ are male.
What is the probability that all the sample are male?

\medskip

Let $X=\{\mbox{The number of male students in the sample}\}$, then
$X \sim Hyp(5, 28, 50)$ and we require
\[
P_X(5) = \frac{{28 \choose 5}{22 \choose 0}}{{50 \choose 5}} = 0.04638562 = 0.0464 \mbox{ (4 dp)}.
\]

If we had "forgotten" that we are sampling without replacement, then we would say that
$X\sim B(n=5, p=M/N=0.56)$ and
\[
p_X(5) = {5 \choose 5} (0.56)^5 (1-0.56)^0 = 0.05507318
= 0.0551  \mbox{ (4 dp)}
\]
which is considerably different to the correct value.



If we repeat these calculations for all maths students, with $N=500$ and $M=280$, then
the exact hyper-geometric  probability is
$p_X(5) = 0.054$, whereas the binomial probability is unchanged at $0.055$ -- which is very close.</div>\EndKnitrBlock{example}




## Additional Examples


\BeginKnitrBlock{example}<div class="example"><span class="example" id="exm:unnamed-chunk-7"><strong>(\#exm:unnamed-chunk-7) </strong></span>Suppose we roll an eight-sided die with sides labelled 1 to 8, where the even values are twice as likely as the odd numbers.


Let $Y$ represent the outcome with probability mass function</div>\EndKnitrBlock{example}

| $Y$ | $1$ | $2$ | $3$ | $4$ | $5$ | $6$ | $7$ | $8$ |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|$p_Y(y)$ | $\frac{1}{12}$ | $\frac16$ | $\frac{1}{12}$ | $\frac16$ | $\frac{1}{12}$ | $\frac16$ | $\frac{1}{12}$ | $\frac16$ |




Now
\begin{align*}
E[Y] & = \sum y \; p_Y(y) \\
&= 1\times \frac{1}{12} +
 2\times \frac{1}{6} +
  3\times \frac{1}{12} +
   4\times \frac{1}{6} +
    5\times \frac{1}{12} +
     6\times \frac{1}{6} +
      7\times \frac{1}{12} +
       8\times \frac{1}{6} =\frac{14}{3}.
\end{align*}

Then, for the variance,
\begin{align*}
E[Y^2] & = \sum y^2 p_Y(y) \\
&= 1^2\hspace{-3pt}\times \hspace{-3pt} \frac{1}{12} +
 2^2\hspace{-3pt}\times\hspace{-3pt} \frac{1}{6} +
  3^2\hspace{-3pt}\times\hspace{-3pt} \frac{1}{12} +
   4^2\hspace{-3pt}\times\hspace{-3pt} \frac{1}{6} +
    5^2\hspace{-3pt}\times\hspace{-3pt} \frac{1}{12} +
     6^2\hspace{-3pt}\times\hspace{-3pt} \frac{1}{6} +
      7^2\hspace{-3pt}\times\hspace{-3pt} \frac{1}{12} +
       8^2\hspace{-3pt}\times\hspace{-3pt} \frac{1}{6} =27
\end{align*}
giving
\[
Var(Y) = E[Y^2]-\{E[Y]\}^2 
= 27- \left\{\frac{14}{3}\right\}^2
=\frac{47}{9}.
\]

The first step can be easily calculated in R using
`sum(yvals*probs)`, with `yvals = 1:8` and `probs = c(1, 2, 1, 2, 1, 2, 1, 2) / 12`, and the second using
`sum(yvals^2 * probs)`.


Next consider $Z=6-3X$ then
\[ 
E[Z] =E[6-3Y] = 6-3E[Y] = 6-3\times \frac{14}{3} = -8
\]
and
\[ 
Var[Y] =Var[6-3Y] = (-3)^2Var[Y] = 9\times  \frac{47}{9}= 47.
\]

Now suppose that we also have a standard six-sided die which has
\( E[X]=7/2\) and \(Var[X]= 35/12\)
Then the expectation of the sum of the two dice is, 
\(E[X+Y] = E[X]+E[Y] = \frac{14}{3}+\frac{7}{2} = \frac{49}{6}
\)
and the variance of the sum  is
\[Var[X+Y] = Var[X]+Var[Y]
= \frac{35}{12}+ \frac{47}{9} = \frac{879}{108} = 8.14.
\]
Note that here, $X$ and $Y$ are physically independent and so using this result for independent random variables is valid.



## Important properties for sums of random variables {-}

Suppose we have two independent random variables, $X_1$ and $X_2$,
but are only interested in their sum, $Z=X_1+X_2$, then
\begin{align*}
G_Z(s) = E[s^Z] = E[s^{X_1+X_2}] \stackrel{indep}{=} E[s^{X_1}]E[s^{X_2}] = G_{X_1}(s) G_{X_2} (s)
\end{align*}
and the generalisation $Z=X_1+X_2+\cdots + X_n$ then
\[
G_Z(s) = G_{X_1}(s) G_{X_2} (s) \cdots G_{X_n}(s).
\]

## Pgfs of standard distributions {-}

| Distribution | pgf                               |
|:-------------|:----------------------------------|
| Bernoulli    | $1-p +ps$                         |
| Binomial     | $\left\{ 1-p +ps\right\}^n$       |
| Geometric    | $ps/\left(1-(1-p)s \right)$       |
| Poisson      | $\exp\left(\lambda (s-1)\right)$  |


\BeginKnitrBlock{example}<div class="example"><span class="example" id="exm:unnamed-chunk-8"><strong>(\#exm:unnamed-chunk-8) </strong></span>Suppose we are interested in the sum of $n$ independent Bernoulli random variables, then\
\[
G_Z(s) = \left(1-p+ps\right) \cdots  \left(1-p+ps\right) 
=  \left(1-p+ps\right) ^n.
\]
Although not yet derived, this is the 
probability generating function of the binomial, and so the sum of Bernoulli random variables is binomial, $X_1+X_2+\cdots + X_n\sim B(n,p)$.</div>\EndKnitrBlock{example}

-->

</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="models-for-measurement-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
